{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcdfea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "\n",
    "from dataloaders import *\n",
    "from util import Config\n",
    "from viz_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b644810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TREC-6-BERT\n",
      "Loading TREC-6 -- ada -- BERT\n",
      "Loading TREC-6 -- ada-besov -- BERT\n"
     ]
    }
   ],
   "source": [
    "datasets = [\"MRPC\", \"TREC-2\", \"SUBJ\", \"AGN-2\", \"TREC-6\", \"AGN-4\", \"SST\"]\n",
    "dataset_map = {\n",
    "    \"TREC-2\": \"TREC-2\",\n",
    "    \"SUBJ\": \"SUBJ\",\n",
    "    \"AGN-2\": \"AGN-2\",\n",
    "    \"TREC-6\": \"TREC-full\",\n",
    "    \"SST\": \"SST\",\n",
    "    \"COLA\": \"COLA\",\n",
    "    \"AGN-4\": \"ag_news-full\",\n",
    "}\n",
    "models = [\"BERT\", \"ELECTRA\"]\n",
    "load_anti = False\n",
    "n = 0  # AL step at which evaluation (AUC) starts\n",
    "model = \"BERT\"\n",
    "mode = \"ada\"\n",
    "dataset = \"TREC-6\"\n",
    "\n",
    "aucs = []\n",
    "trs = []\n",
    "try:\n",
    "    experiments, meta = load_results(\n",
    "        base_dir=f\"results/\",\n",
    "        dataset=dataset,\n",
    "        model=model,\n",
    "    )\n",
    "except:\n",
    "    print(f\"No experiments for {dataset}-{model}-{mode}\")\n",
    "for load_mode in [\"last\", \"best\"]:\n",
    "    if mode == \"short\" and load_mode == \"best\":\n",
    "        continue\n",
    "    mode_print = mode if load_mode == \"last\" else f\"{mode}-besov\"\n",
    "    print(f\"Loading {dataset} -- {mode_print} -- {model}\")\n",
    "    df_tr_i = results_to_df(experiments, mode=load_mode)\n",
    "\n",
    "    df_tr_i[\"model\"] = model\n",
    "    df_tr_i[\"mode\"] = mode_print\n",
    "    df_tr_i[\"dataset\"] = dataset\n",
    "    df_tr_i = df_tr_i.reset_index().set_index(\n",
    "        [\"dataset\", \"model\", \"mode\", \"sampler\", \"experiment\", \"al_iter\"]\n",
    "    )\n",
    "    trs.append(df_tr_i)\n",
    "\n",
    "    df_auc_i = al_auc(df_tr_i)\n",
    "    df_auc_i[\"mode\"] = mode_print\n",
    "    df_auc_i = df_auc_i.reset_index().set_index([\"mode\", \"sampler\"])\n",
    "    aucs.append(df_auc_i)\n",
    "\n",
    "\n",
    "# plot_besov_index(df_tr, ci=0)\n",
    "# plot_al_accuracy(df_tr, metric=\"f1_micro\", ci=0)\n",
    "df_tr = pd.concat(trs)\n",
    "df_auc = pd.concat(aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b2b3b91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200.0, 1000.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "g = sns.lineplot(\n",
    "    data=df_tr[df_tr.index.get_level_values(\"mode\") == \"ada-besov\"],\n",
    "    x=\"labeled\",\n",
    "    y=\"test_accuracy\",\n",
    "    hue=\"sampler\",\n",
    "    style=\"sampler\",\n",
    "    markers=True,\n",
    "    dashes=False,\n",
    "    ci=95,\n",
    "    linewidth=3,\n",
    ")\n",
    "# plt.legend(loc='lower right')\n",
    "g.set_ylim(0.89, 0.98)\n",
    "g.set_xlim(200, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5628051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>labeled</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>selected</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>mode</th>\n",
       "      <th>sampler</th>\n",
       "      <th>experiment</th>\n",
       "      <th>al_iter</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"11\" valign=\"top\">TREC-2</th>\n",
       "      <th rowspan=\"11\" valign=\"top\">BERT</th>\n",
       "      <th rowspan=\"11\" valign=\"top\">ada-besov</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">random</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>0.657703</td>\n",
       "      <td>0.674897</td>\n",
       "      <td>0.674897</td>\n",
       "      <td>0.673964</td>\n",
       "      <td>[1549, 1546, 1179, 111, 561, 1832, 619, 56, 51...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>150</td>\n",
       "      <td>0.662511</td>\n",
       "      <td>0.709877</td>\n",
       "      <td>0.709877</td>\n",
       "      <td>0.701160</td>\n",
       "      <td>[364, 841, 941, 922, 152, 1904, 903, 450, 447,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200</td>\n",
       "      <td>0.651112</td>\n",
       "      <td>0.695473</td>\n",
       "      <td>0.695473</td>\n",
       "      <td>0.691073</td>\n",
       "      <td>[142, 543, 930, 872, 865, 1646, 95, 589, 513, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>250</td>\n",
       "      <td>0.633320</td>\n",
       "      <td>0.738683</td>\n",
       "      <td>0.738683</td>\n",
       "      <td>0.738194</td>\n",
       "      <td>[1630, 1187, 1237, 1552, 682, 1424, 357, 178, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>300</td>\n",
       "      <td>0.642351</td>\n",
       "      <td>0.732510</td>\n",
       "      <td>0.732510</td>\n",
       "      <td>0.732143</td>\n",
       "      <td>[499, 1225, 1423, 438, 1077, 1637, 566, 1410, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">entropy</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">4</th>\n",
       "      <th>14</th>\n",
       "      <td>800</td>\n",
       "      <td>0.658221</td>\n",
       "      <td>0.775720</td>\n",
       "      <td>0.775720</td>\n",
       "      <td>0.771635</td>\n",
       "      <td>[1416, 1090, 83, 524, 1923, 1278, 1293, 882, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>850</td>\n",
       "      <td>0.649009</td>\n",
       "      <td>0.806584</td>\n",
       "      <td>0.806584</td>\n",
       "      <td>0.806466</td>\n",
       "      <td>[1763, 240, 1153, 246, 1013, 1552, 1818, 1850,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>900</td>\n",
       "      <td>0.659438</td>\n",
       "      <td>0.800412</td>\n",
       "      <td>0.800412</td>\n",
       "      <td>0.799964</td>\n",
       "      <td>[168, 1248, 1598, 714, 784, 1841, 1253, 1873, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>950</td>\n",
       "      <td>0.656500</td>\n",
       "      <td>0.818930</td>\n",
       "      <td>0.818930</td>\n",
       "      <td>0.818820</td>\n",
       "      <td>[1455, 1577, 1788, 1880, 1023, 1458, 1126, 791...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.648496</td>\n",
       "      <td>0.816872</td>\n",
       "      <td>0.816872</td>\n",
       "      <td>0.816853</td>\n",
       "      <td>[1606, 456, 1572, 1407, 57, 1461, 463, 294, 12...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>475 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    labeled  train_loss  \\\n",
       "dataset model mode      sampler experiment al_iter                        \n",
       "TREC-2  BERT  ada-besov random  0          0            100    0.657703   \n",
       "                                           1            150    0.662511   \n",
       "                                           2            200    0.651112   \n",
       "                                           3            250    0.633320   \n",
       "                                           4            300    0.642351   \n",
       "...                                                     ...         ...   \n",
       "                        entropy 4          14           800    0.658221   \n",
       "                                           15           850    0.649009   \n",
       "                                           16           900    0.659438   \n",
       "                                           17           950    0.656500   \n",
       "                                           18          1000    0.648496   \n",
       "\n",
       "                                                    test_accuracy  f1_micro  \\\n",
       "dataset model mode      sampler experiment al_iter                            \n",
       "TREC-2  BERT  ada-besov random  0          0             0.674897  0.674897   \n",
       "                                           1             0.709877  0.709877   \n",
       "                                           2             0.695473  0.695473   \n",
       "                                           3             0.738683  0.738683   \n",
       "                                           4             0.732510  0.732510   \n",
       "...                                                           ...       ...   \n",
       "                        entropy 4          14            0.775720  0.775720   \n",
       "                                           15            0.806584  0.806584   \n",
       "                                           16            0.800412  0.800412   \n",
       "                                           17            0.818930  0.818930   \n",
       "                                           18            0.816872  0.816872   \n",
       "\n",
       "                                                    f1_macro  \\\n",
       "dataset model mode      sampler experiment al_iter             \n",
       "TREC-2  BERT  ada-besov random  0          0        0.673964   \n",
       "                                           1        0.701160   \n",
       "                                           2        0.691073   \n",
       "                                           3        0.738194   \n",
       "                                           4        0.732143   \n",
       "...                                                      ...   \n",
       "                        entropy 4          14       0.771635   \n",
       "                                           15       0.806466   \n",
       "                                           16       0.799964   \n",
       "                                           17       0.818820   \n",
       "                                           18       0.816853   \n",
       "\n",
       "                                                                                             selected  \n",
       "dataset model mode      sampler experiment al_iter                                                     \n",
       "TREC-2  BERT  ada-besov random  0          0        [1549, 1546, 1179, 111, 561, 1832, 619, 56, 51...  \n",
       "                                           1        [364, 841, 941, 922, 152, 1904, 903, 450, 447,...  \n",
       "                                           2        [142, 543, 930, 872, 865, 1646, 95, 589, 513, ...  \n",
       "                                           3        [1630, 1187, 1237, 1552, 682, 1424, 357, 178, ...  \n",
       "                                           4        [499, 1225, 1423, 438, 1077, 1637, 566, 1410, ...  \n",
       "...                                                                                               ...  \n",
       "                        entropy 4          14       [1416, 1090, 83, 524, 1923, 1278, 1293, 882, 1...  \n",
       "                                           15       [1763, 240, 1153, 246, 1013, 1552, 1818, 1850,...  \n",
       "                                           16       [168, 1248, 1598, 714, 784, 1841, 1253, 1873, ...  \n",
       "                                           17       [1455, 1577, 1788, 1880, 1023, 1458, 1126, 791...  \n",
       "                                           18       [1606, 456, 1572, 1407, 57, 1461, 463, 294, 12...  \n",
       "\n",
       "[475 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf138bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>labeled</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>selected</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>model</th>\n",
       "      <th>sampler</th>\n",
       "      <th>experiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"25\" valign=\"top\">TREC-2</th>\n",
       "      <th rowspan=\"25\" valign=\"top\">BERT</th>\n",
       "      <th rowspan=\"25\" valign=\"top\">BERT</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">core_set</th>\n",
       "      <th>0</th>\n",
       "      <td>[100, 150, 200, 250, 300, 350, 400, 450, 500, ...</td>\n",
       "      <td>[0.6577029973268509, 0.6469762802124024, 0.636...</td>\n",
       "      <td>[0.6748971193415638, 0.551440329218107, 0.5452...</td>\n",
       "      <td>[0.6748971193415638, 0.551440329218107, 0.5452...</td>\n",
       "      <td>[0.6739639945652174, 0.45776695054045197, 0.44...</td>\n",
       "      <td>[[1549, 1546, 1179, 111, 561, 1832, 619, 56, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[100, 150, 200, 250, 300, 350, 400, 450, 500, ...</td>\n",
       "      <td>[0.6459900438785553, 0.6821997761726379, 0.616...</td>\n",
       "      <td>[0.6748971193415638, 0.49588477366255146, 0.56...</td>\n",
       "      <td>[0.6748971193415638, 0.49588477366255146, 0.56...</td>\n",
       "      <td>[0.6708333333333334, 0.3350830657545721, 0.486...</td>\n",
       "      <td>[[998, 1179, 1018, 232, 1471, 696, 1089, 891, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[100, 150, 200, 250, 300, 350, 400, 450, 500, ...</td>\n",
       "      <td>[0.6794537752866745, 0.6754907011985779, 0.601...</td>\n",
       "      <td>[0.6152263374485597, 0.5061728395061729, 0.5, ...</td>\n",
       "      <td>[0.6152263374485597, 0.5061728395061729, 0.5, ...</td>\n",
       "      <td>[0.6121904696881121, 0.36681649403947625, 0.34...</td>\n",
       "      <td>[[454, 565, 154, 1536, 1952, 959, 1490, 816, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[100, 150, 200, 250, 300, 350, 400, 450, 500, ...</td>\n",
       "      <td>[0.6245803534984589, 0.6158845901489258, 0.592...</td>\n",
       "      <td>[0.5555555555555556, 0.49794238683127573, 0.49...</td>\n",
       "      <td>[0.5555555555555556, 0.49794238683127573, 0.49...</td>\n",
       "      <td>[0.4830601953986763, 0.33955622883621456, 0.33...</td>\n",
       "      <td>[[1736, 11, 1693, 1079, 1304, 1144, 1330, 1452...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[100, 150, 200, 250, 300, 350, 400, 450, 500, ...</td>\n",
       "      <td>[0.650259718298912, 0.6353173375129699, 0.6251...</td>\n",
       "      <td>[0.49176954732510286, 0.49382716049382713, 0.4...</td>\n",
       "      <td>[0.49176954732510286, 0.49382716049382713, 0.4...</td>\n",
       "      <td>[0.3296551724137931, 0.3305785123966942, 0.339...</td>\n",
       "      <td>[[1268, 33, 599, 432, 1073, 156, 1319, 589, 81...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">dal</th>\n",
       "      <th>0</th>\n",
       "      <td>[100, 150, 200, 250, 300, 350, 400, 450, 500, ...</td>\n",
       "      <td>[0.6577029973268509, 0.6642346858978272, 0.663...</td>\n",
       "      <td>[0.6748971193415638, 0.6604938271604939, 0.666...</td>\n",
       "      <td>[0.6748971193415638, 0.6604938271604939, 0.666...</td>\n",
       "      <td>[0.6739639945652174, 0.6492540251151437, 0.648...</td>\n",
       "      <td>[[1549, 1546, 1179, 111, 561, 1832, 619, 56, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[100, 150, 200, 250, 300, 350, 400, 450, 500, ...</td>\n",
       "      <td>[0.6459900438785553, 0.6488877415657044, 0.636...</td>\n",
       "      <td>[0.6748971193415638, 0.6893004115226338, 0.703...</td>\n",
       "      <td>[0.6748971193415638, 0.6893004115226338, 0.703...</td>\n",
       "      <td>[0.6708333333333334, 0.6868489888925396, 0.699...</td>\n",
       "      <td>[[998, 1179, 1018, 232, 1471, 696, 1089, 891, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[100, 150, 200, 250, 300, 350, 400, 450, 500, ...</td>\n",
       "      <td>[0.6794537752866745, 0.651800012588501, 0.6412...</td>\n",
       "      <td>[0.6152263374485597, 0.7078189300411523, 0.713...</td>\n",
       "      <td>[0.6152263374485597, 0.7078189300411522, 0.713...</td>\n",
       "      <td>[0.6121904696881121, 0.7060216739367502, 0.706...</td>\n",
       "      <td>[[454, 565, 154, 1536, 1952, 959, 1490, 816, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[100, 150, 200, 250, 300, 350, 400, 450, 500, ...</td>\n",
       "      <td>[0.6245803534984589, 0.6310327291488648, 0.683...</td>\n",
       "      <td>[0.5555555555555556, 0.6851851851851852, 0.716...</td>\n",
       "      <td>[0.5555555555555556, 0.6851851851851852, 0.716...</td>\n",
       "      <td>[0.4830601953986763, 0.6752514510571208, 0.715...</td>\n",
       "      <td>[[1736, 11, 1693, 1079, 1304, 1144, 1330, 1452...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[100, 150, 200, 250, 300, 350, 400, 450, 500, ...</td>\n",
       "      <td>[0.650259718298912, 0.6481092929840088, 0.6514...</td>\n",
       "      <td>[0.49176954732510286, 0.654320987654321, 0.703...</td>\n",
       "      <td>[0.49176954732510286, 0.654320987654321, 0.703...</td>\n",
       "      <td>[0.3296551724137931, 0.6377318306859525, 0.703...</td>\n",
       "      <td>[[1268, 33, 599, 432, 1073, 156, 1319, 589, 81...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">entropy</th>\n",
       "      <th>0</th>\n",
       "      <td>[100, 150, 200, 250, 300, 350, 400, 450, 500, ...</td>\n",
       "      <td>[0.6577029973268509, 0.6654908657073975, 0.656...</td>\n",
       "      <td>[0.6748971193415638, 0.6728395061728395, 0.699...</td>\n",
       "      <td>[0.6748971193415638, 0.6728395061728395, 0.699...</td>\n",
       "      <td>[0.6739639945652174, 0.6620084242018659, 0.698...</td>\n",
       "      <td>[[1549, 1546, 1179, 111, 561, 1832, 619, 56, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[100, 150, 200, 250, 300, 350, 400, 450, 500, ...</td>\n",
       "      <td>[0.6459900438785553, 0.6613013267517089, 0.672...</td>\n",
       "      <td>[0.6748971193415638, 0.6008230452674898, 0.707...</td>\n",
       "      <td>[0.6748971193415638, 0.6008230452674898, 0.707...</td>\n",
       "      <td>[0.6708333333333334, 0.5784343533704148, 0.697...</td>\n",
       "      <td>[[998, 1179, 1018, 232, 1471, 696, 1089, 891, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[100, 150, 200, 250, 300, 350, 400, 450, 500, ...</td>\n",
       "      <td>[0.6794537752866745, 0.6476762533187866, 0.629...</td>\n",
       "      <td>[0.6152263374485597, 0.7119341563786008, 0.563...</td>\n",
       "      <td>[0.6152263374485597, 0.7119341563786008, 0.563...</td>\n",
       "      <td>[0.6121904696881121, 0.6991989248262569, 0.490...</td>\n",
       "      <td>[[454, 565, 154, 1536, 1952, 959, 1490, 816, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[100, 150, 200, 250, 300, 350, 400, 450, 500, ...</td>\n",
       "      <td>[0.6245803534984589, 0.6380608439445495, 0.643...</td>\n",
       "      <td>[0.5555555555555556, 0.7016460905349794, 0.646...</td>\n",
       "      <td>[0.5555555555555556, 0.7016460905349794, 0.646...</td>\n",
       "      <td>[0.4830601953986763, 0.6958658996059679, 0.637...</td>\n",
       "      <td>[[1736, 11, 1693, 1079, 1304, 1144, 1330, 1452...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[100, 150, 200, 250, 300, 350, 400, 450, 500, ...</td>\n",
       "      <td>[0.650259718298912, 0.642036247253418, 0.67306...</td>\n",
       "      <td>[0.49176954732510286, 0.7160493827160493, 0.68...</td>\n",
       "      <td>[0.49176954732510286, 0.7160493827160493, 0.68...</td>\n",
       "      <td>[0.3296551724137931, 0.7139127764127764, 0.689...</td>\n",
       "      <td>[[1268, 33, 599, 432, 1073, 156, 1319, 589, 81...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">entropy_dropout</th>\n",
       "      <th>0</th>\n",
       "      <td>[100, 150, 200, 250, 300, 350, 400, 450, 500, ...</td>\n",
       "      <td>[0.6577029973268509, 0.6431065201759338, 0.632...</td>\n",
       "      <td>[0.6748971193415638, 0.5267489711934157, 0.707...</td>\n",
       "      <td>[0.6748971193415638, 0.5267489711934157, 0.707...</td>\n",
       "      <td>[0.6739639945652174, 0.425531914893617, 0.7068...</td>\n",
       "      <td>[[1549, 1546, 1179, 111, 561, 1832, 619, 56, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[100, 150, 200, 250, 300, 350, 400, 450, 500, ...</td>\n",
       "      <td>[0.6459900438785553, 0.6572281002998352, 0.609...</td>\n",
       "      <td>[0.6748971193415638, 0.6954732510288066, 0.520...</td>\n",
       "      <td>[0.6748971193415638, 0.6954732510288066, 0.520...</td>\n",
       "      <td>[0.6708333333333334, 0.6904352017628426, 0.392...</td>\n",
       "      <td>[[998, 1179, 1018, 232, 1471, 696, 1089, 891, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[100, 150, 200, 250, 300, 350, 400, 450, 500, ...</td>\n",
       "      <td>[0.6794537752866745, 0.6576382756233216, 0.652...</td>\n",
       "      <td>[0.6152263374485597, 0.7222222222222222, 0.722...</td>\n",
       "      <td>[0.6152263374485597, 0.7222222222222222, 0.722...</td>\n",
       "      <td>[0.6121904696881121, 0.7212296318327633, 0.712...</td>\n",
       "      <td>[[454, 565, 154, 1536, 1952, 959, 1490, 816, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[100, 150, 200, 250, 300, 350, 400, 450, 500, ...</td>\n",
       "      <td>[0.6245803534984589, 0.6216179490089416, 0.625...</td>\n",
       "      <td>[0.5555555555555556, 0.6213991769547325, 0.724...</td>\n",
       "      <td>[0.5555555555555556, 0.6213991769547325, 0.724...</td>\n",
       "      <td>[0.4830601953986763, 0.5993548387096774, 0.722...</td>\n",
       "      <td>[[1736, 11, 1693, 1079, 1304, 1144, 1330, 1452...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[100, 150, 200, 250, 300, 350, 400, 450, 500, ...</td>\n",
       "      <td>[0.650259718298912, 0.6512132883071899, 0.6700...</td>\n",
       "      <td>[0.49176954732510286, 0.6625514403292181, 0.62...</td>\n",
       "      <td>[0.49176954732510286, 0.6625514403292181, 0.62...</td>\n",
       "      <td>[0.3296551724137931, 0.6558133107629591, 0.598...</td>\n",
       "      <td>[[1268, 33, 599, 432, 1073, 156, 1319, 589, 81...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">random</th>\n",
       "      <th>0</th>\n",
       "      <td>[100, 150, 200, 250, 300, 350, 400, 450, 500, ...</td>\n",
       "      <td>[0.6577029973268509, 0.6625113010406494, 0.651...</td>\n",
       "      <td>[0.6748971193415638, 0.7098765432098766, 0.695...</td>\n",
       "      <td>[0.6748971193415638, 0.7098765432098766, 0.695...</td>\n",
       "      <td>[0.6739639945652174, 0.7011604530171343, 0.691...</td>\n",
       "      <td>[[1549, 1546, 1179, 111, 561, 1832, 619, 56, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[100, 150, 200, 250, 300, 350, 400, 450, 500, ...</td>\n",
       "      <td>[0.6459900438785553, 0.6732913494110108, 0.649...</td>\n",
       "      <td>[0.6748971193415638, 0.5987654320987654, 0.730...</td>\n",
       "      <td>[0.6748971193415638, 0.5987654320987654, 0.730...</td>\n",
       "      <td>[0.6708333333333334, 0.5652173913043478, 0.730...</td>\n",
       "      <td>[[998, 1179, 1018, 232, 1471, 696, 1089, 891, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[100, 150, 200, 250, 300, 350, 400, 450, 500, ...</td>\n",
       "      <td>[0.6794537752866745, 0.6374668717384339, 0.640...</td>\n",
       "      <td>[0.6152263374485597, 0.5534979423868313, 0.547...</td>\n",
       "      <td>[0.6152263374485597, 0.5534979423868313, 0.547...</td>\n",
       "      <td>[0.6121904696881121, 0.48521723850107634, 0.46...</td>\n",
       "      <td>[[454, 565, 154, 1536, 1952, 959, 1490, 816, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[100, 150, 200, 250, 300, 350, 400, 450, 500, ...</td>\n",
       "      <td>[0.6245803534984589, 0.6433198928833008, 0.631...</td>\n",
       "      <td>[0.5555555555555556, 0.6090534979423868, 0.730...</td>\n",
       "      <td>[0.5555555555555556, 0.6090534979423868, 0.730...</td>\n",
       "      <td>[0.4830601953986763, 0.589527027027027, 0.7304...</td>\n",
       "      <td>[[1736, 11, 1693, 1079, 1304, 1144, 1330, 1452...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[100, 150, 200, 250, 300, 350, 400, 450, 500, ...</td>\n",
       "      <td>[0.650259718298912, 0.6340792775154114, 0.6493...</td>\n",
       "      <td>[0.49176954732510286, 0.588477366255144, 0.722...</td>\n",
       "      <td>[0.49176954732510286, 0.588477366255144, 0.722...</td>\n",
       "      <td>[0.3296551724137931, 0.5326293924182102, 0.721...</td>\n",
       "      <td>[[1268, 33, 599, 432, 1073, 156, 1319, 589, 81...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                          labeled  \\\n",
       "dataset model model sampler         experiment                                                      \n",
       "TREC-2  BERT  BERT  core_set        0           [100, 150, 200, 250, 300, 350, 400, 450, 500, ...   \n",
       "                                    1           [100, 150, 200, 250, 300, 350, 400, 450, 500, ...   \n",
       "                                    2           [100, 150, 200, 250, 300, 350, 400, 450, 500, ...   \n",
       "                                    3           [100, 150, 200, 250, 300, 350, 400, 450, 500, ...   \n",
       "                                    4           [100, 150, 200, 250, 300, 350, 400, 450, 500, ...   \n",
       "                    dal             0           [100, 150, 200, 250, 300, 350, 400, 450, 500, ...   \n",
       "                                    1           [100, 150, 200, 250, 300, 350, 400, 450, 500, ...   \n",
       "                                    2           [100, 150, 200, 250, 300, 350, 400, 450, 500, ...   \n",
       "                                    3           [100, 150, 200, 250, 300, 350, 400, 450, 500, ...   \n",
       "                                    4           [100, 150, 200, 250, 300, 350, 400, 450, 500, ...   \n",
       "                    entropy         0           [100, 150, 200, 250, 300, 350, 400, 450, 500, ...   \n",
       "                                    1           [100, 150, 200, 250, 300, 350, 400, 450, 500, ...   \n",
       "                                    2           [100, 150, 200, 250, 300, 350, 400, 450, 500, ...   \n",
       "                                    3           [100, 150, 200, 250, 300, 350, 400, 450, 500, ...   \n",
       "                                    4           [100, 150, 200, 250, 300, 350, 400, 450, 500, ...   \n",
       "                    entropy_dropout 0           [100, 150, 200, 250, 300, 350, 400, 450, 500, ...   \n",
       "                                    1           [100, 150, 200, 250, 300, 350, 400, 450, 500, ...   \n",
       "                                    2           [100, 150, 200, 250, 300, 350, 400, 450, 500, ...   \n",
       "                                    3           [100, 150, 200, 250, 300, 350, 400, 450, 500, ...   \n",
       "                                    4           [100, 150, 200, 250, 300, 350, 400, 450, 500, ...   \n",
       "                    random          0           [100, 150, 200, 250, 300, 350, 400, 450, 500, ...   \n",
       "                                    1           [100, 150, 200, 250, 300, 350, 400, 450, 500, ...   \n",
       "                                    2           [100, 150, 200, 250, 300, 350, 400, 450, 500, ...   \n",
       "                                    3           [100, 150, 200, 250, 300, 350, 400, 450, 500, ...   \n",
       "                                    4           [100, 150, 200, 250, 300, 350, 400, 450, 500, ...   \n",
       "\n",
       "                                                                                       train_loss  \\\n",
       "dataset model model sampler         experiment                                                      \n",
       "TREC-2  BERT  BERT  core_set        0           [0.6577029973268509, 0.6469762802124024, 0.636...   \n",
       "                                    1           [0.6459900438785553, 0.6821997761726379, 0.616...   \n",
       "                                    2           [0.6794537752866745, 0.6754907011985779, 0.601...   \n",
       "                                    3           [0.6245803534984589, 0.6158845901489258, 0.592...   \n",
       "                                    4           [0.650259718298912, 0.6353173375129699, 0.6251...   \n",
       "                    dal             0           [0.6577029973268509, 0.6642346858978272, 0.663...   \n",
       "                                    1           [0.6459900438785553, 0.6488877415657044, 0.636...   \n",
       "                                    2           [0.6794537752866745, 0.651800012588501, 0.6412...   \n",
       "                                    3           [0.6245803534984589, 0.6310327291488648, 0.683...   \n",
       "                                    4           [0.650259718298912, 0.6481092929840088, 0.6514...   \n",
       "                    entropy         0           [0.6577029973268509, 0.6654908657073975, 0.656...   \n",
       "                                    1           [0.6459900438785553, 0.6613013267517089, 0.672...   \n",
       "                                    2           [0.6794537752866745, 0.6476762533187866, 0.629...   \n",
       "                                    3           [0.6245803534984589, 0.6380608439445495, 0.643...   \n",
       "                                    4           [0.650259718298912, 0.642036247253418, 0.67306...   \n",
       "                    entropy_dropout 0           [0.6577029973268509, 0.6431065201759338, 0.632...   \n",
       "                                    1           [0.6459900438785553, 0.6572281002998352, 0.609...   \n",
       "                                    2           [0.6794537752866745, 0.6576382756233216, 0.652...   \n",
       "                                    3           [0.6245803534984589, 0.6216179490089416, 0.625...   \n",
       "                                    4           [0.650259718298912, 0.6512132883071899, 0.6700...   \n",
       "                    random          0           [0.6577029973268509, 0.6625113010406494, 0.651...   \n",
       "                                    1           [0.6459900438785553, 0.6732913494110108, 0.649...   \n",
       "                                    2           [0.6794537752866745, 0.6374668717384339, 0.640...   \n",
       "                                    3           [0.6245803534984589, 0.6433198928833008, 0.631...   \n",
       "                                    4           [0.650259718298912, 0.6340792775154114, 0.6493...   \n",
       "\n",
       "                                                                                    test_accuracy  \\\n",
       "dataset model model sampler         experiment                                                      \n",
       "TREC-2  BERT  BERT  core_set        0           [0.6748971193415638, 0.551440329218107, 0.5452...   \n",
       "                                    1           [0.6748971193415638, 0.49588477366255146, 0.56...   \n",
       "                                    2           [0.6152263374485597, 0.5061728395061729, 0.5, ...   \n",
       "                                    3           [0.5555555555555556, 0.49794238683127573, 0.49...   \n",
       "                                    4           [0.49176954732510286, 0.49382716049382713, 0.4...   \n",
       "                    dal             0           [0.6748971193415638, 0.6604938271604939, 0.666...   \n",
       "                                    1           [0.6748971193415638, 0.6893004115226338, 0.703...   \n",
       "                                    2           [0.6152263374485597, 0.7078189300411523, 0.713...   \n",
       "                                    3           [0.5555555555555556, 0.6851851851851852, 0.716...   \n",
       "                                    4           [0.49176954732510286, 0.654320987654321, 0.703...   \n",
       "                    entropy         0           [0.6748971193415638, 0.6728395061728395, 0.699...   \n",
       "                                    1           [0.6748971193415638, 0.6008230452674898, 0.707...   \n",
       "                                    2           [0.6152263374485597, 0.7119341563786008, 0.563...   \n",
       "                                    3           [0.5555555555555556, 0.7016460905349794, 0.646...   \n",
       "                                    4           [0.49176954732510286, 0.7160493827160493, 0.68...   \n",
       "                    entropy_dropout 0           [0.6748971193415638, 0.5267489711934157, 0.707...   \n",
       "                                    1           [0.6748971193415638, 0.6954732510288066, 0.520...   \n",
       "                                    2           [0.6152263374485597, 0.7222222222222222, 0.722...   \n",
       "                                    3           [0.5555555555555556, 0.6213991769547325, 0.724...   \n",
       "                                    4           [0.49176954732510286, 0.6625514403292181, 0.62...   \n",
       "                    random          0           [0.6748971193415638, 0.7098765432098766, 0.695...   \n",
       "                                    1           [0.6748971193415638, 0.5987654320987654, 0.730...   \n",
       "                                    2           [0.6152263374485597, 0.5534979423868313, 0.547...   \n",
       "                                    3           [0.5555555555555556, 0.6090534979423868, 0.730...   \n",
       "                                    4           [0.49176954732510286, 0.588477366255144, 0.722...   \n",
       "\n",
       "                                                                                         f1_micro  \\\n",
       "dataset model model sampler         experiment                                                      \n",
       "TREC-2  BERT  BERT  core_set        0           [0.6748971193415638, 0.551440329218107, 0.5452...   \n",
       "                                    1           [0.6748971193415638, 0.49588477366255146, 0.56...   \n",
       "                                    2           [0.6152263374485597, 0.5061728395061729, 0.5, ...   \n",
       "                                    3           [0.5555555555555556, 0.49794238683127573, 0.49...   \n",
       "                                    4           [0.49176954732510286, 0.49382716049382713, 0.4...   \n",
       "                    dal             0           [0.6748971193415638, 0.6604938271604939, 0.666...   \n",
       "                                    1           [0.6748971193415638, 0.6893004115226338, 0.703...   \n",
       "                                    2           [0.6152263374485597, 0.7078189300411522, 0.713...   \n",
       "                                    3           [0.5555555555555556, 0.6851851851851852, 0.716...   \n",
       "                                    4           [0.49176954732510286, 0.654320987654321, 0.703...   \n",
       "                    entropy         0           [0.6748971193415638, 0.6728395061728395, 0.699...   \n",
       "                                    1           [0.6748971193415638, 0.6008230452674898, 0.707...   \n",
       "                                    2           [0.6152263374485597, 0.7119341563786008, 0.563...   \n",
       "                                    3           [0.5555555555555556, 0.7016460905349794, 0.646...   \n",
       "                                    4           [0.49176954732510286, 0.7160493827160493, 0.68...   \n",
       "                    entropy_dropout 0           [0.6748971193415638, 0.5267489711934157, 0.707...   \n",
       "                                    1           [0.6748971193415638, 0.6954732510288066, 0.520...   \n",
       "                                    2           [0.6152263374485597, 0.7222222222222222, 0.722...   \n",
       "                                    3           [0.5555555555555556, 0.6213991769547325, 0.724...   \n",
       "                                    4           [0.49176954732510286, 0.6625514403292181, 0.62...   \n",
       "                    random          0           [0.6748971193415638, 0.7098765432098766, 0.695...   \n",
       "                                    1           [0.6748971193415638, 0.5987654320987654, 0.730...   \n",
       "                                    2           [0.6152263374485597, 0.5534979423868313, 0.547...   \n",
       "                                    3           [0.5555555555555556, 0.6090534979423868, 0.730...   \n",
       "                                    4           [0.49176954732510286, 0.588477366255144, 0.722...   \n",
       "\n",
       "                                                                                         f1_macro  \\\n",
       "dataset model model sampler         experiment                                                      \n",
       "TREC-2  BERT  BERT  core_set        0           [0.6739639945652174, 0.45776695054045197, 0.44...   \n",
       "                                    1           [0.6708333333333334, 0.3350830657545721, 0.486...   \n",
       "                                    2           [0.6121904696881121, 0.36681649403947625, 0.34...   \n",
       "                                    3           [0.4830601953986763, 0.33955622883621456, 0.33...   \n",
       "                                    4           [0.3296551724137931, 0.3305785123966942, 0.339...   \n",
       "                    dal             0           [0.6739639945652174, 0.6492540251151437, 0.648...   \n",
       "                                    1           [0.6708333333333334, 0.6868489888925396, 0.699...   \n",
       "                                    2           [0.6121904696881121, 0.7060216739367502, 0.706...   \n",
       "                                    3           [0.4830601953986763, 0.6752514510571208, 0.715...   \n",
       "                                    4           [0.3296551724137931, 0.6377318306859525, 0.703...   \n",
       "                    entropy         0           [0.6739639945652174, 0.6620084242018659, 0.698...   \n",
       "                                    1           [0.6708333333333334, 0.5784343533704148, 0.697...   \n",
       "                                    2           [0.6121904696881121, 0.6991989248262569, 0.490...   \n",
       "                                    3           [0.4830601953986763, 0.6958658996059679, 0.637...   \n",
       "                                    4           [0.3296551724137931, 0.7139127764127764, 0.689...   \n",
       "                    entropy_dropout 0           [0.6739639945652174, 0.425531914893617, 0.7068...   \n",
       "                                    1           [0.6708333333333334, 0.6904352017628426, 0.392...   \n",
       "                                    2           [0.6121904696881121, 0.7212296318327633, 0.712...   \n",
       "                                    3           [0.4830601953986763, 0.5993548387096774, 0.722...   \n",
       "                                    4           [0.3296551724137931, 0.6558133107629591, 0.598...   \n",
       "                    random          0           [0.6739639945652174, 0.7011604530171343, 0.691...   \n",
       "                                    1           [0.6708333333333334, 0.5652173913043478, 0.730...   \n",
       "                                    2           [0.6121904696881121, 0.48521723850107634, 0.46...   \n",
       "                                    3           [0.4830601953986763, 0.589527027027027, 0.7304...   \n",
       "                                    4           [0.3296551724137931, 0.5326293924182102, 0.721...   \n",
       "\n",
       "                                                                                         selected  \n",
       "dataset model model sampler         experiment                                                     \n",
       "TREC-2  BERT  BERT  core_set        0           [[1549, 1546, 1179, 111, 561, 1832, 619, 56, 5...  \n",
       "                                    1           [[998, 1179, 1018, 232, 1471, 696, 1089, 891, ...  \n",
       "                                    2           [[454, 565, 154, 1536, 1952, 959, 1490, 816, 1...  \n",
       "                                    3           [[1736, 11, 1693, 1079, 1304, 1144, 1330, 1452...  \n",
       "                                    4           [[1268, 33, 599, 432, 1073, 156, 1319, 589, 81...  \n",
       "                    dal             0           [[1549, 1546, 1179, 111, 561, 1832, 619, 56, 5...  \n",
       "                                    1           [[998, 1179, 1018, 232, 1471, 696, 1089, 891, ...  \n",
       "                                    2           [[454, 565, 154, 1536, 1952, 959, 1490, 816, 1...  \n",
       "                                    3           [[1736, 11, 1693, 1079, 1304, 1144, 1330, 1452...  \n",
       "                                    4           [[1268, 33, 599, 432, 1073, 156, 1319, 589, 81...  \n",
       "                    entropy         0           [[1549, 1546, 1179, 111, 561, 1832, 619, 56, 5...  \n",
       "                                    1           [[998, 1179, 1018, 232, 1471, 696, 1089, 891, ...  \n",
       "                                    2           [[454, 565, 154, 1536, 1952, 959, 1490, 816, 1...  \n",
       "                                    3           [[1736, 11, 1693, 1079, 1304, 1144, 1330, 1452...  \n",
       "                                    4           [[1268, 33, 599, 432, 1073, 156, 1319, 589, 81...  \n",
       "                    entropy_dropout 0           [[1549, 1546, 1179, 111, 561, 1832, 619, 56, 5...  \n",
       "                                    1           [[998, 1179, 1018, 232, 1471, 696, 1089, 891, ...  \n",
       "                                    2           [[454, 565, 154, 1536, 1952, 959, 1490, 816, 1...  \n",
       "                                    3           [[1736, 11, 1693, 1079, 1304, 1144, 1330, 1452...  \n",
       "                                    4           [[1268, 33, 599, 432, 1073, 156, 1319, 589, 81...  \n",
       "                    random          0           [[1549, 1546, 1179, 111, 561, 1832, 619, 56, 5...  \n",
       "                                    1           [[998, 1179, 1018, 232, 1471, 696, 1089, 891, ...  \n",
       "                                    2           [[454, 565, 154, 1536, 1952, 959, 1490, 816, 1...  \n",
       "                                    3           [[1736, 11, 1693, 1079, 1304, 1144, 1330, 1452...  \n",
       "                                    4           [[1268, 33, 599, 432, 1073, 156, 1319, 589, 81...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tr.groupby([\"dataset\", \"model\", \"model\", \"sampler\", \"experiment\"]).agg(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5aa2b72f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad4e86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr.groupby([\"dataset\", \"model\", \"model\", \"sampler\", \"experiment\"]).agg(list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78f7c53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "from podium import Vocab, Field, LabelField, Iterator  # , BucketIterator\n",
    "from podium.datasets import TabularDataset\n",
    "from podium.datasets.hf import HFDatasetConverter\n",
    "from podium.vectorizers import GloVe\n",
    "from podium.utils.general_utils import repr_type_and_attrs\n",
    "\n",
    "from typing import Iterator as PythonIterator\n",
    "from typing import NamedTuple, Tuple\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from util import Config\n",
    "\n",
    "\n",
    "class BucketIterator(Iterator):\n",
    "    \"\"\"\n",
    "    Creates a bucket iterator which uses a look-ahead heuristic to batch\n",
    "    examples in a way that minimizes the amount of necessary padding.\n",
    "\n",
    "    Uses a bucket of size N x batch_size, and sorts instances within the bucket\n",
    "    before splitting into batches, minimizing necessary padding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset=None,\n",
    "        batch_size=32,\n",
    "        sort_key=None,\n",
    "        shuffle=True,\n",
    "        seed=1,\n",
    "        matrix_class=np.array,\n",
    "        internal_random_state=None,\n",
    "        look_ahead_multiplier=100,\n",
    "        bucket_sort_key=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Creates a BucketIterator with the given bucket sort key and look-ahead\n",
    "        multiplier (how many batch_sizes to look ahead when sorting examples for\n",
    "        batches).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        look_ahead_multiplier : int\n",
    "            Multiplier of ``batch_size`` which determines the size of the\n",
    "            look-ahead bucket.\n",
    "            If ``look_ahead_multiplier == 1``, then the BucketIterator behaves\n",
    "            like a normal Iterator.\n",
    "            If ``look_ahead_multiplier >= (num_examples / batch_size)``, then\n",
    "            the BucketIterator behaves like a normal iterator that sorts the\n",
    "            whole dataset.\n",
    "            Default is ``100``.\n",
    "            The callable object used to sort examples in the bucket.\n",
    "            If ``bucket_sort_key=None``, then the ``sort_key`` must not be ``None``,\n",
    "            otherwise a ``ValueError`` is raised.\n",
    "            Default is ``None``.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If sort_key and bucket_sort_key are both None.\n",
    "        \"\"\"\n",
    "\n",
    "        if sort_key is None and bucket_sort_key is None:\n",
    "            raise ValueError(\n",
    "                \"For BucketIterator to work, either sort_key or \"\n",
    "                \"bucket_sort_key must be != None.\"\n",
    "            )\n",
    "\n",
    "        super().__init__(\n",
    "            dataset,\n",
    "            batch_size,\n",
    "            sort_key=sort_key,\n",
    "            shuffle=shuffle,\n",
    "            seed=seed,\n",
    "            matrix_class=matrix_class,\n",
    "            internal_random_state=internal_random_state,\n",
    "        )\n",
    "\n",
    "        self.bucket_sort_key = bucket_sort_key\n",
    "        self.look_ahead_multiplier = look_ahead_multiplier\n",
    "\n",
    "    def __iter__(self) -> PythonIterator[Tuple[NamedTuple, NamedTuple]]:\n",
    "        step = self.batch_size * self.look_ahead_multiplier\n",
    "        dataset = self._dataset\n",
    "\n",
    "        # Fix: Shuffle dataset if the shuffle is turned on, only IF sort key is not none\n",
    "        if self._shuffle and self._sort_key is None:\n",
    "            indices = list(range(len(dataset)))\n",
    "            # Cache state prior to shuffle so we can use it when unpickling\n",
    "            self._shuffler_state = self.get_internal_random_state()\n",
    "            self._shuffler.shuffle(indices)\n",
    "            # dataset.shuffle_examples(random_state=self._shuffler_state)\n",
    "            dataset = dataset[indices]\n",
    "\n",
    "        # Determine the step where iteration was stopped for lookahead & within bucket\n",
    "        lookahead_start = (\n",
    "            self.iterations // self.look_ahead_multiplier * self.look_ahead_multiplier\n",
    "        )\n",
    "        batch_start = self.iterations % self.look_ahead_multiplier\n",
    "\n",
    "        if self._sort_key is not None:\n",
    "            dataset = dataset.sorted(key=self._sort_key)\n",
    "        for i in range(lookahead_start, len(dataset), step):\n",
    "            bucket = dataset[i : i + step]\n",
    "\n",
    "            if self.bucket_sort_key is not None:\n",
    "                bucket = bucket.sorted(key=self.bucket_sort_key)\n",
    "\n",
    "            for j in range(batch_start, len(bucket), self.batch_size):\n",
    "                batch_dataset = bucket[j : j + self.batch_size]\n",
    "                batch = self._create_batch(batch_dataset)\n",
    "\n",
    "                yield batch\n",
    "                self._iterations += 1\n",
    "\n",
    "        # prepare for new epoch\n",
    "        self._iterations = 0\n",
    "        self._epoch += 1\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        attrs = {\n",
    "            \"batch_size\": self._batch_size,\n",
    "            \"epoch\": self._epoch,\n",
    "            \"iteration\": self._iterations,\n",
    "            \"shuffle\": self._shuffle,\n",
    "            \"look_ahead_multiplier\": self.look_ahead_multiplier,\n",
    "        }\n",
    "        return repr_type_and_attrs(self, attrs, with_newlines=True)\n",
    "\n",
    "\n",
    "class TokenizerVocabWrapper:\n",
    "    def __init__(self, tokenizer):\n",
    "        # wrap BertTokenizer so the method signatures align with podium\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def get_padding_index(self):\n",
    "        return self.tokenizer.convert_tokens_to_ids(self.tokenizer.pad_token)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenizer)\n",
    "\n",
    "    def numericalize(self, instance):\n",
    "        # Equivalent to .encode, but I want to delineate the steps\n",
    "        return self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(instance))\n",
    "\n",
    "\n",
    "def load_embeddings(vocab, name=\"glove\"):\n",
    "    if name == \"glove\":\n",
    "        glove = GloVe()\n",
    "        embeddings = glove.load_vocab(vocab)\n",
    "        return embeddings\n",
    "    else:\n",
    "        raise ValueError(f\"Wrong embedding key provided {name}\")\n",
    "\n",
    "\n",
    "def make_iterable(dataset, device, batch_size=32, train=False, indices=None):\n",
    "    \"\"\"\n",
    "    Construct a DataLoader from a podium Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def instance_length(instance):\n",
    "        raw, tokenized = instance.text\n",
    "        return -len(tokenized)\n",
    "\n",
    "    def cast_to_device(data):\n",
    "        return torch.tensor(np.array(data), device=device)\n",
    "\n",
    "    # Selects examples at given indices to support subset iteration.\n",
    "    if indices is not None:\n",
    "        dataset = dataset[indices]\n",
    "\n",
    "    # iterator = BucketIterator(\n",
    "    #     dataset,\n",
    "    #     batch_size=batch_size,\n",
    "    #     sort_key=instance_length,\n",
    "    #     shuffle=train,\n",
    "    #     matrix_class=cast_to_device,\n",
    "    #     look_ahead_multiplier=20,\n",
    "    # )\n",
    "\n",
    "    iterator = Iterator(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=train,\n",
    "        matrix_class=cast_to_device,\n",
    "    )\n",
    "\n",
    "    return iterator\n",
    "\n",
    "\n",
    "class Instance:\n",
    "    def __init__(self, index, text, label, extras=None):\n",
    "        self.index = index\n",
    "        self.text = text\n",
    "        self.label = label\n",
    "        self.extras = extras\n",
    "        self.length = len(text)  # text is already tokenized & filtered\n",
    "\n",
    "    def set_mask(self, masked_text, masked_labels):\n",
    "        # Set the masking as an attribute\n",
    "        self.masked_text = masked_text\n",
    "        self.masked_labels = masked_labels\n",
    "\n",
    "    def set_numericalized(self, indices, target):\n",
    "        self.numericalized_text = indices\n",
    "        self.numericalized_label = target\n",
    "        self.length = len(indices)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.index}: {self.length}, {self.label}\"\n",
    "\n",
    "\n",
    "def generate_eraser_rationale_mask(tokens, evidences):\n",
    "    mask = torch.zeros(len(tokens))  # zeros for where you can attend to\n",
    "\n",
    "    any_evidence_left = False\n",
    "    for ev in evidences:\n",
    "        if ev.start_token > len(tokens) or ev.end_token > len(tokens):\n",
    "            continue  # evidence out of span\n",
    "\n",
    "        if not any_evidence_left:\n",
    "            any_evidence_left = True\n",
    "        # 1. Validate\n",
    "\n",
    "        assert ev.text == \" \".join(\n",
    "            tokens[ev.start_token : ev.end_token]\n",
    "        ), \"Texts dont match; did you filter some tokens?\"\n",
    "\n",
    "        mask[ev.start_token : ev.end_token] = 1\n",
    "    return mask\n",
    "\n",
    "\n",
    "def load_tse(\n",
    "    train_path=\"data/TSE/train.csv\", test_path=\"data/TSE/test.csv\", max_size=20000\n",
    "):\n",
    "\n",
    "    vocab = Vocab(max_size=max_size)\n",
    "    fields = [\n",
    "        Field(\"id\", numericalizer=None),\n",
    "        Field(\"text\", numericalizer=vocab, include_lengths=True),\n",
    "        Field(\"rationale\", numericalizer=vocab),\n",
    "        LabelField(\"label\"),\n",
    "    ]\n",
    "    train_dataset = TabularDataset(\n",
    "        train_path, format=\"csv\", fields=fields, skip_header=True\n",
    "    )\n",
    "    test_dataset = TabularDataset(\n",
    "        test_path, format=\"csv\", fields=fields, skip_header=True\n",
    "    )\n",
    "    train_dataset.finalize_fields()\n",
    "    return (train_dataset, test_dataset), vocab\n",
    "\n",
    "\n",
    "class MaxLenHook:\n",
    "    def __init__(self, max_len):\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __call__(self, raw, tokenized):\n",
    "        return raw, tokenized[: self.max_len]\n",
    "\n",
    "\n",
    "def lowercase_hook(raw, tokenized):\n",
    "    return raw, [tok.lower() for tok in tokenized]\n",
    "\n",
    "\n",
    "def isalnum(token):\n",
    "    return any(c.isalnum() for c in token)\n",
    "\n",
    "\n",
    "def remove_nonalnum(raw, tokenized):\n",
    "    # Remove non alphanumeric tokens\n",
    "    return raw, [tok for tok in tokenized if isalnum(tok)]\n",
    "\n",
    "\n",
    "def load_imdb(\n",
    "    meta,\n",
    "    tokenizer=None,\n",
    "    max_vocab_size=20_000,\n",
    "    max_seq_len=200,\n",
    "):\n",
    "\n",
    "    return load_dataset(\n",
    "        \"data/IMDB\",\n",
    "        meta=meta,\n",
    "        tokenizer=tokenizer,\n",
    "        max_vocab_size=max_vocab_size,\n",
    "        max_seq_len=max_seq_len,\n",
    "    )\n",
    "\n",
    "\n",
    "def load_isear(\n",
    "    meta,\n",
    "    tokenizer=None,\n",
    "    max_vocab_size=20_000,\n",
    "    max_seq_len=200,\n",
    "):\n",
    "\n",
    "    return load_dataset(\n",
    "        \"data/ISEAR\",\n",
    "        meta=meta,\n",
    "        tokenizer=tokenizer,\n",
    "        max_vocab_size=max_vocab_size,\n",
    "        max_seq_len=max_seq_len,\n",
    "    )\n",
    "\n",
    "\n",
    "def load_agn2(\n",
    "    meta,\n",
    "    tokenizer=None,\n",
    "    max_vocab_size=20_000,\n",
    "    max_seq_len=200,\n",
    "):\n",
    "\n",
    "    return load_dataset(\n",
    "        \"data/AGN-2\",\n",
    "        meta=meta,\n",
    "        tokenizer=tokenizer,\n",
    "        max_vocab_size=max_vocab_size,\n",
    "        max_seq_len=max_seq_len,\n",
    "    )\n",
    "\n",
    "\n",
    "def load_agn4(\n",
    "    meta,\n",
    "    tokenizer=None,\n",
    "    max_vocab_size=20_000,\n",
    "    max_seq_len=200,\n",
    "):\n",
    "\n",
    "    return load_dataset(\n",
    "        \"data/AGN-4\",\n",
    "        meta=meta,\n",
    "        tokenizer=tokenizer,\n",
    "        max_vocab_size=max_vocab_size,\n",
    "        max_seq_len=max_seq_len,\n",
    "    )\n",
    "\n",
    "\n",
    "def load_mnli(\n",
    "    meta,\n",
    "    tokenizer=None,\n",
    "    max_vocab_size=20_000,\n",
    "    max_seq_len=200,\n",
    "):\n",
    "    return load_sequence_pair_dataset(\n",
    "        \"data/GLUE/MNLI\",\n",
    "        meta=meta,\n",
    "        tokenizer=tokenizer,\n",
    "        max_vocab_size=max_vocab_size,\n",
    "        max_seq_len=max_seq_len,\n",
    "    )\n",
    "\n",
    "\n",
    "def load_mrpc(\n",
    "    meta,\n",
    "    tokenizer=None,\n",
    "    max_vocab_size=20_000,\n",
    "    max_seq_len=200,\n",
    "):\n",
    "    return load_sequence_pair_dataset(\n",
    "        \"data/GLUE/MRPC\",\n",
    "        meta=meta,\n",
    "        tokenizer=tokenizer,\n",
    "        max_vocab_size=max_vocab_size,\n",
    "        max_seq_len=max_seq_len,\n",
    "    )\n",
    "\n",
    "\n",
    "def load_qqp(\n",
    "    meta,\n",
    "    tokenizer=None,\n",
    "    max_vocab_size=20_000,\n",
    "    max_seq_len=200,\n",
    "):\n",
    "    return load_sequence_pair_dataset(\n",
    "        \"data/GLUE/QQP\",\n",
    "        meta=meta,\n",
    "        tokenizer=tokenizer,\n",
    "        max_vocab_size=max_vocab_size,\n",
    "        max_seq_len=max_seq_len,\n",
    "    )\n",
    "\n",
    "\n",
    "def test_load_cola(meta, tok):\n",
    "    splits, vocab = load_cola(meta, tok)\n",
    "    print(vocab)\n",
    "    train, valid, test = splits\n",
    "    print(len(train), len(valid), len(test))\n",
    "\n",
    "    print(train)\n",
    "    print(train[0])\n",
    "\n",
    "    device = torch.device(\"cpu\")\n",
    "    train_iter = make_iterable(test, device, batch_size=2)\n",
    "    batch = next(iter(train_iter))\n",
    "\n",
    "    print(batch)\n",
    "    text, length = batch.text\n",
    "    print(length[0])\n",
    "    print(vocab.get_padding_index())\n",
    "\n",
    "\n",
    "def load_sequence_pair_dataset(\n",
    "    data_dir, meta, tokenizer=None, max_vocab_size=20_000, max_seq_len=200\n",
    "):\n",
    "\n",
    "    # Use BERT subword tokenization\n",
    "    vocab = TokenizerVocabWrapper(tokenizer)\n",
    "    print(vocab.get_padding_index())\n",
    "    pad_index = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "    fields = [\n",
    "        Field(\"id\", disable_batch_matrix=True),\n",
    "        Field(\n",
    "            \"sequence1\",\n",
    "            tokenizer=tokenizer.tokenize,\n",
    "            padding_token=pad_index,\n",
    "            numericalizer=tokenizer.convert_tokens_to_ids,\n",
    "            include_lengths=True,\n",
    "            posttokenize_hooks=[\n",
    "                remove_nonalnum,\n",
    "                MaxLenHook(max_seq_len),\n",
    "                lowercase_hook,\n",
    "            ],\n",
    "        ),\n",
    "        Field(\n",
    "            \"sequence2\",\n",
    "            tokenizer=tokenizer.tokenize,\n",
    "            padding_token=pad_index,\n",
    "            numericalizer=tokenizer.convert_tokens_to_ids,\n",
    "            include_lengths=True,\n",
    "            posttokenize_hooks=[\n",
    "                remove_nonalnum,\n",
    "                MaxLenHook(max_seq_len),\n",
    "                lowercase_hook,\n",
    "            ],\n",
    "        ),\n",
    "        LabelField(\"label\"),\n",
    "    ]\n",
    "\n",
    "    train = TabularDataset(\n",
    "        os.path.join(data_dir, \"train.csv\"), format=\"csv\", fields=fields\n",
    "    )\n",
    "    val = TabularDataset(\n",
    "        os.path.join(data_dir, \"validation.csv\"), format=\"csv\", fields=fields\n",
    "    )\n",
    "    test = TabularDataset(\n",
    "        os.path.join(data_dir, \"test.csv\"), format=\"csv\", fields=fields\n",
    "    )\n",
    "\n",
    "    train.finalize_fields()\n",
    "\n",
    "    meta.vocab = vocab\n",
    "    meta.num_tokens = len(vocab)\n",
    "    meta.padding_idx = vocab.get_padding_index()\n",
    "    meta.num_labels = len(train.field(\"label\").vocab)\n",
    "\n",
    "    return (train, val, test), vocab\n",
    "\n",
    "\n",
    "def load_dataset(\n",
    "    data_dir, meta, tokenizer=None, max_vocab_size=20_000, max_seq_len=200\n",
    "):\n",
    "\n",
    "    # Use BERT subword tokenization\n",
    "    vocab = TokenizerVocabWrapper(tokenizer)\n",
    "    pad_index = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "    fields = [\n",
    "        Field(\"id\", disable_batch_matrix=True),\n",
    "        Field(\n",
    "            \"text\",\n",
    "            tokenizer=tokenizer.tokenize,\n",
    "            padding_token=pad_index,\n",
    "            numericalizer=tokenizer.convert_tokens_to_ids,\n",
    "            include_lengths=True,\n",
    "            posttokenize_hooks=[\n",
    "                remove_nonalnum,\n",
    "                MaxLenHook(max_seq_len),\n",
    "                lowercase_hook,\n",
    "            ],\n",
    "        ),\n",
    "#         LabelField(\"label\"),\n",
    "    ]\n",
    "\n",
    "    train = TabularDataset(\n",
    "        os.path.join(data_dir, \"train.csv\"), format=\"csv\", fields=fields\n",
    "    )\n",
    "    val = TabularDataset(\n",
    "        os.path.join(data_dir, \"validation.csv\"), format=\"csv\", fields=fields\n",
    "    )\n",
    "    test = TabularDataset(\n",
    "        os.path.join(data_dir, \"test.csv\"), format=\"csv\", fields=fields\n",
    "    )\n",
    "\n",
    "    train.finalize_fields()\n",
    "\n",
    "    meta.vocab = vocab\n",
    "    meta.num_tokens = len(vocab)\n",
    "    meta.padding_idx = vocab.get_padding_index()\n",
    "    meta.num_labels = len(train.field(\"label\").vocab)\n",
    "\n",
    "    return (train, val, test), vocab\n",
    "\n",
    "\n",
    "def load_sst(\n",
    "    meta,\n",
    "    tokenizer=None,\n",
    "    max_vocab_size=20_000,\n",
    "    max_seq_len=200,\n",
    "):\n",
    "    return load_dataset(\n",
    "        \"data/GLUE/SST-2\",\n",
    "        meta=meta,\n",
    "        tokenizer=tokenizer,\n",
    "        max_vocab_size=max_vocab_size,\n",
    "        max_seq_len=max_seq_len,\n",
    "    )\n",
    "\n",
    "\n",
    "def test_load_sst(max_vocab_size=20_000, max_seq_len=200):\n",
    "    splits, vocab = load_sst()\n",
    "    print(vocab)\n",
    "    train, valid, test = splits\n",
    "    print(len(train), len(valid), len(test))\n",
    "\n",
    "    print(train)\n",
    "    print(train[0])\n",
    "\n",
    "    device = torch.device(\"cpu\")\n",
    "    train_iter = make_iterable(train, device, batch_size=2)\n",
    "    batch = next(iter(train_iter))\n",
    "\n",
    "    print(batch)\n",
    "    text, length = batch.text\n",
    "    print(vocab.reverse_numericalize(text[0]))\n",
    "    print(length[0])\n",
    "    print(vocab.get_padding_index())\n",
    "\n",
    "\n",
    "def load_trec2(\n",
    "    meta,\n",
    "    tokenizer=None,\n",
    "    max_vocab_size=20_000,\n",
    "    max_seq_len=200,\n",
    "):\n",
    "\n",
    "    return load_dataset(\n",
    "        \"data/TREC-2\",\n",
    "        meta=meta,\n",
    "        tokenizer=tokenizer,\n",
    "        max_vocab_size=max_vocab_size,\n",
    "        max_seq_len=max_seq_len,\n",
    "    )\n",
    "\n",
    "\n",
    "def load_trec6(\n",
    "    meta,\n",
    "    tokenizer=None,\n",
    "    max_vocab_size=20_000,\n",
    "    max_seq_len=200,\n",
    "):\n",
    "\n",
    "    return load_dataset(\n",
    "        \"data/TREC-6\",\n",
    "        meta=meta,\n",
    "        tokenizer=tokenizer,\n",
    "        max_vocab_size=max_vocab_size,\n",
    "        max_seq_len=max_seq_len,\n",
    "    )\n",
    "\n",
    "\n",
    "def load_cola(\n",
    "    meta,\n",
    "    tokenizer=None,\n",
    "    max_vocab_size=20_000,\n",
    "    max_seq_len=200,\n",
    "):\n",
    "\n",
    "    return load_dataset(\n",
    "        \"data/GLUE/COLA\",\n",
    "        meta=meta,\n",
    "        tokenizer=tokenizer,\n",
    "        max_vocab_size=max_vocab_size,\n",
    "        max_seq_len=max_seq_len,\n",
    "    )\n",
    "\n",
    "\n",
    "def load_polarity(\n",
    "    meta,\n",
    "    tokenizer=None,\n",
    "    max_vocab_size=20_000,\n",
    "    max_seq_len=200,\n",
    "):\n",
    "\n",
    "    return load_dataset(\n",
    "        \"data/POL\",\n",
    "        meta=meta,\n",
    "        tokenizer=tokenizer,\n",
    "        max_vocab_size=max_vocab_size,\n",
    "        max_seq_len=max_seq_len,\n",
    "    )\n",
    "\n",
    "\n",
    "def load_subj(\n",
    "    meta,\n",
    "    tokenizer=None,\n",
    "    max_vocab_size=20_000,\n",
    "    max_seq_len=200,\n",
    "):\n",
    "\n",
    "    return load_dataset(\n",
    "        \"data/SUBJ\",\n",
    "        meta=meta,\n",
    "        tokenizer=tokenizer,\n",
    "        max_vocab_size=max_vocab_size,\n",
    "        max_seq_len=max_seq_len,\n",
    "    )\n",
    "\n",
    "\n",
    "def load_trec_hf(label=\"label-coarse\", max_vocab_size=20_000, max_seq_len=200):\n",
    "    vocab = Vocab(max_size=max_vocab_size)\n",
    "    fields = [\n",
    "        Field(\n",
    "            \"text\",\n",
    "            numericalizer=vocab,\n",
    "            include_lengths=True,\n",
    "            posttokenize_hooks=[MaxLenHook(max_seq_len)],\n",
    "            keep_raw=True,\n",
    "        ),\n",
    "        LabelField(\"label\"),\n",
    "    ]\n",
    "    hf_dataset = load_dataset(\"trec\")\n",
    "    hf_dataset = hf_dataset.rename_column(label, \"label\")\n",
    "    print(hf_dataset)\n",
    "    hf_train_val, hf_test = (\n",
    "        hf_dataset[\"train\"],\n",
    "        hf_dataset[\"test\"],\n",
    "    )\n",
    "    train_val_conv = HFDatasetConverter(hf_train_val, fields=fields)\n",
    "    test_conv = HFDatasetConverter(hf_test, fields=fields)\n",
    "    train_val, test = (\n",
    "        train_val_conv.as_dataset(),\n",
    "        test_conv.as_dataset(),\n",
    "    )\n",
    "    train, val = train_val.split(split_ratio=0.8, random_state=0)\n",
    "    train.finalize_fields()\n",
    "    print(train)\n",
    "    return (train, val, test), vocab\n",
    "\n",
    "\n",
    "def add_ids_to_files(root_folder):\n",
    "    split_ins = [\"train_old.csv\", \"dev_old.csv\", \"test_old.csv\"]\n",
    "    split_outs = [\"train.csv\", \"dev.csv\", \"test.csv\"]\n",
    "\n",
    "    for split_in, split_out in zip(split_ins, split_outs):\n",
    "        with open(os.path.join(root_folder, split_in), \"r\") as infile:\n",
    "            with open(os.path.join(root_folder, split_out), \"w\") as outfile:\n",
    "                for idx, line in enumerate(infile):\n",
    "                    parts = line.strip().split(\",\")\n",
    "                    if idx == 0:\n",
    "                        continue\n",
    "                    outfile.write(f\"{idx-1},{parts[0]},{parts[1]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58f7155d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "from args import *\n",
    "\n",
    "args = Config()\n",
    "args.lr = 2e-5\n",
    "args.l2 = 0\n",
    "args.model = \"BERT\"\n",
    "args.data = \"COLA\"\n",
    "args.adapter = \"unipelt\"\n",
    "args.batch_size = 32\n",
    "args.epochs = 10\n",
    "args.clip = 1\n",
    "\n",
    "meta = Config()\n",
    "\n",
    "dataloader = dataset_loaders[args.data]\n",
    "tokenizer = AutoTokenizer.from_pretrained(TRANSFORMERS[args.model])\n",
    "(train, val, test), vocab = dataloader(meta=meta, tokenizer=tokenizer)\n",
    "\n",
    "if args.data in pair_sequence_datasets:\n",
    "    meta.pair_sequence = True\n",
    "else:\n",
    "    meta.pair_sequence = False\n",
    "\n",
    "if meta.num_labels == 2:\n",
    "    # Binary classification\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    meta.num_targets = 1\n",
    "else:\n",
    "    # Multiclass classification\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    meta.num_targets = meta.num_labels\n",
    "    \n",
    "model = Transformer(args, meta, args.model, adapter=args.adapter)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def make_iterable(dataset, device, batch_size=32, train=False, indices=None):\n",
    "    \"\"\"\n",
    "    Construct a DataLoader from a podium Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def instance_length(instance):\n",
    "        raw, tokenized = instance.text\n",
    "        return -len(tokenized)\n",
    "\n",
    "    def cast_to_device(data):\n",
    "        return torch.tensor(np.array(data), device=device)\n",
    "\n",
    "    # Selects examples at given indices to support subset iteration.\n",
    "    if indices is not None:\n",
    "        dataset = dataset[indices]\n",
    "\n",
    "    iterator = BucketIterator(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        sort_key=instance_length,\n",
    "        shuffle=train,\n",
    "        matrix_class=cast_to_device,\n",
    "        look_ahead_multiplier=20,\n",
    "    )\n",
    "\n",
    "    return iterator\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "train_iter = make_iterable(\n",
    "    train,\n",
    "    device,\n",
    "    batch_size=args.batch_size,\n",
    "    train=True,\n",
    "#     indices=indices,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b948087d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (classifier): BertAdapterModel(\n",
       "    (shared_parameters): ModuleDict()\n",
       "    (bert): BertModel(\n",
       "      (shared_parameters): ModuleDict()\n",
       "      (invertible_adapters): ModuleDict()\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict(\n",
       "                    (COLA): LoRA(\n",
       "                      (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (key): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (value): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict(\n",
       "                    (COLA): LoRA(\n",
       "                      (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (prefix_tuning): PrefixTuningShim(\n",
       "                  (prefix_gates): ModuleDict(\n",
       "                    (COLA): Linear(in_features=768, out_features=1, bias=True)\n",
       "                  )\n",
       "                  (pool): PrefixTuningPool(\n",
       "                    (prefix_tunings): ModuleDict(\n",
       "                      (COLA): PrefixTuningGroup(\n",
       "                        (self_prefix): PrefixTuning(\n",
       "                          (wte): Embedding(10, 768)\n",
       "                          (control_trans): Sequential(\n",
       "                            (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "                            (1): Activation_Function_Class(\n",
       "                              (f): Tanh()\n",
       "                            )\n",
       "                            (2): Linear(in_features=512, out_features=18432, bias=True)\n",
       "                          )\n",
       "                          (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        )\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (adapters): ModuleDict()\n",
       "                (adapter_fusion_layer): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(\n",
       "                in_features=768, out_features=3072, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(\n",
       "                in_features=3072, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict(\n",
       "                (COLA): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (1): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict(\n",
       "                    (COLA): LoRA(\n",
       "                      (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (key): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (value): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict(\n",
       "                    (COLA): LoRA(\n",
       "                      (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (prefix_tuning): PrefixTuningShim(\n",
       "                  (prefix_gates): ModuleDict(\n",
       "                    (COLA): Linear(in_features=768, out_features=1, bias=True)\n",
       "                  )\n",
       "                  (pool): PrefixTuningPool(\n",
       "                    (prefix_tunings): ModuleDict(\n",
       "                      (COLA): PrefixTuningGroup(\n",
       "                        (self_prefix): PrefixTuning(\n",
       "                          (wte): Embedding(10, 768)\n",
       "                          (control_trans): Sequential(\n",
       "                            (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "                            (1): Activation_Function_Class(\n",
       "                              (f): Tanh()\n",
       "                            )\n",
       "                            (2): Linear(in_features=512, out_features=18432, bias=True)\n",
       "                          )\n",
       "                          (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        )\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (adapters): ModuleDict()\n",
       "                (adapter_fusion_layer): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(\n",
       "                in_features=768, out_features=3072, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(\n",
       "                in_features=3072, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict(\n",
       "                (COLA): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (2): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict(\n",
       "                    (COLA): LoRA(\n",
       "                      (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (key): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (value): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict(\n",
       "                    (COLA): LoRA(\n",
       "                      (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (prefix_tuning): PrefixTuningShim(\n",
       "                  (prefix_gates): ModuleDict(\n",
       "                    (COLA): Linear(in_features=768, out_features=1, bias=True)\n",
       "                  )\n",
       "                  (pool): PrefixTuningPool(\n",
       "                    (prefix_tunings): ModuleDict(\n",
       "                      (COLA): PrefixTuningGroup(\n",
       "                        (self_prefix): PrefixTuning(\n",
       "                          (wte): Embedding(10, 768)\n",
       "                          (control_trans): Sequential(\n",
       "                            (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "                            (1): Activation_Function_Class(\n",
       "                              (f): Tanh()\n",
       "                            )\n",
       "                            (2): Linear(in_features=512, out_features=18432, bias=True)\n",
       "                          )\n",
       "                          (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        )\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (adapters): ModuleDict()\n",
       "                (adapter_fusion_layer): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(\n",
       "                in_features=768, out_features=3072, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(\n",
       "                in_features=3072, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict(\n",
       "                (COLA): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (3): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict(\n",
       "                    (COLA): LoRA(\n",
       "                      (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (key): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (value): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict(\n",
       "                    (COLA): LoRA(\n",
       "                      (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (prefix_tuning): PrefixTuningShim(\n",
       "                  (prefix_gates): ModuleDict(\n",
       "                    (COLA): Linear(in_features=768, out_features=1, bias=True)\n",
       "                  )\n",
       "                  (pool): PrefixTuningPool(\n",
       "                    (prefix_tunings): ModuleDict(\n",
       "                      (COLA): PrefixTuningGroup(\n",
       "                        (self_prefix): PrefixTuning(\n",
       "                          (wte): Embedding(10, 768)\n",
       "                          (control_trans): Sequential(\n",
       "                            (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "                            (1): Activation_Function_Class(\n",
       "                              (f): Tanh()\n",
       "                            )\n",
       "                            (2): Linear(in_features=512, out_features=18432, bias=True)\n",
       "                          )\n",
       "                          (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        )\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (adapters): ModuleDict()\n",
       "                (adapter_fusion_layer): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(\n",
       "                in_features=768, out_features=3072, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(\n",
       "                in_features=3072, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict(\n",
       "                (COLA): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (4): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict(\n",
       "                    (COLA): LoRA(\n",
       "                      (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (key): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (value): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict(\n",
       "                    (COLA): LoRA(\n",
       "                      (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (prefix_tuning): PrefixTuningShim(\n",
       "                  (prefix_gates): ModuleDict(\n",
       "                    (COLA): Linear(in_features=768, out_features=1, bias=True)\n",
       "                  )\n",
       "                  (pool): PrefixTuningPool(\n",
       "                    (prefix_tunings): ModuleDict(\n",
       "                      (COLA): PrefixTuningGroup(\n",
       "                        (self_prefix): PrefixTuning(\n",
       "                          (wte): Embedding(10, 768)\n",
       "                          (control_trans): Sequential(\n",
       "                            (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "                            (1): Activation_Function_Class(\n",
       "                              (f): Tanh()\n",
       "                            )\n",
       "                            (2): Linear(in_features=512, out_features=18432, bias=True)\n",
       "                          )\n",
       "                          (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        )\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (adapters): ModuleDict()\n",
       "                (adapter_fusion_layer): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(\n",
       "                in_features=768, out_features=3072, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(\n",
       "                in_features=3072, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict(\n",
       "                (COLA): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (5): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict(\n",
       "                    (COLA): LoRA(\n",
       "                      (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (key): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (value): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict(\n",
       "                    (COLA): LoRA(\n",
       "                      (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (prefix_tuning): PrefixTuningShim(\n",
       "                  (prefix_gates): ModuleDict(\n",
       "                    (COLA): Linear(in_features=768, out_features=1, bias=True)\n",
       "                  )\n",
       "                  (pool): PrefixTuningPool(\n",
       "                    (prefix_tunings): ModuleDict(\n",
       "                      (COLA): PrefixTuningGroup(\n",
       "                        (self_prefix): PrefixTuning(\n",
       "                          (wte): Embedding(10, 768)\n",
       "                          (control_trans): Sequential(\n",
       "                            (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "                            (1): Activation_Function_Class(\n",
       "                              (f): Tanh()\n",
       "                            )\n",
       "                            (2): Linear(in_features=512, out_features=18432, bias=True)\n",
       "                          )\n",
       "                          (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        )\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (adapters): ModuleDict()\n",
       "                (adapter_fusion_layer): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(\n",
       "                in_features=768, out_features=3072, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(\n",
       "                in_features=3072, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict(\n",
       "                (COLA): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (6): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict(\n",
       "                    (COLA): LoRA(\n",
       "                      (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (key): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (value): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict(\n",
       "                    (COLA): LoRA(\n",
       "                      (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (prefix_tuning): PrefixTuningShim(\n",
       "                  (prefix_gates): ModuleDict(\n",
       "                    (COLA): Linear(in_features=768, out_features=1, bias=True)\n",
       "                  )\n",
       "                  (pool): PrefixTuningPool(\n",
       "                    (prefix_tunings): ModuleDict(\n",
       "                      (COLA): PrefixTuningGroup(\n",
       "                        (self_prefix): PrefixTuning(\n",
       "                          (wte): Embedding(10, 768)\n",
       "                          (control_trans): Sequential(\n",
       "                            (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "                            (1): Activation_Function_Class(\n",
       "                              (f): Tanh()\n",
       "                            )\n",
       "                            (2): Linear(in_features=512, out_features=18432, bias=True)\n",
       "                          )\n",
       "                          (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        )\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (adapters): ModuleDict()\n",
       "                (adapter_fusion_layer): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(\n",
       "                in_features=768, out_features=3072, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(\n",
       "                in_features=3072, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict(\n",
       "                (COLA): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (7): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict(\n",
       "                    (COLA): LoRA(\n",
       "                      (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (key): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (value): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict(\n",
       "                    (COLA): LoRA(\n",
       "                      (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (prefix_tuning): PrefixTuningShim(\n",
       "                  (prefix_gates): ModuleDict(\n",
       "                    (COLA): Linear(in_features=768, out_features=1, bias=True)\n",
       "                  )\n",
       "                  (pool): PrefixTuningPool(\n",
       "                    (prefix_tunings): ModuleDict(\n",
       "                      (COLA): PrefixTuningGroup(\n",
       "                        (self_prefix): PrefixTuning(\n",
       "                          (wte): Embedding(10, 768)\n",
       "                          (control_trans): Sequential(\n",
       "                            (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "                            (1): Activation_Function_Class(\n",
       "                              (f): Tanh()\n",
       "                            )\n",
       "                            (2): Linear(in_features=512, out_features=18432, bias=True)\n",
       "                          )\n",
       "                          (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        )\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (adapters): ModuleDict()\n",
       "                (adapter_fusion_layer): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(\n",
       "                in_features=768, out_features=3072, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(\n",
       "                in_features=3072, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict(\n",
       "                (COLA): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (8): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict(\n",
       "                    (COLA): LoRA(\n",
       "                      (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (key): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (value): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict(\n",
       "                    (COLA): LoRA(\n",
       "                      (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (prefix_tuning): PrefixTuningShim(\n",
       "                  (prefix_gates): ModuleDict(\n",
       "                    (COLA): Linear(in_features=768, out_features=1, bias=True)\n",
       "                  )\n",
       "                  (pool): PrefixTuningPool(\n",
       "                    (prefix_tunings): ModuleDict(\n",
       "                      (COLA): PrefixTuningGroup(\n",
       "                        (self_prefix): PrefixTuning(\n",
       "                          (wte): Embedding(10, 768)\n",
       "                          (control_trans): Sequential(\n",
       "                            (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "                            (1): Activation_Function_Class(\n",
       "                              (f): Tanh()\n",
       "                            )\n",
       "                            (2): Linear(in_features=512, out_features=18432, bias=True)\n",
       "                          )\n",
       "                          (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        )\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (adapters): ModuleDict()\n",
       "                (adapter_fusion_layer): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(\n",
       "                in_features=768, out_features=3072, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(\n",
       "                in_features=3072, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict(\n",
       "                (COLA): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (9): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict(\n",
       "                    (COLA): LoRA(\n",
       "                      (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (key): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (value): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict(\n",
       "                    (COLA): LoRA(\n",
       "                      (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (prefix_tuning): PrefixTuningShim(\n",
       "                  (prefix_gates): ModuleDict(\n",
       "                    (COLA): Linear(in_features=768, out_features=1, bias=True)\n",
       "                  )\n",
       "                  (pool): PrefixTuningPool(\n",
       "                    (prefix_tunings): ModuleDict(\n",
       "                      (COLA): PrefixTuningGroup(\n",
       "                        (self_prefix): PrefixTuning(\n",
       "                          (wte): Embedding(10, 768)\n",
       "                          (control_trans): Sequential(\n",
       "                            (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "                            (1): Activation_Function_Class(\n",
       "                              (f): Tanh()\n",
       "                            )\n",
       "                            (2): Linear(in_features=512, out_features=18432, bias=True)\n",
       "                          )\n",
       "                          (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        )\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (adapters): ModuleDict()\n",
       "                (adapter_fusion_layer): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(\n",
       "                in_features=768, out_features=3072, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(\n",
       "                in_features=3072, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict(\n",
       "                (COLA): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (10): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict(\n",
       "                    (COLA): LoRA(\n",
       "                      (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (key): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (value): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict(\n",
       "                    (COLA): LoRA(\n",
       "                      (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (prefix_tuning): PrefixTuningShim(\n",
       "                  (prefix_gates): ModuleDict(\n",
       "                    (COLA): Linear(in_features=768, out_features=1, bias=True)\n",
       "                  )\n",
       "                  (pool): PrefixTuningPool(\n",
       "                    (prefix_tunings): ModuleDict(\n",
       "                      (COLA): PrefixTuningGroup(\n",
       "                        (self_prefix): PrefixTuning(\n",
       "                          (wte): Embedding(10, 768)\n",
       "                          (control_trans): Sequential(\n",
       "                            (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "                            (1): Activation_Function_Class(\n",
       "                              (f): Tanh()\n",
       "                            )\n",
       "                            (2): Linear(in_features=512, out_features=18432, bias=True)\n",
       "                          )\n",
       "                          (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        )\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (adapters): ModuleDict()\n",
       "                (adapter_fusion_layer): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(\n",
       "                in_features=768, out_features=3072, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(\n",
       "                in_features=3072, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict(\n",
       "                (COLA): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (11): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict(\n",
       "                    (COLA): LoRA(\n",
       "                      (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (key): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (value): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict(\n",
       "                    (COLA): LoRA(\n",
       "                      (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (prefix_tuning): PrefixTuningShim(\n",
       "                  (prefix_gates): ModuleDict(\n",
       "                    (COLA): Linear(in_features=768, out_features=1, bias=True)\n",
       "                  )\n",
       "                  (pool): PrefixTuningPool(\n",
       "                    (prefix_tunings): ModuleDict(\n",
       "                      (COLA): PrefixTuningGroup(\n",
       "                        (self_prefix): PrefixTuning(\n",
       "                          (wte): Embedding(10, 768)\n",
       "                          (control_trans): Sequential(\n",
       "                            (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "                            (1): Activation_Function_Class(\n",
       "                              (f): Tanh()\n",
       "                            )\n",
       "                            (2): Linear(in_features=512, out_features=18432, bias=True)\n",
       "                          )\n",
       "                          (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        )\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (adapters): ModuleDict()\n",
       "                (adapter_fusion_layer): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(\n",
       "                in_features=768, out_features=3072, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(\n",
       "                in_features=3072, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict(\n",
       "                (COLA): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "      (prefix_tuning): PrefixTuningPool(\n",
       "        (prefix_tunings): ModuleDict(\n",
       "          (COLA): PrefixTuningGroup(\n",
       "            (self_prefix): PrefixTuning(\n",
       "              (wte): Embedding(10, 768)\n",
       "              (control_trans): Sequential(\n",
       "                (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "                (1): Activation_Function_Class(\n",
       "                  (f): Tanh()\n",
       "                )\n",
       "                (2): Linear(in_features=512, out_features=18432, bias=True)\n",
       "              )\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (heads): ModuleDict(\n",
       "      (COLA): ClassificationHead(\n",
       "        (0): Dropout(p=0.1, inplace=False)\n",
       "        (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (2): Activation_Function_Class(\n",
       "          (f): Tanh()\n",
       "        )\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "        (4): Linear(in_features=768, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a11544a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Padding symbol is not in the vocabulary.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1664478/1352271905.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/adapter-al/dataloaders.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0mbatch_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbucket\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gen/lib/python3.9/site-packages/podium/datasets/iterator.py\u001b[0m in \u001b[0;36m_create_batch\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    366\u001b[0m                 \u001b[0;32mand\u001b[0m \u001b[0mpossible_cast_to_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m             ):\n\u001b[0;32m--> 368\u001b[0;31m                 batch = Iterator._arrays_to_matrix(\n\u001b[0m\u001b[1;32m    369\u001b[0m                     \u001b[0mfield\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumericalizations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix_class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                 )\n",
      "\u001b[0;32m~/.conda/envs/gen/lib/python3.9/site-packages/podium/datasets/iterator.py\u001b[0m in \u001b[0;36m_arrays_to_matrix\u001b[0;34m(field, arrays, matrix_class)\u001b[0m\n\u001b[1;32m    438\u001b[0m     ) -> np.ndarray:\n\u001b[1;32m    439\u001b[0m         \u001b[0mpad_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_pad_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0mpadded_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pad_to_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_length\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmatrix_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_arrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gen/lib/python3.9/site-packages/podium/datasets/iterator.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    438\u001b[0m     ) -> np.ndarray:\n\u001b[1;32m    439\u001b[0m         \u001b[0mpad_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_pad_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0mpadded_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pad_to_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_length\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmatrix_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_arrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gen/lib/python3.9/site-packages/podium/field.py\u001b[0m in \u001b[0;36m_pad_to_length\u001b[0;34m(self, array, length, custom_pad_symbol, pad_left, truncate_left)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_vocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m                 \u001b[0mpad_symbol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_padding_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gen/lib/python3.9/site-packages/podium/vocab.py\u001b[0m in \u001b[0;36mget_padding_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \"\"\"\n\u001b[1;32m    342\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mVocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pad\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Padding symbol is not in the vocabulary.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mVocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pad\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Padding symbol is not in the vocabulary."
     ]
    }
   ],
   "source": [
    "for i in train_iter:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea71c157",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.fields[1].vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd1a6909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from al.experiment import Experiment\n",
    "\n",
    "\n",
    "def train_model(model, optimizer, criterion, train_iter):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    accuracy, confusion_matrix = 0, np.zeros(\n",
    "        (meta.num_labels, meta.num_labels), dtype=int\n",
    "    )\n",
    "\n",
    "    logit_list = []\n",
    "    y_true_list = []\n",
    "    ids = []\n",
    "    for batch_num, batch in enumerate(train_iter, 1):\n",
    "        t = time.time()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ids.extend([int(id[0]) for id in batch.id])\n",
    "\n",
    "        # Unpack batch & cast to device\n",
    "        if meta.pair_sequence:\n",
    "            (x_sequence1, sequence1_lengths) = batch.sequence1\n",
    "            (x_sequence2, sequence2_lengths) = batch.sequence2\n",
    "        else:\n",
    "            (x, lengths) = batch.text\n",
    "\n",
    "        y = batch.label\n",
    "        y_true_list.append(y.squeeze(0) if y.numel() == 1 else y.squeeze())\n",
    "\n",
    "        if meta.pair_sequence:\n",
    "            # PSQ\n",
    "            lengths = (sequence1_lengths, sequence2_lengths)\n",
    "            logits, return_dict = model(x_sequence1, x_sequence2, lengths)\n",
    "        else:\n",
    "            # SSQ\n",
    "            logits, return_dict = model(x, lengths)\n",
    "        logit_list.append(logits)\n",
    "\n",
    "        # Bookkeeping and cast label to float\n",
    "        accuracy, confusion_matrix = Experiment.update_stats(\n",
    "            accuracy, confusion_matrix, logits, y\n",
    "        )\n",
    "        if logits.shape[-1] == 1:\n",
    "            # binary cross entropy, cast labels to float\n",
    "            y = y.type(torch.float)\n",
    "\n",
    "        loss = criterion(logits.view(-1, meta.num_targets).squeeze(), y.squeeze())\n",
    "\n",
    "        total_loss += float(loss)\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        print(\n",
    "            \"[Batch]: {}/{} in {:.5f} seconds\".format(\n",
    "                batch_num, len(train_iter), time.time() - t\n",
    "            ),\n",
    "            end=\"\\r\",\n",
    "            flush=True,\n",
    "        )\n",
    "\n",
    "    loss = total_loss / len(train_iter)\n",
    "    result_dict = {\"loss\": loss}\n",
    "    logit_tensor = torch.cat(logit_list)\n",
    "    y_true = torch.cat(y_true_list)\n",
    "    return result_dict, logit_tensor, y_true, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "022001ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Padding symbol is not in the vocabulary.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1441522/3166091545.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training epoch: {epoch}/{args.epochs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# a) Train for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     result_dict_train, logits, y_true, ids = train_model(\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     )\n",
      "\u001b[0;32m/tmp/ipykernel_1441522/1683375134.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, criterion, train_iter)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0my_true_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/adapter-al/dataloaders.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0mbatch_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbucket\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gen/lib/python3.9/site-packages/podium/datasets/iterator.py\u001b[0m in \u001b[0;36m_create_batch\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    366\u001b[0m                 \u001b[0;32mand\u001b[0m \u001b[0mpossible_cast_to_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m             ):\n\u001b[0;32m--> 368\u001b[0;31m                 batch = Iterator._arrays_to_matrix(\n\u001b[0m\u001b[1;32m    369\u001b[0m                     \u001b[0mfield\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumericalizations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix_class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                 )\n",
      "\u001b[0;32m~/.conda/envs/gen/lib/python3.9/site-packages/podium/datasets/iterator.py\u001b[0m in \u001b[0;36m_arrays_to_matrix\u001b[0;34m(field, arrays, matrix_class)\u001b[0m\n\u001b[1;32m    438\u001b[0m     ) -> np.ndarray:\n\u001b[1;32m    439\u001b[0m         \u001b[0mpad_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_pad_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0mpadded_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pad_to_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_length\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmatrix_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_arrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gen/lib/python3.9/site-packages/podium/datasets/iterator.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    438\u001b[0m     ) -> np.ndarray:\n\u001b[1;32m    439\u001b[0m         \u001b[0mpad_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_pad_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0mpadded_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pad_to_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_length\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmatrix_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_arrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gen/lib/python3.9/site-packages/podium/field.py\u001b[0m in \u001b[0;36m_pad_to_length\u001b[0;34m(self, array, length, custom_pad_symbol, pad_left, truncate_left)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_vocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m                 \u001b[0mpad_symbol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_padding_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gen/lib/python3.9/site-packages/podium/vocab.py\u001b[0m in \u001b[0;36mget_padding_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \"\"\"\n\u001b[1;32m    342\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mVocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pad\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Padding symbol is not in the vocabulary.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mVocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pad\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Padding symbol is not in the vocabulary."
     ]
    }
   ],
   "source": [
    "train_results = []\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    print(f\"Training epoch: {epoch}/{args.epochs}\")\n",
    "    # a) Train for one epoch\n",
    "    result_dict_train, logits, y_true, ids = train_model(\n",
    "        model, optimizer, criterion, train_iter\n",
    "    )\n",
    "    print(result_dict_train)\n",
    "    train_results.append(result_dict_train)\n",
    "\n",
    "    # b) Evaluate model (test set)\n",
    "#     eval_result_dict = self._evaluate_model(model)\n",
    "#     acc.append(eval_result_dict[\"accuracy\"])\n",
    "#     loss.append(result_dict_train[\"loss\"])\n",
    "#     eval_results.append(eval_result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "57b1c29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "name = model.get_classifier_name()\n",
    "clf = getattr(model.classifier, name)\n",
    "config = model.classifier.config\n",
    "num_layers = config.num_hidden_layers\n",
    "\n",
    "print(num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "27410ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch({\n",
      "    id: [['0'], ['1'], ['2'], ['3'], ['4'], ['5'], ['6'], ['7'], ['8'], ['9'], ['10'], ['11'], ['12'], ['13'], ['14'], ['15'], ['16'], ['17'], ['18'], ['19'], ['20'], ['21'], ['22'], ['23'], ['24'], ['25'], ['26'], ['27'], ['28'], ['29'], ['30'], ['31']],\n",
      "    sequence1: (tensor([[2054, 2055, 1996,  ...,    0,    0,    0],\n",
      "            [2515, 5423, 2669,  ...,    0,    0,    0],\n",
      "            [2054, 2003, 2115,  ...,    0,    0,    0],\n",
      "            ...,\n",
      "            [2054, 2024, 1996,  ...,    0,    0,    0],\n",
      "            [2003, 2009, 1037,  ...,    0,    0,    0],\n",
      "            [2054, 2097, 2022,  ...,    0,    0,    0]], device='cuda:0'), tensor([ 7,  9,  6,  9, 22, 10, 23, 11, 13,  6,  9,  8, 14, 11,  7, 12,  7,  7,\n",
      "             5, 11, 17, 16, 12,  7, 38, 14, 11,  6,  9, 16, 23, 14],\n",
      "           device='cuda:0')),\n",
      "    sequence2: (tensor([[ 2054,  1055,  1996,  2087,  2709,  2017,  2310,  2081,  1999,  1037,\n",
      "              4518,  7047, 17795,  5211,  3119,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2515,  2665,  5572,  2428, 13416,  3635,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2054,  2024,  1996,  2070,  1997,  1996,  2204,  5365,  2774,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2054,  2003,  1996,  2190,  2126,  2000,  4553,  2151,  2047,  3097,\n",
      "              2653,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2129,  2172,  2515,  2019, 19330,  2050,  7163,  9298,  7796,  2006,\n",
      "              2779,  2566,  3204,  2013,  2019, 14316,  1055,  2391,  1997,  3193,\n",
      "              1999,  9212,  4305, 13484,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2073,  2064,  1045,  4553,  2397,  2595,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2323,  1045,  2175,  2000,  1037,  2047,  2231,  2966,  2267,  3225,\n",
      "              2034,  5219,  2013,  2023,  2095,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2129,  2064,  1045,  4468,  2893, 14255, 23344,  2015,  3561,  2007,\n",
      "              2668,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2429,  2000,  7025,  2054,  6433,  2000,  2111,  1999,  1996, 25115,\n",
      "              2040,  2973,  2077,  7187,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2079,  2308,  2428,  2424, 23905,  2158,  8702,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2129,  2323,  1045, 15697,  2871,  2243,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2129,  2323,  2017,  2707,  4083,  4730,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2054,  2003,  1996,  5197,  1997,  1996,  7328,  2422, 24635,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2064,  2070,  2028,  2507,  2033, 10247,  2006,  4083,  3682,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2323,  1045,  6366,  2659,  3737,  4180,  2013,  2026,  2609,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2054,  2003,  1996,  5675,  2005,  1996, 16913, 11627,  1997, 21274,\n",
      "              3012,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2054,  2079,  7325, 22752,  2015,  2079,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2054,  2024,  1996,  2190,  4684,  4773, 10439,  7705,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2003,  1996,  3103,  5255,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2003,  1037,  2117,  3147,  2162,  2007,  3607,  1037,  2613,  6061,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 5702,  1999, 16595,     0,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2339,  2097,  1996,  3818, 13926, 15687,  3578,  2022,  2028,  3329,\n",
      "              7820,  2084,  2028,  1059, 13535,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2054,  1055,  1037,  2204,  2299,  1045,  2064,  2224,  2000, 13677,\n",
      "             26418,  2026,  3124,  2767,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2129,  2515, 19902,  7461,  2256,  2166,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2339,  2079,  9430, 14555,  2012,  2312,  2695, 12997,  2080,  6627,\n",
      "              3316,  2421, 10439,  2890,  7405,  3468,  9430,  1999,  4518,  2738,\n",
      "              2084,  2074,  2164,  2035,  1996,  9430,  1999,  1996,  2918, 10300],\n",
      "            [ 2129,  2064,  1045, 10172,  2122,  2015,  2006,  7388,  2470,  2006,\n",
      "              2248,  3075,  2005,  9459,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2129,  2079,  1045,  7374,  2005,  1996, 21307,  4523,  8325,  2961,\n",
      "              1045,  1056,  2961, 11360,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2129,  2515,  4702,  6444,  6541,  2147,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2054,  2003,  1996,  2364,  2801,  1997,  2023,  2338,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2054,  2024,  1996,  3808, 29361,  2006,  8304, 13305,  2015,  3818,\n",
      "              2011,  1996, 17212,  2050,  1999,  5334,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2339,  2003,  2045,  2145,  1037, 12078,  2306,  1996,  2142,  2983,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2054,  2097,  2022,  1996, 18520,  7207,  1055,  2634,  3343,  2065,\n",
      "              2016,  2468,  1996,  2343,  1997,  3915,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0]],\n",
      "           device='cuda:0'), tensor([15,  6,  9, 11, 24,  6, 15, 11, 14,  7,  6,  6,  9,  9,  9, 11,  6,  8,\n",
      "             4, 10,  3, 15, 14,  6, 30, 14, 14,  6,  8, 16, 10, 16],\n",
      "           device='cuda:0')),\n",
      "    label: tensor([[0],\n",
      "            [1],\n",
      "            [1],\n",
      "            [1],\n",
      "            [0],\n",
      "            [1],\n",
      "            [0],\n",
      "            [0],\n",
      "            [0],\n",
      "            [0],\n",
      "            [1],\n",
      "            [0],\n",
      "            [1],\n",
      "            [1],\n",
      "            [0],\n",
      "            [0],\n",
      "            [0],\n",
      "            [1],\n",
      "            [0],\n",
      "            [1],\n",
      "            [0],\n",
      "            [0],\n",
      "            [0],\n",
      "            [1],\n",
      "            [0],\n",
      "            [0],\n",
      "            [1],\n",
      "            [1],\n",
      "            [0],\n",
      "            [1],\n",
      "            [0],\n",
      "            [1]], device='cuda:0')\n",
      "})\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Batch' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1437920/3226874358.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gen/lib/python3.9/site-packages/podium/datasets/iterator.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Batch' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "enc = []\n",
    "grads = []\n",
    "labels = []\n",
    "enc_layers = {i: [] for i in range(num_layers)}\n",
    "\n",
    "\n",
    "train_iter = make_iterable(\n",
    "    train,\n",
    "    device,\n",
    "    batch_size=args.batch_size,\n",
    "    train=False,\n",
    "#     indices=indices,\n",
    ")\n",
    "\n",
    "for batch in train_iter:\n",
    "\n",
    "    print(batch)\n",
    "    inputs, _ = batch.text\n",
    "    labels.append(batch.label)\n",
    "    inputs.requires_grad = False\n",
    "\n",
    "    embedded_tokens = clf.embeddings(inputs)\n",
    "    embedded_tokens = torch.autograd.Variable(\n",
    "        embedded_tokens, requires_grad=True\n",
    "    )\n",
    "    encoded_all = clf.encoder(\n",
    "        embedded_tokens,\n",
    "        output_hidden_states=True,\n",
    "        # head_mask=head_mask,\n",
    "        # attention_mask=attention_mask,\n",
    "    )\n",
    "    # Skip the embedding layer [1:]\n",
    "    for i, enc_layer in enumerate(encoded_all[1][1:]):\n",
    "        enc_layers[i].append(enc_layer[:, 0].cpu())\n",
    "\n",
    "    encoded = encoded_all[0][:, 0]\n",
    "    enc.append(encoded.cpu())\n",
    "\n",
    "    mean = encoded.mean()\n",
    "    mean.backward()\n",
    "    enc_grad = embedded_tokens.grad.data\n",
    "    grads.append(enc_grad.norm(p=2, dim=(1, 2)))\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "y = torch.cat(labels)\n",
    "for k, v in enc_layers.items():\n",
    "    X = torch.cat(v)\n",
    "    enc_layers[k] = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "1748e9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0\n",
      "Layer 1\n",
      "Layer 2\n",
      "Layer 3\n",
      "Layer 4\n",
      "Layer 5\n",
      "Layer 6\n",
      "Layer 7\n",
      "Layer 8\n",
      "Layer 9\n",
      "Layer 10\n",
      "Layer 11\n"
     ]
    }
   ],
   "source": [
    "from cka import *\n",
    "\n",
    "lin_vals = np.empty((num_layers, num_layers))\n",
    "rbf_vals = np.empty((num_layers, num_layers))\n",
    "for i in range(num_layers):\n",
    "    X = enc_layers[i].detach().numpy()\n",
    "    print(f\"Layer {i}\")\n",
    "    for j in range(num_layers-1, i-1, -1):\n",
    "        Y = enc_layers[j].detach().numpy()\n",
    "        lin = linear_CKA(X, Y)\n",
    "        rbf = kernel_CKA(X, Y)\n",
    "        lin_vals[i, j] = lin_vals[j, i] = lin\n",
    "        rbf_vals[i, j] = rbf_vals[j, i] = rbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a95f274a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(np.flip(lin_vals, 1), xticklabels=range(1, 12+1), yticklabels=range(12, 0, -1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9c3d2c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.98248242 0.96114412 0.95121319 0.90460212 0.71041576\n",
      "  0.47315452 0.38221755 0.31722107 0.27903575 0.25892686 0.24506719]\n",
      " [0.98248242 1.         0.99294096 0.98335407 0.95183908 0.75358897\n",
      "  0.50744447 0.41371993 0.34471121 0.30324662 0.28150918 0.26641857]\n",
      " [0.96114412 0.99294096 1.         0.99445655 0.96767089 0.76605118\n",
      "  0.51650029 0.42217762 0.3524165  0.31063183 0.28872567 0.27348558]\n",
      " [0.95121319 0.98335407 0.99445655 1.         0.97601547 0.78270474\n",
      "  0.53881083 0.44590866 0.37730722 0.33579883 0.31386207 0.29857912]\n",
      " [0.90460212 0.95183908 0.96767089 0.97601547 1.         0.86372819\n",
      "  0.64402416 0.55500195 0.48563069 0.44082381 0.41642609 0.39986809]\n",
      " [0.71041576 0.75358897 0.76605118 0.78270474 0.86372819 1.\n",
      "  0.91641715 0.85514805 0.80074403 0.7619774  0.73903904 0.72382307]\n",
      " [0.47315452 0.50744447 0.51650029 0.53881083 0.64402416 0.91641715\n",
      "  1.         0.98668085 0.96352048 0.94102505 0.92561751 0.91487866]\n",
      " [0.38221755 0.41371993 0.42217762 0.44590866 0.55500195 0.85514805\n",
      "  0.98668085 1.         0.99073546 0.97514698 0.96322383 0.95479626]\n",
      " [0.31722107 0.34471121 0.3524165  0.37730722 0.48563069 0.80074403\n",
      "  0.96352048 0.99073546 1.         0.99368034 0.98627515 0.980397  ]\n",
      " [0.27903575 0.30324662 0.31063183 0.33579883 0.44082381 0.7619774\n",
      "  0.94102505 0.97514698 0.99368034 1.         0.99771465 0.99458776]\n",
      " [0.25892686 0.28150918 0.28872567 0.31386207 0.41642609 0.73903904\n",
      "  0.92561751 0.96322383 0.98627515 0.99771465 1.         0.99915245]\n",
      " [0.24506719 0.26641857 0.27348558 0.29857912 0.39986809 0.72382307\n",
      "  0.91487866 0.95479626 0.980397   0.99458776 0.99915245 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(lin_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e25b09b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.98822128 0.96864577 0.94506764 0.84834738 0.58058964\n",
      "  0.38203942 0.30834932 0.27336096 0.25623951 0.2498741  0.24393084]\n",
      " [0.98822128 1.         0.98886123 0.96045117 0.87905716 0.61475402\n",
      "  0.41201909 0.33673577 0.30072946 0.28261005 0.27595218 0.26954692]\n",
      " [0.96864577 0.98886123 1.         0.9828193  0.90749149 0.63409695\n",
      "  0.42791082 0.35237271 0.31687092 0.29899749 0.29216579 0.28537392]\n",
      " [0.94506764 0.96045117 0.9828193  1.         0.93184827 0.66351566\n",
      "  0.46338771 0.3904026  0.3567184  0.33894004 0.33143854 0.32377679]\n",
      " [0.84834738 0.87905716 0.90749149 0.93184827 1.         0.83521269\n",
      "  0.66546147 0.5968039  0.55941694 0.53621035 0.52402475 0.51286493]\n",
      " [0.58058964 0.61475402 0.63409695 0.66351566 0.83521269 1.\n",
      "  0.94082216 0.89526446 0.86034934 0.83374846 0.81842072 0.80571088]\n",
      " [0.38203942 0.41201909 0.42791082 0.46338771 0.66546147 0.94082216\n",
      "  1.         0.9877828  0.96883966 0.94919752 0.9360177  0.92503854]\n",
      " [0.30834932 0.33673577 0.35237271 0.3904026  0.5968039  0.89526446\n",
      "  0.9877828  1.         0.99094239 0.97567427 0.96464111 0.95507823]\n",
      " [0.27336096 0.30072946 0.31687092 0.3567184  0.55941694 0.86034934\n",
      "  0.96883966 0.99094239 1.         0.99297577 0.98468996 0.97586278]\n",
      " [0.25623951 0.28261005 0.29899749 0.33894004 0.53621035 0.83374846\n",
      "  0.94919752 0.97567427 0.99297577 1.         0.99728635 0.99119932]\n",
      " [0.2498741  0.27595218 0.29216579 0.33143854 0.52402475 0.81842072\n",
      "  0.9360177  0.96464111 0.98468996 0.99728635 1.         0.99753495]\n",
      " [0.24393084 0.26954692 0.28537392 0.32377679 0.51286493 0.80571088\n",
      "  0.92503854 0.95507823 0.97586278 0.99119932 0.99753495 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(lin_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b913a0e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAD8CAYAAADUv3dIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAb5ElEQVR4nO3de7RdVXn38e8vJxdyIyGAFJMIoQ1WBlqEGLBaBgWpAR2AWl8DfYdg0dghKGqrDQNfLDjU0lZ9eYdRm2JUbCECWj2lqaAC0otiYg00F8A0UnIChHsCBJNzed4/9gpjczh7r73PXnNfVn4fxhrZe12ePTnJec48c801H0UEZmbWHhM63QAzs/2Jk66ZWRs56ZqZtZGTrplZGznpmpm1kZOumVkbOemamdUgaZWkRyVtqHFckv6fpC2S7pF0fF5MJ10zs9q+Diypc/wMYGG2LQO+nBfQSdfMrIaIuBN4ss4pZwPXRsVPgdmSDq8Xc2KRDRzzAybPTfbI2wQpUdw0P4umTJyUJO6cKTOSxAVYOK3uv59xu2TvQUninvqZuUniAmjRqWnizpiTJO6EabOSxAVg0pQ0YQ85quVv6sHHtzaccyYf+pvvp9JD3WdlRKxs4uPmAtuq3g9k+x6udUHypGtm1lYjww2fmiXYZpJsy5x0zaxcYqSdn7YdmF/1fl62ryaP6ZpZuYyMNL61rh94dzaL4SRgZ0TUHFoA93TNrGSiwJ6upOuBU4BDJA0AnwQmVT4nvgKsAc4EtgC7gffkxXTSNbNyGR4qLFREnJtzPICLmonppGtm5dLEjbROcNI1s3Jp7420pjnpmlm5FHODLJkkSVfSMrIJx+qbxYQJ01N8jJnZSxR5Iy2FulPGJB0o6bOSvinpvFHHvlTruohYGRGLImKRE66ZtVV7p4w1LW+e7tcAAd8Glkr6tqR9z/+dlLRlZmbjMTzY+NYBecMLvxkR78hef1fSZcBtks5K3C4zs/Hp8uGFvKQ7RdKEyAZJIuLTkrYDdwLpVlkxMxuvLr+Rlje88E/Ai5ZWioivA38K7E3UJjOz8YuRxrcOqNvTjYiP19j/fUmfSdMkM7MW9HhPt54rCmuFmVlBYmSw4a0T6vZ0Jd1T6xBwWPHNMTNrUZf3dPNupB0GvBl4atR+Af/RyAdMS7TCPEBfogoPE/v6ksSd2jc5SdwDJ6WbCz1rQpq/v4P79iSJy5xD0sQFNCXN1zlZhYeE33sMJvr7K0KPz164GZgREetHH5B0R4oGmZm1pJcXvImIC+scO6/WMTOzjunxnq6ZWW/p8TFdM7PeUuAi5ik46ZpZubina2bWPhE9fCPNzKzndHlP1yXYzaxcClx7QdISSfdJ2iJp+RjHj5D0I0n3SLpD0ry8mE66ZlYuBS1iLqkPWAGcARwDnCvpmFGn/Q1wbUS8BrgS+Gxe88addCX9S51jyyStk7Ru79Cu8X6EmVnzhoca3+pbDGyJiK0RsRdYDZw96pxjgNuy17ePcfwl8tZeOL7WIeC4WtdFxEpgJcCB04+KvEaYmRWmiYcjqus5ZlZm+QtgLrCt6tgAcOKoEHcDbweuBt4GzJR0cEQ8Uesz826krQV+TCXJjjY751ozs/Zr4kZadQdxnP4M+KKkC6gUd9gO1J0+kZd0NwPvj4hfjj4gadsY55uZdVZxsxe2A/Or3s/L9r0gIh6i0tNF0gzgHRHxdL2geWO6f1HnnA/mXGtm1n7FzV5YCyyUtEDSZGAp0F99gqRDpBeWO7wUWJUXNG/Bm5vqHD4oL7iZWdsV9BhwRAxJuhi4BegDVkXERklXAusioh84BfispKAyvHBRXtxWHo64gkqJdjOz7lHgwxERsQZYM2rf5VWvbwLqdU5fwpUjzKxcenxpx5YrR5iZtVWXPwacvHLE1IlpStQATEhUrqdvQpq4UxKV65kyYVKSuADTlGZ5jmmT9yaJy4Fz0sQFNCNR7FRldRKW1BnZvTNZ7Jb1ctJ15Qgz6znR3c9jeZUxMyuXIS9ibmbWPj1+I83MrLf08piumVnP8ZiumVkbuadrZtZGTrpmZu0Tw91dmLLuUwCSZkn6S0n3SnpS0hOSNmf7Zte57oXKEc/vfbroNpuZ1VZQuZ5U8h69uoHKI8CnRMSciDgY+P1s3w21LoqIlRGxKCIWTZ08u7DGmpnlKrAwZQp5SffIiLgqIh7ZtyMiHomIq4Aj0jbNzGwcRqLxrQPyku7/SPq4pBdWFJN0mKQ/58W1g8zMukOPDy+8CzgY+HE2pvskcAcwB3hn4raZmTVveLjxrQPyFrx5CvjzbHsRSe/Bi5ibWbfp8iljraxheEVhrTAzK0qXj+m6coSZlUuPL3jjyhFm1ls61INtVPLKEXOmHNh8qxrUl6hyxCT1JYl7QKLKEQf1TU0SF+Bg0lSlmDHzmSRxNTNdkWodMD1J3Pj1c2niPvtkkrgAsSdNm4sQBY7pSloCXE2lGvA1EfGXo46/AvgGMDs7Z3lWzLImV44ws3IpaFaCpD5gBXA6MACsldQfEZuqTvsEcENEfFnSMVQqBx9ZL26arqKZWacUdyNtMbAlIrZGxF5gNXD2qHMC2Pfr/CzgobygXvDGzMqlieEFScuAZVW7VkbEyuz1XF78ENgAcOKoEH8B3Crpg8B04E15n+mka2bl0sSNtCzBrsw9sbZzga9HxOckvR74pqRjI2pPoXDSNbNyKW7K2HZgftX7edm+ahcCSwAi4ieSDgAOAR6tFdRjumZWLsWN6a4FFkpaIGkysBToH3XOg8BpAJJeBRwAPFYvaG5PV9JRwNupZPxh4H7guojYlXetmVm7xVAxsxciYkjSxcAtVKaDrYqIjZKuBNZFRD/wp8DfSfoIlZtqF0TUL9KW90Tah4C3AncCrwN+QSX5/lTSByLijhb/v8zMilXgwxHZnNs1o/ZdXvV6E/CGZmLm9XTfBxwXEcOSPg+siYhTJP0t8D3gtWNdVH1H8DdmHMHsqS9rpk1mZuPX5Y8BNzKmuy8xTwFmAETEg1D7UaXqyhFOuGbWVr284A1wDZWnMO4Cfg+4CkDSoUC6ZwzNzMYpennthYi4WtIPgVcBn4uIe7P9jwEnt6F9ZmbNKehGWiq5sxciYiOwsQ1tMTNrXS/3dM3Meo6TrplZ++RMk+04J10zKxf3dM3M2mh/T7rLDjg6WexJib62UxLFPXA4TeBDhwaTxAU48mVpZgb+xgdemSRu31EnJIkLMLz150nixjOjq2EVZFfCWZ1PPp4m7u+c2XKIGOruhyPc0zWzcununOuka2bl0tMPR5iZ9RwnXTOzNvLwgplZ+3h4wcysjWKoh5NuVYmKhyLih5LOA34X2Eylama6uUpmZuPR48MLX8vOmSbpfCrr6X6HSk2gxcD5aZtnZtacLl/DPDfpvjoiXiNpIpUqmC/Pqkj8PXB3rYuqK0e886DFvH7GwsIabGZWV5cn3bzKEROyIYaZwDRgVrZ/Cg1WjnDCNbN2ipHGtzySlki6T9IWScvHOP4FSeuz7X5JT+fFzOvpfhW4l0olzMuAGyVtBU4CVuc32cysvWKomDiS+oAVwOnAAJUqOv1ZMcrKZ0V8pOr8D1KjbmS1vMoRX5D0rez1Q5KuBd4E/F1E/Gxc/ydmZgkVOKa7GNgSEVsBJK0GzgY21Tj/XOCTeUEbqRzxUNXrp4GbGmismVlHFJh05wLbqt4PACeOdaKkI4AFwG15QRupBmxm1jtCDW+SlklaV7UtG+enLgVuiojcAm1+OMLMSqWZnm5ErARW1ji8HZhf9X5etm8sS4GLGvlMJ10zK5UYUVGh1gILJS2gkmyXAueNPknSbwMHAT9pJKiTrpmVyshwMUk3IoYkXQzcQmUG16qI2CjpSmBdRPRnpy4FVkeDxdmUuojbc59+d7IP0JTJaQJPPSBJWM2alX/SeBx8WJq4gA47MkncvlccmyTu4I1fSBIX4JEv3Zck7rPPTEkSd/femlPpW/bEcJo2v2XH9S1nzIETT20458y767bCusWNck/XzEqlwOGFJJx0zaxUurwCu5OumZWLe7pmZm1U1I20VJx0zaxU3NM1M2ujiO5OunUfA5b0IUnz651jZtZNilzaMYW8tRc+Bdwl6V8lfUDSoe1olJnZeI2EGt46IS/pbqXyvPGngBOATZK+L+l8STNrXVS9iMSqtfcX2Fwzs/oi1PDWCXlJNyJiJCJujYgLgZcDXwKWUEnItS56oXLEH7/u6AKba2ZW38iwGt46Ie9G2otalVX/7Qf6JU1L1iozs3Hq9dkL76p1ICJ2F9wWM7OWdWqstlF55Xo8IGtmPaXbp4x5nq6ZlYrXXjAza6OeHl4wM+s1Iz1+I83MrKfs9z3dvlPPSBd88tQkYTUpTVymp6kcoak1n1NpPXair/HwgxuSxE1V3QHgi8/MSRL3CQaTxN09aShJXICdfU8lifuWAmL4RpqZWRvt9z1dM7N26vLJC7mPAZuZ9ZThkQkNb3kkLZF0n6QtkpbXOOd/SdokaaOk6/JiuqdrZqVS1IqNkvqAFcDpwACwVlJ/RGyqOmchcCnwhoh4StLL8uK6p2tmpRKo4S3HYmBLRGyNiL3AauDsUee8D1gREU8BRMSjeUGddM2sVEai8a16GdpsW1YVai6wrer9QLav2tHA0ZL+XdJPJS3Ja1/d4QVJJwKbI2KXpKnAcuB4YBPwmYjY2cDXwMysbUbye7AviIiVwMoWPm4isBA4hcra43dKenVEPF3rgrye7ipg32piVwOzgKuyfV9roaFmZkkUOLywHaguVzYv21dtAOiPiMGI+BVwP5UkXFNe0p0QEftmWC+KiA9HxL9FxBXAUbUuqu6yf/W7P8r5CDOz4gyjhrcca4GFkhZImgwspbKeeLXvUunlIukQKsMNNQs8QH7S3SDpPdnruyUtyoIfDbUfo6muHHHhOaflfISZWXFGmtjqyTqcFwO3AJuBGyJio6QrJZ2VnXYL8ISkTcDtwMci4ol6cfOmjL0XuFrSJ4DHgZ9I2kZlcPm9OdeambVdkUV+I2INsGbUvsurXgfw0WxrSN4i5juBCyQdCCzIzh+IiB1NtNvMrG0aGKvtqIYejoiIXcDdidtiZtayLl/Z0U+kmVm5NDNlrBOcdM2sVIY73YAcTrpmViojck/XzKxtun1px+RJVwfmLrozfskqR0xOE3fK9CRxmTQlTVwg9j6fJu6OB5LEfeDR2UniAtw7ZVeSuI8PPZck7p6RNBUpAHYNpmlzEYqcMpaCe7pmViqevWBm1kYNPN7bUU66ZlYq7umambWRx3TNzNpov5+9YGbWTqUaXpD0Rip1gzZExK1pmmRmNn7dPrxQdz1dST+rev0+4IvATOCTtcoRm5l10rAa3zohbxHzSVWvlwGnZ1Uj/gD4o1oXVVeOuObGmwtopplZY4paxDyVvOGFCZIOopKcFRGPAUTEc5KGal1UXextz8Yfdfu4tpmVSLcPL+Ql3VnAzwEBIenwiHhY0oxsn5lZV+n2Xl5e5YgjaxwaAd5WeGvMzFpUqtkL+0TEbuBXBbfFzKxl3T68kHcjzcyspww3seWRtETSfZK2jDVjS9IFkh6TtD7bcgv2+uEIMyuVooYXJPUBK4DTgQFgraT+iNg06tRvRcTFjcZ1T9fMSqXAKWOLgS0RsTUi9gKrgbNbbZ+TrpmVSjSxVT9TkG3LqkLNBbZVvR/I9o32Dkn3SLpJ0vy89qWvHDF9VrrgE9NUTEhVOYK+SfnnjMfgnjRxgXj+mTSBn9iRJOxjExJ9jYGnhtNU0Xhq8NkkcfcM700SF2Dn3t3JYrdqpIlJY9XPFIzTPwHXR8QeSe8HvgGcWu8C93TNrFQKvJG2Hajuuc7L9r0gIp6IiH29nmuAE/KCOumaWakUOKa7FlgoaYGkycBSoL/6BEmHV709C9icF9SzF8ysVIqavRARQ5IuBm4B+oBVEbFR0pXAuojoBz4k6SxgCHgSuCAvrpOumZVKM2O6eSJiDbBm1L7Lq15fClzaTEwnXTMrlZ5ee8HMrNeU7jFgSdemaIiZWRGGiYa3Tqjb05XUP3oX8PuSZgNExFmJ2mVmNi693tOdB+wCPg98LtueqXo9phdVjrjuO0W11cws1wjR8NYJeWO6i4BLgMuAj0XEeknPR8SP611U/ZTH3gfWdfu4tpmVSLcnnLxFzEeAL0i6MftzR941Zmad1O3DCw0l0IgYAN4p6S1UhhvMzLpSp26QNaqpXmtE/DPwz4naYmbWsk6N1TbKQwVmVirdnXKddM2sZNzTNTNro1LcSDMz6xWxv/d0NXVmuuCpKjGkMjyYJGzseS5JXACe25kkbOxME3dXX0Hr+o3h10NpKjE8P5ym8seeRP/eAPYMpYvdqlLNXjAz63YeXjAza6ORcE/XzKxtujvlOumaWcl4ypiZWRvt97MXzMzaaajLk65LsJtZqUQT/+WRtETSfZK2SFpe57x3SApJi/Ji5iZdSYslvS57fYykj0o6M7e1ZmYdMNLEVo+kPmAFcAZwDHCupGPGOG8mlXXH72qkfXnlej6ZfeBEST8ATgRuB5ZLem1EfLqRDzEza5cobsrYYmBLRGwFkLQaOBvYNOq8TwFXAR9rJGheT/cPgTcAJwMXAedExKeANwPvqnXRi8r1fPNbjbTDzKwQzZTrqc5V2basKtRcYFvV+4Fs3wskHQ/Mz5a9bUjejbShiBgGdkv674jYBRARz0uq2TuvLtczuOO+7h7VNrNSaeYx4Opc1SxJE6jUj7ygmevyerp7JU3LXp9Q9WGz6P6n7cxsP1RgYcrtwPyq9/OyffvMBI4F7pD0AHAS0J93My2vp3tyROyBF+ql7TMJOD+vxWZm7VbgmO5aYKGkBVSS7VLgvKrP2Qkcsu+9pDuAP4uIdfWC5hWmHHP5o4h4HHi80ZabmbVLUb+CR8SQpIuBW4A+YFVEbJR0JbAuIvrHE9cPR5hZqRT5RFpErAHWjNp3eY1zT2kkppOumZWK114wM2uj4ejue/xOumZWKl7wptdK6iQUg2nKvaSKW4n9fJrAz/86Sdg96ar1MBjDSeIOj6TpmQ0Np2kvwEgX9ya9iLmZWRt1d8p10jWzkvGNNDOzNnLSNTNrI89eMDNrI89eMDNrowLXXkiikcoRvy3pNEkzRu1fkq5ZZmbjU+AqY0nUTbqSPgR8D/ggsEHS2VWHP5OyYWZm4xERDW+dkNfTfR9wQkScA5wC/B9Jl2THak5Df1HliGuvL6ShZmaNGGak4a0T8sZ0J0TEswAR8YCkU4CbJB1BnaT7osoRj2/t7gEWMyuVbn8iLa+nu0PScfveZAn4rVQW7n11wnaZmY1LkSXYU8jr6b4bGKreERFDwLsl/W2yVpmZjVO393TzKkcM1Dn278U3x8ysNZ6na2bWRj3d0zUz6zV+DNjMrI26fXgh94k0M7NeEjHS8JZH0hJJ90naImn5GMf/RNJ/SVov6d8kHZMX0z3dsQwPpok7NGZF+9btTVTdIWHs2JOm2sVgwsoRqX5tTVWFIeWv2d08blrU472S+oAVwOnAALBWUn9EbKo67bqI+Ep2/lnA54G6SyS4p2tmpVLgY8CLgS0RsTUi9gKrgeqlEIiIXVVvp9NA4Qr3dM2sVJrp6UpaBiyr2rUye6IWYC6wrerYAHDiGDEuAj4KTAZOzftMJ10zK5VmCn1WL1kwXhGxAlgh6TzgE8D59c738IKZlUqBjwFvB+ZXvZ+X7atlNXBOXlAnXTMrlQLHdNcCCyUtkDQZWAr0V58gaWHV27cAv8wL6uEFMyuVomYvRMSQpIuBW4A+YFVEbJR0JbAuIvqBiyW9CRgEniJnaAGcdM2sZIpcnDwi1gBrRu27vOr1JS+5KMe4hxckvWe815qZpTI8MtLw1gmtjOleUeuAK0eYWad0e420usMLku6pdQg4rNZ1rhxhZp3S7dWA88Z0DwPeTGWAuJqA/0jSIjOzFnTzI8qQn3RvBmZExPrRByTdkaJBZmat6PZVxvIqR1xY59h5xTfHzKw1vd7TNTPrKalWbSuKk66ZlUqv30gzM+spTrpmZm3U3SmX5haHSL0By3otdq/F7cU2+2vhr0WZtm5bZWxZ/ildF7vX4qaM3WtxU8butbgpY6dsc8/ptqRrZlZqTrpmZm3UbUm3pbIZHYrda3FTxu61uClj91rclLFTtrnnKBvoNjOzNui2nq6ZWak56ZqZtVFXJF1JqyQ9KmlDwXHnS7pd0iZJGyU1XVqjTuwDJP1M0t1Z7JqLuo8zfp+kX0i6ucCYD0j6L0nrJa0rKm4We7akmyTdK2mzpNcXEPOVWVv3bbskfbiA5iLpI9nf2wZJ10s6oKC4l2QxN7ba1rG+LyTNkfQDSb/M/jyooLjvzNo8ImlRwW3+6+zfxT2S/lHS7PHGL4VOTxTOxpRPBo4HNhQc93Dg+Oz1TOB+4JiCYovKspcAk4C7gJMKbPtHgeuAmwuM+QBwSKK/w28A781eTwZmFxy/D3gEOKKAWHOBXwFTs/c3ABcUEPdYYAMwjcrTnj8EfquFeC/5vgD+ClievV4OXFVQ3FcBrwTuABYV3OY/ACZmr68aT5vLtHVFTzci7gSeTBD34Yj4z+z1M8BmKt9wRcSOiHg2ezsp2wq5KylpHpVyztcUES81SbOofLN9FSAi9kbE0wV/zGnAf0fE/xQUbyIwVdJEKknyoQJivgq4KyJ2R8QQ8GPg7eMNVuP74mwqP+DI/jyniLgRsTki7htHMxuJfWv29QD4KTCv1c/pZV2RdNtB0pHAa6n0SIuK2SdpPfAo8IOIKCr2/wU+DhS9Rl0At0r6uaQinxJaADwGfC0bErlG0vQC4wMsBQopuBcR24G/AR4EHgZ2RsStBYTeAPyepIMlTQPOBOYXELfaYRHxcPb6EeqUzepSfwz8S6cb0Un7RdKVNAP4NvDhiNhVVNyIGI6I46j85F4s6dhWY0p6K/BoRPy81VhjeGNEHA+cAVwk6eSC4k6k8ivllyPitcBzVH71LYSkycBZwI0FxTuISo9xAfByYLqk/91q3IjYTOXX51uB7wPrgeFW49b5vKAH1nfZR9JlwBDwD51uSyeVPulKmkQl4f5DRHwnxWdkv0rfDiwpINwbgLMkPQCsBk6V9PcFxN3XwyMiHgX+EVhcRFxgABio6unfRCUJF+UM4D8jYkdB8d4E/CoiHouIQeA7wO8WETgivhoRJ0TEyVRqC95fRNwqOyQdDpD9+WjB8ZOQdAHwVuCPsh8W+61SJ11JojLOuDkiPl9w7EP33YWVNBU4Hbi31bgRcWlEzIuII6n8Sn1bRLTcC5M0XdLMfa+p3NwoZLZIRDwCbJP0ymzXacCmImJnzqWgoYXMg8BJkqZl/0ZOozLe3zJJL8v+fAWV8dzriohbpR84P3t9PvC9guMXTtISKsNlZ0XE7k63p+M6fScv+6F3PZWxtUEqvaYLC4r7Riq/ft1D5Ve99cCZBcV+DfCLLPYG4PIEX5dTKGj2AnAUcHe2bQQuK7itxwHrsq/Hd4GDCoo7HXgCmFVwe6+g8kNyA/BNYEpBcf+Vyg+cu4HTWoz1ku8L4GDgR8AvqcyOmFNQ3Ldlr/cAO4BbCmzzFmBb1ffgV4r8u+y1zY8Bm5m1UamHF8zMuo2TrplZGznpmpm1kZOumVkbOemambWRk66ZWRs56ZqZtdH/BwBCguESC7rKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Random\n",
    "sns.heatmap(np.flip(lin_vals, 1), xticklabels=range(1, 12+1), yticklabels=range(12, 0, -1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fda8257d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Core set\n",
    "sns.heatmap(np.flip(lin_vals, 1), xticklabels=range(1, 12+1), yticklabels=range(12, 0, -1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "617026dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Entropy\n",
    "sns.heatmap(np.flip(lin_vals, 1), xticklabels=range(1, 12+1), yticklabels=range(12, 0, -1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc285785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200.0, 1000.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "g = sns.lineplot(\n",
    "    data=df_tr[df_tr.index.get_level_values(\"mode\") == \"long-besov\"],\n",
    "    x=\"labeled\",\n",
    "    y=\"f1_micro\",\n",
    "    hue=\"sampler\",\n",
    "    style=\"sampler\",\n",
    "    markers=True,\n",
    "    dashes=False,\n",
    "    ci=95,\n",
    "    linewidth=3,\n",
    ")\n",
    "g.set_ylim(0.89, 0.98)\n",
    "g.set_xlim(200, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33d25a5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not interpret value `acuraccy` for parameter `y`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1434047/548833310.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m g = sns.lineplot(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_tr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mode\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"ada-besov\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"labeled\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"acuraccy\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mhue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sampler\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gen/lib/python3.9/site-packages/seaborn/_decorators.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m             )\n\u001b[1;32m     45\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gen/lib/python3.9/site-packages/seaborn/relational.py\u001b[0m in \u001b[0;36mlineplot\u001b[0;34m(x, y, hue, size, style, data, palette, hue_order, hue_norm, sizes, size_order, size_norm, dashes, markers, style_order, units, estimator, ci, n_boot, seed, sort, err_style, err_kws, legend, ax, **kwargs)\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m     \u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_LinePlotter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_semantics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m     p = _LinePlotter(\n\u001b[0m\u001b[1;32m    686\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mci\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mci\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_boot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_boot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gen/lib/python3.9/site-packages/seaborn/relational.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, variables, estimator, ci, n_boot, seed, sort, err_style, err_kws, legend)\u001b[0m\n\u001b[1;32m    365\u001b[0m         )\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gen/lib/python3.9/site-packages/seaborn/_core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m    602\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_semantic_mappings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gen/lib/python3.9/site-packages/seaborn/_core.py\u001b[0m in \u001b[0;36massign_variables\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m    665\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"long\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             plot_data, variables = self._assign_variables_longform(\n\u001b[0m\u001b[1;32m    668\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m             )\n",
      "\u001b[0;32m~/.conda/envs/gen/lib/python3.9/site-packages/seaborn/_core.py\u001b[0m in \u001b[0;36m_assign_variables_longform\u001b[0;34m(self, data, **kwargs)\u001b[0m\n\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m                 \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Could not interpret value `{val}` for parameter `{key}`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 902\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Could not interpret value `acuraccy` for parameter `y`"
     ]
    }
   ],
   "source": [
    "g = sns.lineplot(\n",
    "    data=df_tr[df_tr.index.get_level_values(\"mode\") == \"ada-besov\"],\n",
    "    x=\"labeled\",\n",
    "    y=\"f1_micro\",\n",
    "    hue=\"sampler\",\n",
    "    style=\"sampler\",\n",
    "    markers=True,\n",
    "    dashes=False,\n",
    "    ci=95,\n",
    "    linewidth=3,\n",
    ")\n",
    "# g.set_ylim(0.82, 0.92)\n",
    "# g.set_xlim(500, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "44e34a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoAdapterModel, AdapterConfig\n",
    "from datasets import load_dataset\n",
    "\n",
    "d = load_dataset(\"glue\", \"qqp\")\n",
    "\n",
    "\n",
    "def save_dataset(hfd, name):\n",
    "    hfd[\"train\"].to_pandas()[[\"question1\", \"question2\", \"label\"]].sample(\n",
    "        5_000\n",
    "    ).reset_index(drop=True).to_csv(f\"data/{name}/train.csv\", header=False)\n",
    "    hfd[\"test\"].to_pandas()[[\"question1\", \"question2\", \"label\"]].sample(\n",
    "        2_000\n",
    "    ).reset_index(drop=True).to_csv(f\"data/{name}/test.csv\", header=False)\n",
    "    hfd[\"validation\"].to_pandas()[[\"question1\", \"question2\", \"label\"]].sample(\n",
    "        1_000\n",
    "    ).reset_index(drop=True).to_csv(f\"data/{name}/validation.csv\", header=False)\n",
    "\n",
    "\n",
    "save_dataset(d, \"QQP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe182e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoAdapterModel.from_pretrained(\"bert-base-uncased\")\n",
    "a = model.load_adapter(\"adapters/TREC-2-BERT-pfeiffer\")\n",
    "model.add_classification_head(\"head\", num_labels=2)\n",
    "model.add_adapter(\"head\")\n",
    "model.train_adapter(\"head\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f14e35e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/jjukic/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01c15a5dfd90473ba1783414b42d727c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "d = load_dataset(\"glue\", \"cola\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a5243de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'the kittens yawned awake and played.', 'label': -1, 'idx': 3}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[\"test\"][3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "23fee3a0d468748c44cd2b5f7c2d15d26ebb787aed261e84470feedc3724e7bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
