{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fcdfea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "\n",
    "from dataloaders import *\n",
    "from util import Config\n",
    "from viz_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b644810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUBJ-BERT\n",
      "Loading SUBJ -- ada -- BERT\n",
      "Loading SUBJ -- ada-besov -- BERT\n"
     ]
    }
   ],
   "source": [
    "datasets = [\"MRPC\", \"TREC-2\", \"SUBJ\", \"AGN-2\", \"TREC-6\", \"AGN-4\", \"SST\"]\n",
    "dataset_map = {\n",
    "    \"TREC-2\": \"TREC-2\",\n",
    "    \"SUBJ\": \"SUBJ\",\n",
    "    \"AGN-2\": \"AGN-2\",\n",
    "    \"TREC-6\": \"TREC-full\",\n",
    "    \"SST\": \"SST\",\n",
    "    \"COLA\": \"COLA\",\n",
    "    \"AGN-4\": \"ag_news-full\",\n",
    "}\n",
    "models = [\"BERT\", \"ELECTRA\"]\n",
    "load_anti = False\n",
    "n = 0  # AL step at which evaluation (AUC) starts\n",
    "model = \"BERT\"\n",
    "mode = \"ada\"\n",
    "dataset = \"SUBJ\"\n",
    "\n",
    "aucs = []\n",
    "trs = []\n",
    "try:\n",
    "    experiments, meta = load_results(\n",
    "        base_dir=f\"results/\",\n",
    "        dataset=dataset,\n",
    "        model=model,\n",
    "    )\n",
    "except:\n",
    "    print(f\"No experiments for {dataset}-{model}-{mode}\")\n",
    "for load_mode in [\"last\", \"best\"]:\n",
    "    if mode == \"short\" and load_mode == \"best\":\n",
    "        continue\n",
    "    mode_print = mode if load_mode == \"last\" else f\"{mode}-besov\"\n",
    "    print(f\"Loading {dataset} -- {mode_print} -- {model}\")\n",
    "    df_tr_i = results_to_df(experiments, mode=load_mode)\n",
    "\n",
    "    df_tr_i[\"model\"] = model\n",
    "    df_tr_i[\"mode\"] = mode_print\n",
    "    df_tr_i[\"dataset\"] = dataset\n",
    "    df_tr_i = df_tr_i.reset_index().set_index(\n",
    "        [\"dataset\", \"model\", \"mode\", \"sampler\", \"experiment\", \"al_iter\"]\n",
    "    )\n",
    "    trs.append(df_tr_i)\n",
    "\n",
    "    df_auc_i = al_auc(df_tr_i)\n",
    "    df_auc_i[\"mode\"] = mode_print\n",
    "    df_auc_i = df_auc_i.reset_index().set_index([\"mode\", \"sampler\"])\n",
    "    aucs.append(df_auc_i)\n",
    "\n",
    "\n",
    "# plot_besov_index(df_tr, ci=0)\n",
    "# plot_al_accuracy(df_tr, metric=\"f1_micro\", ci=0)\n",
    "df_tr = pd.concat(trs)\n",
    "df_auc = pd.concat(aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b2b3b91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fbf71454e80>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "g = sns.lineplot(\n",
    "    data=df_tr[df_tr.index.get_level_values(\"mode\") == \"ada-besov\"],\n",
    "    x=\"labeled\",\n",
    "    y=\"f1_micro\",\n",
    "    hue=\"sampler\",\n",
    "    style=\"sampler\",\n",
    "    markers=True,\n",
    "    dashes=False,\n",
    "    ci=95,\n",
    "    linewidth=3,\n",
    ")\n",
    "plt.legend(loc='lower right')\n",
    "# g.set_ylim(0.89, 0.98)\n",
    "# g.set_xlim(100, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5628051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>labeled</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>selected</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>mode</th>\n",
       "      <th>sampler</th>\n",
       "      <th>experiment</th>\n",
       "      <th>al_iter</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"11\" valign=\"top\">TREC-2</th>\n",
       "      <th rowspan=\"11\" valign=\"top\">BERT</th>\n",
       "      <th rowspan=\"11\" valign=\"top\">ada-besov</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">random</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>0.657703</td>\n",
       "      <td>0.674897</td>\n",
       "      <td>0.674897</td>\n",
       "      <td>0.673964</td>\n",
       "      <td>[1549, 1546, 1179, 111, 561, 1832, 619, 56, 51...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>150</td>\n",
       "      <td>0.662511</td>\n",
       "      <td>0.709877</td>\n",
       "      <td>0.709877</td>\n",
       "      <td>0.701160</td>\n",
       "      <td>[364, 841, 941, 922, 152, 1904, 903, 450, 447,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200</td>\n",
       "      <td>0.651112</td>\n",
       "      <td>0.695473</td>\n",
       "      <td>0.695473</td>\n",
       "      <td>0.691073</td>\n",
       "      <td>[142, 543, 930, 872, 865, 1646, 95, 589, 513, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>250</td>\n",
       "      <td>0.633320</td>\n",
       "      <td>0.738683</td>\n",
       "      <td>0.738683</td>\n",
       "      <td>0.738194</td>\n",
       "      <td>[1630, 1187, 1237, 1552, 682, 1424, 357, 178, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>300</td>\n",
       "      <td>0.642351</td>\n",
       "      <td>0.732510</td>\n",
       "      <td>0.732510</td>\n",
       "      <td>0.732143</td>\n",
       "      <td>[499, 1225, 1423, 438, 1077, 1637, 566, 1410, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">entropy</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">4</th>\n",
       "      <th>14</th>\n",
       "      <td>800</td>\n",
       "      <td>0.658221</td>\n",
       "      <td>0.775720</td>\n",
       "      <td>0.775720</td>\n",
       "      <td>0.771635</td>\n",
       "      <td>[1416, 1090, 83, 524, 1923, 1278, 1293, 882, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>850</td>\n",
       "      <td>0.649009</td>\n",
       "      <td>0.806584</td>\n",
       "      <td>0.806584</td>\n",
       "      <td>0.806466</td>\n",
       "      <td>[1763, 240, 1153, 246, 1013, 1552, 1818, 1850,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>900</td>\n",
       "      <td>0.659438</td>\n",
       "      <td>0.800412</td>\n",
       "      <td>0.800412</td>\n",
       "      <td>0.799964</td>\n",
       "      <td>[168, 1248, 1598, 714, 784, 1841, 1253, 1873, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>950</td>\n",
       "      <td>0.656500</td>\n",
       "      <td>0.818930</td>\n",
       "      <td>0.818930</td>\n",
       "      <td>0.818820</td>\n",
       "      <td>[1455, 1577, 1788, 1880, 1023, 1458, 1126, 791...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.648496</td>\n",
       "      <td>0.816872</td>\n",
       "      <td>0.816872</td>\n",
       "      <td>0.816853</td>\n",
       "      <td>[1606, 456, 1572, 1407, 57, 1461, 463, 294, 12...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>475 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    labeled  train_loss  \\\n",
       "dataset model mode      sampler experiment al_iter                        \n",
       "TREC-2  BERT  ada-besov random  0          0            100    0.657703   \n",
       "                                           1            150    0.662511   \n",
       "                                           2            200    0.651112   \n",
       "                                           3            250    0.633320   \n",
       "                                           4            300    0.642351   \n",
       "...                                                     ...         ...   \n",
       "                        entropy 4          14           800    0.658221   \n",
       "                                           15           850    0.649009   \n",
       "                                           16           900    0.659438   \n",
       "                                           17           950    0.656500   \n",
       "                                           18          1000    0.648496   \n",
       "\n",
       "                                                    test_accuracy  f1_micro  \\\n",
       "dataset model mode      sampler experiment al_iter                            \n",
       "TREC-2  BERT  ada-besov random  0          0             0.674897  0.674897   \n",
       "                                           1             0.709877  0.709877   \n",
       "                                           2             0.695473  0.695473   \n",
       "                                           3             0.738683  0.738683   \n",
       "                                           4             0.732510  0.732510   \n",
       "...                                                           ...       ...   \n",
       "                        entropy 4          14            0.775720  0.775720   \n",
       "                                           15            0.806584  0.806584   \n",
       "                                           16            0.800412  0.800412   \n",
       "                                           17            0.818930  0.818930   \n",
       "                                           18            0.816872  0.816872   \n",
       "\n",
       "                                                    f1_macro  \\\n",
       "dataset model mode      sampler experiment al_iter             \n",
       "TREC-2  BERT  ada-besov random  0          0        0.673964   \n",
       "                                           1        0.701160   \n",
       "                                           2        0.691073   \n",
       "                                           3        0.738194   \n",
       "                                           4        0.732143   \n",
       "...                                                      ...   \n",
       "                        entropy 4          14       0.771635   \n",
       "                                           15       0.806466   \n",
       "                                           16       0.799964   \n",
       "                                           17       0.818820   \n",
       "                                           18       0.816853   \n",
       "\n",
       "                                                                                             selected  \n",
       "dataset model mode      sampler experiment al_iter                                                     \n",
       "TREC-2  BERT  ada-besov random  0          0        [1549, 1546, 1179, 111, 561, 1832, 619, 56, 51...  \n",
       "                                           1        [364, 841, 941, 922, 152, 1904, 903, 450, 447,...  \n",
       "                                           2        [142, 543, 930, 872, 865, 1646, 95, 589, 513, ...  \n",
       "                                           3        [1630, 1187, 1237, 1552, 682, 1424, 357, 178, ...  \n",
       "                                           4        [499, 1225, 1423, 438, 1077, 1637, 566, 1410, ...  \n",
       "...                                                                                               ...  \n",
       "                        entropy 4          14       [1416, 1090, 83, 524, 1923, 1278, 1293, 882, 1...  \n",
       "                                           15       [1763, 240, 1153, 246, 1013, 1552, 1818, 1850,...  \n",
       "                                           16       [168, 1248, 1598, 714, 784, 1841, 1253, 1873, ...  \n",
       "                                           17       [1455, 1577, 1788, 1880, 1023, 1458, 1126, 791...  \n",
       "                                           18       [1606, 456, 1572, 1407, 57, 1461, 463, 294, 12...  \n",
       "\n",
       "[475 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf138bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>labeled</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>selected</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>model</th>\n",
       "      <th>sampler</th>\n",
       "      <th>experiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"25\" valign=\"top\">TREC-2</th>\n",
       "      <th rowspan=\"25\" valign=\"top\">BERT</th>\n",
       "      <th rowspan=\"25\" valign=\"top\">BERT</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">core_set</th>\n",
       "      <th>0</th>\n",
       "      <td>[100, 150, 200, 250, 300, 350, 400, 450, 500, ...</td>\n",
       "      <td>[0.6577029973268509, 0.6469762802124024, 0.636...</td>\n",
       "      <td>[0.6748971193415638, 0.551440329218107, 0.5452...</td>\n",
       "      <td>[0.6748971193415638, 0.551440329218107, 0.5452...</td>\n",
       "      <td>[0.6739639945652174, 0.45776695054045197, 0.44...</td>\n",
       "      <td>[[1549, 1546, 1179, 111, 561, 1832, 619, 56, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[100, 150, 200, 250, 300, 350, 400, 450, 500, ...</td>\n",
       "      <td>[0.6459900438785553, 0.6821997761726379, 0.616...</td>\n",
       "      <td>[0.6748971193415638, 0.49588477366255146, 0.56...</td>\n",
       "      <td>[0.6748971193415638, 0.49588477366255146, 0.56...</td>\n",
       "      <td>[0.6708333333333334, 0.3350830657545721, 0.486...</td>\n",
       "      <td>[[998, 1179, 1018, 232, 1471, 696, 1089, 891, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[100, 150, 200, 250, 300, 350, 400, 450, 500, ...</td>\n",
       "      <td>[0.6794537752866745, 0.6754907011985779, 0.601...</td>\n",
       "      <td>[0.6152263374485597, 0.5061728395061729, 0.5, ...</td>\n",
       "      <td>[0.6152263374485597, 0.5061728395061729, 0.5, ...</td>\n",
       "      <td>[0.6121904696881121, 0.36681649403947625, 0.34...</td>\n",
       "      <td>[[454, 565, 154, 1536, 1952, 959, 1490, 816, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[100, 150, 200, 250, 300, 350, 400, 450, 500, ...</td>\n",
       "      <td>[0.6245803534984589, 0.6158845901489258, 0.592...</td>\n",
       "      <td>[0.5555555555555556, 0.49794238683127573, 0.49...</td>\n",
       "      <td>[0.5555555555555556, 0.49794238683127573, 0.49...</td>\n",
       "      <td>[0.4830601953986763, 0.33955622883621456, 0.33...</td>\n",
       "      <td>[[1736, 11, 1693, 1079, 1304, 1144, 1330, 1452...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[100, 150, 200, 250, 300, 350, 400, 450, 500, ...</td>\n",
       "      <td>[0.650259718298912, 0.6353173375129699, 0.6251...</td>\n",
       "      <td>[0.49176954732510286, 0.49382716049382713, 0.4...</td>\n",
       "      <td>[0.49176954732510286, 0.49382716049382713, 0.4...</td>\n",
       "      <td>[0.3296551724137931, 0.3305785123966942, 0.339...</td>\n",
       "      <td>[[1268, 33, 599, 432, 1073, 156, 1319, 589, 81...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">dal</th>\n",
       "      <th>0</th>\n",
       "      <td>[100, 150, 200, 250, 300, 350, 400, 450, 500, ...</td>\n",
       "      <td>[0.6577029973268509, 0.6642346858978272, 0.663...</td>\n",
       "      <td>[0.6748971193415638, 0.6604938271604939, 0.666...</td>\n",
       "      <td>[0.6748971193415638, 0.6604938271604939, 0.666...</td>\n",
       "      <td>[0.6739639945652174, 0.6492540251151437, 0.648...</td>\n",
       "      <td>[[1549, 1546, 1179, 111, 561, 1832, 619, 56, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[100, 150, 200, 250, 300, 350, 400, 450, 500, ...</td>\n",
       "      <td>[0.6459900438785553, 0.6488877415657044, 0.636...</td>\n",
       "      <td>[0.6748971193415638, 0.6893004115226338, 0.703...</td>\n",
       "      <td>[0.6748971193415638, 0.6893004115226338, 0.703...</td>\n",
       "      <td>[0.6708333333333334, 0.6868489888925396, 0.699...</td>\n",
       "      <td>[[998, 1179, 1018, 232, 1471, 696, 1089, 891, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[100, 150, 200, 250, 300, 350, 400, 450, 500, ...</td>\n",
       "      <td>[0.6794537752866745, 0.651800012588501, 0.6412...</td>\n",
       "      <td>[0.6152263374485597, 0.7078189300411523, 0.713...</td>\n",
       "      <td>[0.6152263374485597, 0.7078189300411522, 0.713...</td>\n",
       "      <td>[0.6121904696881121, 0.7060216739367502, 0.706...</td>\n",
       "      <td>[[454, 565, 154, 1536, 1952, 959, 1490, 816, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[100, 150, 200, 250, 300, 350, 400, 450, 500, ...</td>\n",
       "      <td>[0.6245803534984589, 0.6310327291488648, 0.683...</td>\n",
       "      <td>[0.5555555555555556, 0.6851851851851852, 0.716...</td>\n",
       "      <td>[0.5555555555555556, 0.6851851851851852, 0.716...</td>\n",
       "      <td>[0.4830601953986763, 0.6752514510571208, 0.715...</td>\n",
       "      <td>[[1736, 11, 1693, 1079, 1304, 1144, 1330, 1452...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[100, 150, 200, 250, 300, 350, 400, 450, 500, ...</td>\n",
       "      <td>[0.650259718298912, 0.6481092929840088, 0.6514...</td>\n",
       "      <td>[0.49176954732510286, 0.654320987654321, 0.703...</td>\n",
       "      <td>[0.49176954732510286, 0.654320987654321, 0.703...</td>\n",
       "      <td>[0.3296551724137931, 0.6377318306859525, 0.703...</td>\n",
       "      <td>[[1268, 33, 599, 432, 1073, 156, 1319, 589, 81...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">entropy</th>\n",
       "      <th>0</th>\n",
       "      <td>[100, 150, 200, 250, 300, 350, 400, 450, 500, ...</td>\n",
       "      <td>[0.6577029973268509, 0.6654908657073975, 0.656...</td>\n",
       "      <td>[0.6748971193415638, 0.6728395061728395, 0.699...</td>\n",
       "      <td>[0.6748971193415638, 0.6728395061728395, 0.699...</td>\n",
       "      <td>[0.6739639945652174, 0.6620084242018659, 0.698...</td>\n",
       "      <td>[[1549, 1546, 1179, 111, 561, 1832, 619, 56, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[100, 150, 200, 250, 300, 350, 400, 450, 500, ...</td>\n",
       "      <td>[0.6459900438785553, 0.6613013267517089, 0.672...</td>\n",
       "      <td>[0.6748971193415638, 0.6008230452674898, 0.707...</td>\n",
       "      <td>[0.6748971193415638, 0.6008230452674898, 0.707...</td>\n",
       "      <td>[0.6708333333333334, 0.5784343533704148, 0.697...</td>\n",
       "      <td>[[998, 1179, 1018, 232, 1471, 696, 1089, 891, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[100, 150, 200, 250, 300, 350, 400, 450, 500, ...</td>\n",
       "      <td>[0.6794537752866745, 0.6476762533187866, 0.629...</td>\n",
       "      <td>[0.6152263374485597, 0.7119341563786008, 0.563...</td>\n",
       "      <td>[0.6152263374485597, 0.7119341563786008, 0.563...</td>\n",
       "      <td>[0.6121904696881121, 0.6991989248262569, 0.490...</td>\n",
       "      <td>[[454, 565, 154, 1536, 1952, 959, 1490, 816, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[100, 150, 200, 250, 300, 350, 400, 450, 500, ...</td>\n",
       "      <td>[0.6245803534984589, 0.6380608439445495, 0.643...</td>\n",
       "      <td>[0.5555555555555556, 0.7016460905349794, 0.646...</td>\n",
       "      <td>[0.5555555555555556, 0.7016460905349794, 0.646...</td>\n",
       "      <td>[0.4830601953986763, 0.6958658996059679, 0.637...</td>\n",
       "      <td>[[1736, 11, 1693, 1079, 1304, 1144, 1330, 1452...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[100, 150, 200, 250, 300, 350, 400, 450, 500, ...</td>\n",
       "      <td>[0.650259718298912, 0.642036247253418, 0.67306...</td>\n",
       "      <td>[0.49176954732510286, 0.7160493827160493, 0.68...</td>\n",
       "      <td>[0.49176954732510286, 0.7160493827160493, 0.68...</td>\n",
       "      <td>[0.3296551724137931, 0.7139127764127764, 0.689...</td>\n",
       "      <td>[[1268, 33, 599, 432, 1073, 156, 1319, 589, 81...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">entropy_dropout</th>\n",
       "      <th>0</th>\n",
       "      <td>[100, 150, 200, 250, 300, 350, 400, 450, 500, ...</td>\n",
       "      <td>[0.6577029973268509, 0.6431065201759338, 0.632...</td>\n",
       "      <td>[0.6748971193415638, 0.5267489711934157, 0.707...</td>\n",
       "      <td>[0.6748971193415638, 0.5267489711934157, 0.707...</td>\n",
       "      <td>[0.6739639945652174, 0.425531914893617, 0.7068...</td>\n",
       "      <td>[[1549, 1546, 1179, 111, 561, 1832, 619, 56, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[100, 150, 200, 250, 300, 350, 400, 450, 500, ...</td>\n",
       "      <td>[0.6459900438785553, 0.6572281002998352, 0.609...</td>\n",
       "      <td>[0.6748971193415638, 0.6954732510288066, 0.520...</td>\n",
       "      <td>[0.6748971193415638, 0.6954732510288066, 0.520...</td>\n",
       "      <td>[0.6708333333333334, 0.6904352017628426, 0.392...</td>\n",
       "      <td>[[998, 1179, 1018, 232, 1471, 696, 1089, 891, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[100, 150, 200, 250, 300, 350, 400, 450, 500, ...</td>\n",
       "      <td>[0.6794537752866745, 0.6576382756233216, 0.652...</td>\n",
       "      <td>[0.6152263374485597, 0.7222222222222222, 0.722...</td>\n",
       "      <td>[0.6152263374485597, 0.7222222222222222, 0.722...</td>\n",
       "      <td>[0.6121904696881121, 0.7212296318327633, 0.712...</td>\n",
       "      <td>[[454, 565, 154, 1536, 1952, 959, 1490, 816, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[100, 150, 200, 250, 300, 350, 400, 450, 500, ...</td>\n",
       "      <td>[0.6245803534984589, 0.6216179490089416, 0.625...</td>\n",
       "      <td>[0.5555555555555556, 0.6213991769547325, 0.724...</td>\n",
       "      <td>[0.5555555555555556, 0.6213991769547325, 0.724...</td>\n",
       "      <td>[0.4830601953986763, 0.5993548387096774, 0.722...</td>\n",
       "      <td>[[1736, 11, 1693, 1079, 1304, 1144, 1330, 1452...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[100, 150, 200, 250, 300, 350, 400, 450, 500, ...</td>\n",
       "      <td>[0.650259718298912, 0.6512132883071899, 0.6700...</td>\n",
       "      <td>[0.49176954732510286, 0.6625514403292181, 0.62...</td>\n",
       "      <td>[0.49176954732510286, 0.6625514403292181, 0.62...</td>\n",
       "      <td>[0.3296551724137931, 0.6558133107629591, 0.598...</td>\n",
       "      <td>[[1268, 33, 599, 432, 1073, 156, 1319, 589, 81...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">random</th>\n",
       "      <th>0</th>\n",
       "      <td>[100, 150, 200, 250, 300, 350, 400, 450, 500, ...</td>\n",
       "      <td>[0.6577029973268509, 0.6625113010406494, 0.651...</td>\n",
       "      <td>[0.6748971193415638, 0.7098765432098766, 0.695...</td>\n",
       "      <td>[0.6748971193415638, 0.7098765432098766, 0.695...</td>\n",
       "      <td>[0.6739639945652174, 0.7011604530171343, 0.691...</td>\n",
       "      <td>[[1549, 1546, 1179, 111, 561, 1832, 619, 56, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[100, 150, 200, 250, 300, 350, 400, 450, 500, ...</td>\n",
       "      <td>[0.6459900438785553, 0.6732913494110108, 0.649...</td>\n",
       "      <td>[0.6748971193415638, 0.5987654320987654, 0.730...</td>\n",
       "      <td>[0.6748971193415638, 0.5987654320987654, 0.730...</td>\n",
       "      <td>[0.6708333333333334, 0.5652173913043478, 0.730...</td>\n",
       "      <td>[[998, 1179, 1018, 232, 1471, 696, 1089, 891, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[100, 150, 200, 250, 300, 350, 400, 450, 500, ...</td>\n",
       "      <td>[0.6794537752866745, 0.6374668717384339, 0.640...</td>\n",
       "      <td>[0.6152263374485597, 0.5534979423868313, 0.547...</td>\n",
       "      <td>[0.6152263374485597, 0.5534979423868313, 0.547...</td>\n",
       "      <td>[0.6121904696881121, 0.48521723850107634, 0.46...</td>\n",
       "      <td>[[454, 565, 154, 1536, 1952, 959, 1490, 816, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[100, 150, 200, 250, 300, 350, 400, 450, 500, ...</td>\n",
       "      <td>[0.6245803534984589, 0.6433198928833008, 0.631...</td>\n",
       "      <td>[0.5555555555555556, 0.6090534979423868, 0.730...</td>\n",
       "      <td>[0.5555555555555556, 0.6090534979423868, 0.730...</td>\n",
       "      <td>[0.4830601953986763, 0.589527027027027, 0.7304...</td>\n",
       "      <td>[[1736, 11, 1693, 1079, 1304, 1144, 1330, 1452...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[100, 150, 200, 250, 300, 350, 400, 450, 500, ...</td>\n",
       "      <td>[0.650259718298912, 0.6340792775154114, 0.6493...</td>\n",
       "      <td>[0.49176954732510286, 0.588477366255144, 0.722...</td>\n",
       "      <td>[0.49176954732510286, 0.588477366255144, 0.722...</td>\n",
       "      <td>[0.3296551724137931, 0.5326293924182102, 0.721...</td>\n",
       "      <td>[[1268, 33, 599, 432, 1073, 156, 1319, 589, 81...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                          labeled  \\\n",
       "dataset model model sampler         experiment                                                      \n",
       "TREC-2  BERT  BERT  core_set        0           [100, 150, 200, 250, 300, 350, 400, 450, 500, ...   \n",
       "                                    1           [100, 150, 200, 250, 300, 350, 400, 450, 500, ...   \n",
       "                                    2           [100, 150, 200, 250, 300, 350, 400, 450, 500, ...   \n",
       "                                    3           [100, 150, 200, 250, 300, 350, 400, 450, 500, ...   \n",
       "                                    4           [100, 150, 200, 250, 300, 350, 400, 450, 500, ...   \n",
       "                    dal             0           [100, 150, 200, 250, 300, 350, 400, 450, 500, ...   \n",
       "                                    1           [100, 150, 200, 250, 300, 350, 400, 450, 500, ...   \n",
       "                                    2           [100, 150, 200, 250, 300, 350, 400, 450, 500, ...   \n",
       "                                    3           [100, 150, 200, 250, 300, 350, 400, 450, 500, ...   \n",
       "                                    4           [100, 150, 200, 250, 300, 350, 400, 450, 500, ...   \n",
       "                    entropy         0           [100, 150, 200, 250, 300, 350, 400, 450, 500, ...   \n",
       "                                    1           [100, 150, 200, 250, 300, 350, 400, 450, 500, ...   \n",
       "                                    2           [100, 150, 200, 250, 300, 350, 400, 450, 500, ...   \n",
       "                                    3           [100, 150, 200, 250, 300, 350, 400, 450, 500, ...   \n",
       "                                    4           [100, 150, 200, 250, 300, 350, 400, 450, 500, ...   \n",
       "                    entropy_dropout 0           [100, 150, 200, 250, 300, 350, 400, 450, 500, ...   \n",
       "                                    1           [100, 150, 200, 250, 300, 350, 400, 450, 500, ...   \n",
       "                                    2           [100, 150, 200, 250, 300, 350, 400, 450, 500, ...   \n",
       "                                    3           [100, 150, 200, 250, 300, 350, 400, 450, 500, ...   \n",
       "                                    4           [100, 150, 200, 250, 300, 350, 400, 450, 500, ...   \n",
       "                    random          0           [100, 150, 200, 250, 300, 350, 400, 450, 500, ...   \n",
       "                                    1           [100, 150, 200, 250, 300, 350, 400, 450, 500, ...   \n",
       "                                    2           [100, 150, 200, 250, 300, 350, 400, 450, 500, ...   \n",
       "                                    3           [100, 150, 200, 250, 300, 350, 400, 450, 500, ...   \n",
       "                                    4           [100, 150, 200, 250, 300, 350, 400, 450, 500, ...   \n",
       "\n",
       "                                                                                       train_loss  \\\n",
       "dataset model model sampler         experiment                                                      \n",
       "TREC-2  BERT  BERT  core_set        0           [0.6577029973268509, 0.6469762802124024, 0.636...   \n",
       "                                    1           [0.6459900438785553, 0.6821997761726379, 0.616...   \n",
       "                                    2           [0.6794537752866745, 0.6754907011985779, 0.601...   \n",
       "                                    3           [0.6245803534984589, 0.6158845901489258, 0.592...   \n",
       "                                    4           [0.650259718298912, 0.6353173375129699, 0.6251...   \n",
       "                    dal             0           [0.6577029973268509, 0.6642346858978272, 0.663...   \n",
       "                                    1           [0.6459900438785553, 0.6488877415657044, 0.636...   \n",
       "                                    2           [0.6794537752866745, 0.651800012588501, 0.6412...   \n",
       "                                    3           [0.6245803534984589, 0.6310327291488648, 0.683...   \n",
       "                                    4           [0.650259718298912, 0.6481092929840088, 0.6514...   \n",
       "                    entropy         0           [0.6577029973268509, 0.6654908657073975, 0.656...   \n",
       "                                    1           [0.6459900438785553, 0.6613013267517089, 0.672...   \n",
       "                                    2           [0.6794537752866745, 0.6476762533187866, 0.629...   \n",
       "                                    3           [0.6245803534984589, 0.6380608439445495, 0.643...   \n",
       "                                    4           [0.650259718298912, 0.642036247253418, 0.67306...   \n",
       "                    entropy_dropout 0           [0.6577029973268509, 0.6431065201759338, 0.632...   \n",
       "                                    1           [0.6459900438785553, 0.6572281002998352, 0.609...   \n",
       "                                    2           [0.6794537752866745, 0.6576382756233216, 0.652...   \n",
       "                                    3           [0.6245803534984589, 0.6216179490089416, 0.625...   \n",
       "                                    4           [0.650259718298912, 0.6512132883071899, 0.6700...   \n",
       "                    random          0           [0.6577029973268509, 0.6625113010406494, 0.651...   \n",
       "                                    1           [0.6459900438785553, 0.6732913494110108, 0.649...   \n",
       "                                    2           [0.6794537752866745, 0.6374668717384339, 0.640...   \n",
       "                                    3           [0.6245803534984589, 0.6433198928833008, 0.631...   \n",
       "                                    4           [0.650259718298912, 0.6340792775154114, 0.6493...   \n",
       "\n",
       "                                                                                    test_accuracy  \\\n",
       "dataset model model sampler         experiment                                                      \n",
       "TREC-2  BERT  BERT  core_set        0           [0.6748971193415638, 0.551440329218107, 0.5452...   \n",
       "                                    1           [0.6748971193415638, 0.49588477366255146, 0.56...   \n",
       "                                    2           [0.6152263374485597, 0.5061728395061729, 0.5, ...   \n",
       "                                    3           [0.5555555555555556, 0.49794238683127573, 0.49...   \n",
       "                                    4           [0.49176954732510286, 0.49382716049382713, 0.4...   \n",
       "                    dal             0           [0.6748971193415638, 0.6604938271604939, 0.666...   \n",
       "                                    1           [0.6748971193415638, 0.6893004115226338, 0.703...   \n",
       "                                    2           [0.6152263374485597, 0.7078189300411523, 0.713...   \n",
       "                                    3           [0.5555555555555556, 0.6851851851851852, 0.716...   \n",
       "                                    4           [0.49176954732510286, 0.654320987654321, 0.703...   \n",
       "                    entropy         0           [0.6748971193415638, 0.6728395061728395, 0.699...   \n",
       "                                    1           [0.6748971193415638, 0.6008230452674898, 0.707...   \n",
       "                                    2           [0.6152263374485597, 0.7119341563786008, 0.563...   \n",
       "                                    3           [0.5555555555555556, 0.7016460905349794, 0.646...   \n",
       "                                    4           [0.49176954732510286, 0.7160493827160493, 0.68...   \n",
       "                    entropy_dropout 0           [0.6748971193415638, 0.5267489711934157, 0.707...   \n",
       "                                    1           [0.6748971193415638, 0.6954732510288066, 0.520...   \n",
       "                                    2           [0.6152263374485597, 0.7222222222222222, 0.722...   \n",
       "                                    3           [0.5555555555555556, 0.6213991769547325, 0.724...   \n",
       "                                    4           [0.49176954732510286, 0.6625514403292181, 0.62...   \n",
       "                    random          0           [0.6748971193415638, 0.7098765432098766, 0.695...   \n",
       "                                    1           [0.6748971193415638, 0.5987654320987654, 0.730...   \n",
       "                                    2           [0.6152263374485597, 0.5534979423868313, 0.547...   \n",
       "                                    3           [0.5555555555555556, 0.6090534979423868, 0.730...   \n",
       "                                    4           [0.49176954732510286, 0.588477366255144, 0.722...   \n",
       "\n",
       "                                                                                         f1_micro  \\\n",
       "dataset model model sampler         experiment                                                      \n",
       "TREC-2  BERT  BERT  core_set        0           [0.6748971193415638, 0.551440329218107, 0.5452...   \n",
       "                                    1           [0.6748971193415638, 0.49588477366255146, 0.56...   \n",
       "                                    2           [0.6152263374485597, 0.5061728395061729, 0.5, ...   \n",
       "                                    3           [0.5555555555555556, 0.49794238683127573, 0.49...   \n",
       "                                    4           [0.49176954732510286, 0.49382716049382713, 0.4...   \n",
       "                    dal             0           [0.6748971193415638, 0.6604938271604939, 0.666...   \n",
       "                                    1           [0.6748971193415638, 0.6893004115226338, 0.703...   \n",
       "                                    2           [0.6152263374485597, 0.7078189300411522, 0.713...   \n",
       "                                    3           [0.5555555555555556, 0.6851851851851852, 0.716...   \n",
       "                                    4           [0.49176954732510286, 0.654320987654321, 0.703...   \n",
       "                    entropy         0           [0.6748971193415638, 0.6728395061728395, 0.699...   \n",
       "                                    1           [0.6748971193415638, 0.6008230452674898, 0.707...   \n",
       "                                    2           [0.6152263374485597, 0.7119341563786008, 0.563...   \n",
       "                                    3           [0.5555555555555556, 0.7016460905349794, 0.646...   \n",
       "                                    4           [0.49176954732510286, 0.7160493827160493, 0.68...   \n",
       "                    entropy_dropout 0           [0.6748971193415638, 0.5267489711934157, 0.707...   \n",
       "                                    1           [0.6748971193415638, 0.6954732510288066, 0.520...   \n",
       "                                    2           [0.6152263374485597, 0.7222222222222222, 0.722...   \n",
       "                                    3           [0.5555555555555556, 0.6213991769547325, 0.724...   \n",
       "                                    4           [0.49176954732510286, 0.6625514403292181, 0.62...   \n",
       "                    random          0           [0.6748971193415638, 0.7098765432098766, 0.695...   \n",
       "                                    1           [0.6748971193415638, 0.5987654320987654, 0.730...   \n",
       "                                    2           [0.6152263374485597, 0.5534979423868313, 0.547...   \n",
       "                                    3           [0.5555555555555556, 0.6090534979423868, 0.730...   \n",
       "                                    4           [0.49176954732510286, 0.588477366255144, 0.722...   \n",
       "\n",
       "                                                                                         f1_macro  \\\n",
       "dataset model model sampler         experiment                                                      \n",
       "TREC-2  BERT  BERT  core_set        0           [0.6739639945652174, 0.45776695054045197, 0.44...   \n",
       "                                    1           [0.6708333333333334, 0.3350830657545721, 0.486...   \n",
       "                                    2           [0.6121904696881121, 0.36681649403947625, 0.34...   \n",
       "                                    3           [0.4830601953986763, 0.33955622883621456, 0.33...   \n",
       "                                    4           [0.3296551724137931, 0.3305785123966942, 0.339...   \n",
       "                    dal             0           [0.6739639945652174, 0.6492540251151437, 0.648...   \n",
       "                                    1           [0.6708333333333334, 0.6868489888925396, 0.699...   \n",
       "                                    2           [0.6121904696881121, 0.7060216739367502, 0.706...   \n",
       "                                    3           [0.4830601953986763, 0.6752514510571208, 0.715...   \n",
       "                                    4           [0.3296551724137931, 0.6377318306859525, 0.703...   \n",
       "                    entropy         0           [0.6739639945652174, 0.6620084242018659, 0.698...   \n",
       "                                    1           [0.6708333333333334, 0.5784343533704148, 0.697...   \n",
       "                                    2           [0.6121904696881121, 0.6991989248262569, 0.490...   \n",
       "                                    3           [0.4830601953986763, 0.6958658996059679, 0.637...   \n",
       "                                    4           [0.3296551724137931, 0.7139127764127764, 0.689...   \n",
       "                    entropy_dropout 0           [0.6739639945652174, 0.425531914893617, 0.7068...   \n",
       "                                    1           [0.6708333333333334, 0.6904352017628426, 0.392...   \n",
       "                                    2           [0.6121904696881121, 0.7212296318327633, 0.712...   \n",
       "                                    3           [0.4830601953986763, 0.5993548387096774, 0.722...   \n",
       "                                    4           [0.3296551724137931, 0.6558133107629591, 0.598...   \n",
       "                    random          0           [0.6739639945652174, 0.7011604530171343, 0.691...   \n",
       "                                    1           [0.6708333333333334, 0.5652173913043478, 0.730...   \n",
       "                                    2           [0.6121904696881121, 0.48521723850107634, 0.46...   \n",
       "                                    3           [0.4830601953986763, 0.589527027027027, 0.7304...   \n",
       "                                    4           [0.3296551724137931, 0.5326293924182102, 0.721...   \n",
       "\n",
       "                                                                                         selected  \n",
       "dataset model model sampler         experiment                                                     \n",
       "TREC-2  BERT  BERT  core_set        0           [[1549, 1546, 1179, 111, 561, 1832, 619, 56, 5...  \n",
       "                                    1           [[998, 1179, 1018, 232, 1471, 696, 1089, 891, ...  \n",
       "                                    2           [[454, 565, 154, 1536, 1952, 959, 1490, 816, 1...  \n",
       "                                    3           [[1736, 11, 1693, 1079, 1304, 1144, 1330, 1452...  \n",
       "                                    4           [[1268, 33, 599, 432, 1073, 156, 1319, 589, 81...  \n",
       "                    dal             0           [[1549, 1546, 1179, 111, 561, 1832, 619, 56, 5...  \n",
       "                                    1           [[998, 1179, 1018, 232, 1471, 696, 1089, 891, ...  \n",
       "                                    2           [[454, 565, 154, 1536, 1952, 959, 1490, 816, 1...  \n",
       "                                    3           [[1736, 11, 1693, 1079, 1304, 1144, 1330, 1452...  \n",
       "                                    4           [[1268, 33, 599, 432, 1073, 156, 1319, 589, 81...  \n",
       "                    entropy         0           [[1549, 1546, 1179, 111, 561, 1832, 619, 56, 5...  \n",
       "                                    1           [[998, 1179, 1018, 232, 1471, 696, 1089, 891, ...  \n",
       "                                    2           [[454, 565, 154, 1536, 1952, 959, 1490, 816, 1...  \n",
       "                                    3           [[1736, 11, 1693, 1079, 1304, 1144, 1330, 1452...  \n",
       "                                    4           [[1268, 33, 599, 432, 1073, 156, 1319, 589, 81...  \n",
       "                    entropy_dropout 0           [[1549, 1546, 1179, 111, 561, 1832, 619, 56, 5...  \n",
       "                                    1           [[998, 1179, 1018, 232, 1471, 696, 1089, 891, ...  \n",
       "                                    2           [[454, 565, 154, 1536, 1952, 959, 1490, 816, 1...  \n",
       "                                    3           [[1736, 11, 1693, 1079, 1304, 1144, 1330, 1452...  \n",
       "                                    4           [[1268, 33, 599, 432, 1073, 156, 1319, 589, 81...  \n",
       "                    random          0           [[1549, 1546, 1179, 111, 561, 1832, 619, 56, 5...  \n",
       "                                    1           [[998, 1179, 1018, 232, 1471, 696, 1089, 891, ...  \n",
       "                                    2           [[454, 565, 154, 1536, 1952, 959, 1490, 816, 1...  \n",
       "                                    3           [[1736, 11, 1693, 1079, 1304, 1144, 1330, 1452...  \n",
       "                                    4           [[1268, 33, 599, 432, 1073, 156, 1319, 589, 81...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tr.groupby([\"dataset\", \"model\", \"model\", \"sampler\", \"experiment\"]).agg(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5aa2b72f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad4e86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr.groupby([\"dataset\", \"model\", \"model\", \"sampler\", \"experiment\"]).agg(list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78f7c53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "from podium import Vocab, Field, LabelField, Iterator  # , BucketIterator\n",
    "from podium.datasets import TabularDataset\n",
    "from podium.datasets.hf import HFDatasetConverter\n",
    "from podium.vectorizers import GloVe\n",
    "from podium.utils.general_utils import repr_type_and_attrs\n",
    "\n",
    "from typing import Iterator as PythonIterator\n",
    "from typing import NamedTuple, Tuple\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from util import Config\n",
    "\n",
    "\n",
    "class BucketIterator(Iterator):\n",
    "    \"\"\"\n",
    "    Creates a bucket iterator which uses a look-ahead heuristic to batch\n",
    "    examples in a way that minimizes the amount of necessary padding.\n",
    "\n",
    "    Uses a bucket of size N x batch_size, and sorts instances within the bucket\n",
    "    before splitting into batches, minimizing necessary padding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset=None,\n",
    "        batch_size=32,\n",
    "        sort_key=None,\n",
    "        shuffle=True,\n",
    "        seed=1,\n",
    "        matrix_class=np.array,\n",
    "        internal_random_state=None,\n",
    "        look_ahead_multiplier=100,\n",
    "        bucket_sort_key=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Creates a BucketIterator with the given bucket sort key and look-ahead\n",
    "        multiplier (how many batch_sizes to look ahead when sorting examples for\n",
    "        batches).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        look_ahead_multiplier : int\n",
    "            Multiplier of ``batch_size`` which determines the size of the\n",
    "            look-ahead bucket.\n",
    "            If ``look_ahead_multiplier == 1``, then the BucketIterator behaves\n",
    "            like a normal Iterator.\n",
    "            If ``look_ahead_multiplier >= (num_examples / batch_size)``, then\n",
    "            the BucketIterator behaves like a normal iterator that sorts the\n",
    "            whole dataset.\n",
    "            Default is ``100``.\n",
    "            The callable object used to sort examples in the bucket.\n",
    "            If ``bucket_sort_key=None``, then the ``sort_key`` must not be ``None``,\n",
    "            otherwise a ``ValueError`` is raised.\n",
    "            Default is ``None``.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If sort_key and bucket_sort_key are both None.\n",
    "        \"\"\"\n",
    "\n",
    "        if sort_key is None and bucket_sort_key is None:\n",
    "            raise ValueError(\n",
    "                \"For BucketIterator to work, either sort_key or \"\n",
    "                \"bucket_sort_key must be != None.\"\n",
    "            )\n",
    "\n",
    "        super().__init__(\n",
    "            dataset,\n",
    "            batch_size,\n",
    "            sort_key=sort_key,\n",
    "            shuffle=shuffle,\n",
    "            seed=seed,\n",
    "            matrix_class=matrix_class,\n",
    "            internal_random_state=internal_random_state,\n",
    "        )\n",
    "\n",
    "        self.bucket_sort_key = bucket_sort_key\n",
    "        self.look_ahead_multiplier = look_ahead_multiplier\n",
    "\n",
    "    def __iter__(self) -> PythonIterator[Tuple[NamedTuple, NamedTuple]]:\n",
    "        step = self.batch_size * self.look_ahead_multiplier\n",
    "        dataset = self._dataset\n",
    "\n",
    "        # Fix: Shuffle dataset if the shuffle is turned on, only IF sort key is not none\n",
    "        if self._shuffle and self._sort_key is None:\n",
    "            indices = list(range(len(dataset)))\n",
    "            # Cache state prior to shuffle so we can use it when unpickling\n",
    "            self._shuffler_state = self.get_internal_random_state()\n",
    "            self._shuffler.shuffle(indices)\n",
    "            # dataset.shuffle_examples(random_state=self._shuffler_state)\n",
    "            dataset = dataset[indices]\n",
    "\n",
    "        # Determine the step where iteration was stopped for lookahead & within bucket\n",
    "        lookahead_start = (\n",
    "            self.iterations // self.look_ahead_multiplier * self.look_ahead_multiplier\n",
    "        )\n",
    "        batch_start = self.iterations % self.look_ahead_multiplier\n",
    "\n",
    "        if self._sort_key is not None:\n",
    "            dataset = dataset.sorted(key=self._sort_key)\n",
    "        for i in range(lookahead_start, len(dataset), step):\n",
    "            bucket = dataset[i : i + step]\n",
    "\n",
    "            if self.bucket_sort_key is not None:\n",
    "                bucket = bucket.sorted(key=self.bucket_sort_key)\n",
    "\n",
    "            for j in range(batch_start, len(bucket), self.batch_size):\n",
    "                batch_dataset = bucket[j : j + self.batch_size]\n",
    "                batch = self._create_batch(batch_dataset)\n",
    "\n",
    "                yield batch\n",
    "                self._iterations += 1\n",
    "\n",
    "        # prepare for new epoch\n",
    "        self._iterations = 0\n",
    "        self._epoch += 1\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        attrs = {\n",
    "            \"batch_size\": self._batch_size,\n",
    "            \"epoch\": self._epoch,\n",
    "            \"iteration\": self._iterations,\n",
    "            \"shuffle\": self._shuffle,\n",
    "            \"look_ahead_multiplier\": self.look_ahead_multiplier,\n",
    "        }\n",
    "        return repr_type_and_attrs(self, attrs, with_newlines=True)\n",
    "\n",
    "\n",
    "class TokenizerVocabWrapper:\n",
    "    def __init__(self, tokenizer):\n",
    "        # wrap BertTokenizer so the method signatures align with podium\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def get_padding_index(self):\n",
    "        return self.tokenizer.convert_tokens_to_ids(self.tokenizer.pad_token)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenizer)\n",
    "\n",
    "    def numericalize(self, instance):\n",
    "        # Equivalent to .encode, but I want to delineate the steps\n",
    "        return self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(instance))\n",
    "\n",
    "\n",
    "def load_embeddings(vocab, name=\"glove\"):\n",
    "    if name == \"glove\":\n",
    "        glove = GloVe()\n",
    "        embeddings = glove.load_vocab(vocab)\n",
    "        return embeddings\n",
    "    else:\n",
    "        raise ValueError(f\"Wrong embedding key provided {name}\")\n",
    "\n",
    "\n",
    "def make_iterable(dataset, device, batch_size=32, train=False, indices=None):\n",
    "    \"\"\"\n",
    "    Construct a DataLoader from a podium Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def instance_length(instance):\n",
    "        raw, tokenized = instance.text\n",
    "        return -len(tokenized)\n",
    "\n",
    "    def cast_to_device(data):\n",
    "        return torch.tensor(np.array(data), device=device)\n",
    "\n",
    "    # Selects examples at given indices to support subset iteration.\n",
    "    if indices is not None:\n",
    "        dataset = dataset[indices]\n",
    "\n",
    "    # iterator = BucketIterator(\n",
    "    #     dataset,\n",
    "    #     batch_size=batch_size,\n",
    "    #     sort_key=instance_length,\n",
    "    #     shuffle=train,\n",
    "    #     matrix_class=cast_to_device,\n",
    "    #     look_ahead_multiplier=20,\n",
    "    # )\n",
    "\n",
    "    iterator = Iterator(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=train,\n",
    "        matrix_class=cast_to_device,\n",
    "    )\n",
    "\n",
    "    return iterator\n",
    "\n",
    "\n",
    "class Instance:\n",
    "    def __init__(self, index, text, label, extras=None):\n",
    "        self.index = index\n",
    "        self.text = text\n",
    "        self.label = label\n",
    "        self.extras = extras\n",
    "        self.length = len(text)  # text is already tokenized & filtered\n",
    "\n",
    "    def set_mask(self, masked_text, masked_labels):\n",
    "        # Set the masking as an attribute\n",
    "        self.masked_text = masked_text\n",
    "        self.masked_labels = masked_labels\n",
    "\n",
    "    def set_numericalized(self, indices, target):\n",
    "        self.numericalized_text = indices\n",
    "        self.numericalized_label = target\n",
    "        self.length = len(indices)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.index}: {self.length}, {self.label}\"\n",
    "\n",
    "\n",
    "def generate_eraser_rationale_mask(tokens, evidences):\n",
    "    mask = torch.zeros(len(tokens))  # zeros for where you can attend to\n",
    "\n",
    "    any_evidence_left = False\n",
    "    for ev in evidences:\n",
    "        if ev.start_token > len(tokens) or ev.end_token > len(tokens):\n",
    "            continue  # evidence out of span\n",
    "\n",
    "        if not any_evidence_left:\n",
    "            any_evidence_left = True\n",
    "        # 1. Validate\n",
    "\n",
    "        assert ev.text == \" \".join(\n",
    "            tokens[ev.start_token : ev.end_token]\n",
    "        ), \"Texts dont match; did you filter some tokens?\"\n",
    "\n",
    "        mask[ev.start_token : ev.end_token] = 1\n",
    "    return mask\n",
    "\n",
    "\n",
    "def load_tse(\n",
    "    train_path=\"data/TSE/train.csv\", test_path=\"data/TSE/test.csv\", max_size=20000\n",
    "):\n",
    "\n",
    "    vocab = Vocab(max_size=max_size)\n",
    "    fields = [\n",
    "        Field(\"id\", numericalizer=None),\n",
    "        Field(\"text\", numericalizer=vocab, include_lengths=True),\n",
    "        Field(\"rationale\", numericalizer=vocab),\n",
    "        LabelField(\"label\"),\n",
    "    ]\n",
    "    train_dataset = TabularDataset(\n",
    "        train_path, format=\"csv\", fields=fields, skip_header=True\n",
    "    )\n",
    "    test_dataset = TabularDataset(\n",
    "        test_path, format=\"csv\", fields=fields, skip_header=True\n",
    "    )\n",
    "    train_dataset.finalize_fields()\n",
    "    return (train_dataset, test_dataset), vocab\n",
    "\n",
    "\n",
    "class MaxLenHook:\n",
    "    def __init__(self, max_len):\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __call__(self, raw, tokenized):\n",
    "        return raw, tokenized[: self.max_len]\n",
    "\n",
    "\n",
    "def lowercase_hook(raw, tokenized):\n",
    "    return raw, [tok.lower() for tok in tokenized]\n",
    "\n",
    "\n",
    "def isalnum(token):\n",
    "    return any(c.isalnum() for c in token)\n",
    "\n",
    "\n",
    "def remove_nonalnum(raw, tokenized):\n",
    "    # Remove non alphanumeric tokens\n",
    "    return raw, [tok for tok in tokenized if isalnum(tok)]\n",
    "\n",
    "\n",
    "def load_imdb(\n",
    "    meta,\n",
    "    tokenizer=None,\n",
    "    max_vocab_size=20_000,\n",
    "    max_seq_len=200,\n",
    "):\n",
    "\n",
    "    return load_dataset(\n",
    "        \"data/IMDB\",\n",
    "        meta=meta,\n",
    "        tokenizer=tokenizer,\n",
    "        max_vocab_size=max_vocab_size,\n",
    "        max_seq_len=max_seq_len,\n",
    "    )\n",
    "\n",
    "\n",
    "def load_isear(\n",
    "    meta,\n",
    "    tokenizer=None,\n",
    "    max_vocab_size=20_000,\n",
    "    max_seq_len=200,\n",
    "):\n",
    "\n",
    "    return load_dataset(\n",
    "        \"data/ISEAR\",\n",
    "        meta=meta,\n",
    "        tokenizer=tokenizer,\n",
    "        max_vocab_size=max_vocab_size,\n",
    "        max_seq_len=max_seq_len,\n",
    "    )\n",
    "\n",
    "\n",
    "def load_agn2(\n",
    "    meta,\n",
    "    tokenizer=None,\n",
    "    max_vocab_size=20_000,\n",
    "    max_seq_len=200,\n",
    "):\n",
    "\n",
    "    return load_dataset(\n",
    "        \"data/AGN-2\",\n",
    "        meta=meta,\n",
    "        tokenizer=tokenizer,\n",
    "        max_vocab_size=max_vocab_size,\n",
    "        max_seq_len=max_seq_len,\n",
    "    )\n",
    "\n",
    "\n",
    "def load_agn4(\n",
    "    meta,\n",
    "    tokenizer=None,\n",
    "    max_vocab_size=20_000,\n",
    "    max_seq_len=200,\n",
    "):\n",
    "\n",
    "    return load_dataset(\n",
    "        \"data/AGN-4\",\n",
    "        meta=meta,\n",
    "        tokenizer=tokenizer,\n",
    "        max_vocab_size=max_vocab_size,\n",
    "        max_seq_len=max_seq_len,\n",
    "    )\n",
    "\n",
    "\n",
    "def load_mnli(\n",
    "    meta,\n",
    "    tokenizer=None,\n",
    "    max_vocab_size=20_000,\n",
    "    max_seq_len=200,\n",
    "):\n",
    "    return load_sequence_pair_dataset(\n",
    "        \"data/GLUE/MNLI\",\n",
    "        meta=meta,\n",
    "        tokenizer=tokenizer,\n",
    "        max_vocab_size=max_vocab_size,\n",
    "        max_seq_len=max_seq_len,\n",
    "    )\n",
    "\n",
    "\n",
    "def load_mrpc(\n",
    "    meta,\n",
    "    tokenizer=None,\n",
    "    max_vocab_size=20_000,\n",
    "    max_seq_len=200,\n",
    "):\n",
    "    return load_sequence_pair_dataset(\n",
    "        \"data/GLUE/MRPC\",\n",
    "        meta=meta,\n",
    "        tokenizer=tokenizer,\n",
    "        max_vocab_size=max_vocab_size,\n",
    "        max_seq_len=max_seq_len,\n",
    "    )\n",
    "\n",
    "\n",
    "def load_qqp(\n",
    "    meta,\n",
    "    tokenizer=None,\n",
    "    max_vocab_size=20_000,\n",
    "    max_seq_len=200,\n",
    "):\n",
    "    return load_sequence_pair_dataset(\n",
    "        \"data/GLUE/QQP\",\n",
    "        meta=meta,\n",
    "        tokenizer=tokenizer,\n",
    "        max_vocab_size=max_vocab_size,\n",
    "        max_seq_len=max_seq_len,\n",
    "    )\n",
    "\n",
    "\n",
    "def test_load_cola(meta, tok):\n",
    "    splits, vocab = load_cola(meta, tok)\n",
    "    print(vocab)\n",
    "    train, valid, test = splits\n",
    "    print(len(train), len(valid), len(test))\n",
    "\n",
    "    print(train)\n",
    "    print(train[0])\n",
    "\n",
    "    device = torch.device(\"cpu\")\n",
    "    train_iter = make_iterable(test, device, batch_size=2)\n",
    "    batch = next(iter(train_iter))\n",
    "\n",
    "    print(batch)\n",
    "    text, length = batch.text\n",
    "    print(length[0])\n",
    "    print(vocab.get_padding_index())\n",
    "\n",
    "\n",
    "def load_sequence_pair_dataset(\n",
    "    data_dir, meta, tokenizer=None, max_vocab_size=20_000, max_seq_len=200\n",
    "):\n",
    "\n",
    "    # Use BERT subword tokenization\n",
    "    vocab = TokenizerVocabWrapper(tokenizer)\n",
    "    print(vocab.get_padding_index())\n",
    "    pad_index = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "    fields = [\n",
    "        Field(\"id\", disable_batch_matrix=True),\n",
    "        Field(\n",
    "            \"sequence1\",\n",
    "            tokenizer=tokenizer.tokenize,\n",
    "            padding_token=pad_index,\n",
    "            numericalizer=tokenizer.convert_tokens_to_ids,\n",
    "            include_lengths=True,\n",
    "            posttokenize_hooks=[\n",
    "                remove_nonalnum,\n",
    "                MaxLenHook(max_seq_len),\n",
    "                lowercase_hook,\n",
    "            ],\n",
    "        ),\n",
    "        Field(\n",
    "            \"sequence2\",\n",
    "            tokenizer=tokenizer.tokenize,\n",
    "            padding_token=pad_index,\n",
    "            numericalizer=tokenizer.convert_tokens_to_ids,\n",
    "            include_lengths=True,\n",
    "            posttokenize_hooks=[\n",
    "                remove_nonalnum,\n",
    "                MaxLenHook(max_seq_len),\n",
    "                lowercase_hook,\n",
    "            ],\n",
    "        ),\n",
    "        LabelField(\"label\"),\n",
    "    ]\n",
    "\n",
    "    train = TabularDataset(\n",
    "        os.path.join(data_dir, \"train.csv\"), format=\"csv\", fields=fields\n",
    "    )\n",
    "    val = TabularDataset(\n",
    "        os.path.join(data_dir, \"validation.csv\"), format=\"csv\", fields=fields\n",
    "    )\n",
    "    test = TabularDataset(\n",
    "        os.path.join(data_dir, \"test.csv\"), format=\"csv\", fields=fields\n",
    "    )\n",
    "\n",
    "    train.finalize_fields()\n",
    "\n",
    "    meta.vocab = vocab\n",
    "    meta.num_tokens = len(vocab)\n",
    "    meta.padding_idx = vocab.get_padding_index()\n",
    "    meta.num_labels = len(train.field(\"label\").vocab)\n",
    "\n",
    "    return (train, val, test), vocab\n",
    "\n",
    "\n",
    "def load_dataset(\n",
    "    data_dir, meta, tokenizer=None, max_vocab_size=20_000, max_seq_len=200\n",
    "):\n",
    "\n",
    "    # Use BERT subword tokenization\n",
    "    vocab = TokenizerVocabWrapper(tokenizer)\n",
    "    pad_index = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "    fields = [\n",
    "        Field(\"id\", disable_batch_matrix=True),\n",
    "        Field(\n",
    "            \"text\",\n",
    "            tokenizer=tokenizer.tokenize,\n",
    "            padding_token=pad_index,\n",
    "            numericalizer=tokenizer.convert_tokens_to_ids,\n",
    "            include_lengths=True,\n",
    "            posttokenize_hooks=[\n",
    "                remove_nonalnum,\n",
    "                MaxLenHook(max_seq_len),\n",
    "                lowercase_hook,\n",
    "            ],\n",
    "        ),\n",
    "#         LabelField(\"label\"),\n",
    "    ]\n",
    "\n",
    "    train = TabularDataset(\n",
    "        os.path.join(data_dir, \"train.csv\"), format=\"csv\", fields=fields\n",
    "    )\n",
    "    val = TabularDataset(\n",
    "        os.path.join(data_dir, \"validation.csv\"), format=\"csv\", fields=fields\n",
    "    )\n",
    "    test = TabularDataset(\n",
    "        os.path.join(data_dir, \"test.csv\"), format=\"csv\", fields=fields\n",
    "    )\n",
    "\n",
    "    train.finalize_fields()\n",
    "\n",
    "    meta.vocab = vocab\n",
    "    meta.num_tokens = len(vocab)\n",
    "    meta.padding_idx = vocab.get_padding_index()\n",
    "    meta.num_labels = len(train.field(\"label\").vocab)\n",
    "\n",
    "    return (train, val, test), vocab\n",
    "\n",
    "\n",
    "def load_sst(\n",
    "    meta,\n",
    "    tokenizer=None,\n",
    "    max_vocab_size=20_000,\n",
    "    max_seq_len=200,\n",
    "):\n",
    "    return load_dataset(\n",
    "        \"data/GLUE/SST-2\",\n",
    "        meta=meta,\n",
    "        tokenizer=tokenizer,\n",
    "        max_vocab_size=max_vocab_size,\n",
    "        max_seq_len=max_seq_len,\n",
    "    )\n",
    "\n",
    "\n",
    "def test_load_sst(max_vocab_size=20_000, max_seq_len=200):\n",
    "    splits, vocab = load_sst()\n",
    "    print(vocab)\n",
    "    train, valid, test = splits\n",
    "    print(len(train), len(valid), len(test))\n",
    "\n",
    "    print(train)\n",
    "    print(train[0])\n",
    "\n",
    "    device = torch.device(\"cpu\")\n",
    "    train_iter = make_iterable(train, device, batch_size=2)\n",
    "    batch = next(iter(train_iter))\n",
    "\n",
    "    print(batch)\n",
    "    text, length = batch.text\n",
    "    print(vocab.reverse_numericalize(text[0]))\n",
    "    print(length[0])\n",
    "    print(vocab.get_padding_index())\n",
    "\n",
    "\n",
    "def load_trec2(\n",
    "    meta,\n",
    "    tokenizer=None,\n",
    "    max_vocab_size=20_000,\n",
    "    max_seq_len=200,\n",
    "):\n",
    "\n",
    "    return load_dataset(\n",
    "        \"data/TREC-2\",\n",
    "        meta=meta,\n",
    "        tokenizer=tokenizer,\n",
    "        max_vocab_size=max_vocab_size,\n",
    "        max_seq_len=max_seq_len,\n",
    "    )\n",
    "\n",
    "\n",
    "def load_trec6(\n",
    "    meta,\n",
    "    tokenizer=None,\n",
    "    max_vocab_size=20_000,\n",
    "    max_seq_len=200,\n",
    "):\n",
    "\n",
    "    return load_dataset(\n",
    "        \"data/TREC-6\",\n",
    "        meta=meta,\n",
    "        tokenizer=tokenizer,\n",
    "        max_vocab_size=max_vocab_size,\n",
    "        max_seq_len=max_seq_len,\n",
    "    )\n",
    "\n",
    "\n",
    "def load_cola(\n",
    "    meta,\n",
    "    tokenizer=None,\n",
    "    max_vocab_size=20_000,\n",
    "    max_seq_len=200,\n",
    "):\n",
    "\n",
    "    return load_dataset(\n",
    "        \"data/GLUE/COLA\",\n",
    "        meta=meta,\n",
    "        tokenizer=tokenizer,\n",
    "        max_vocab_size=max_vocab_size,\n",
    "        max_seq_len=max_seq_len,\n",
    "    )\n",
    "\n",
    "\n",
    "def load_polarity(\n",
    "    meta,\n",
    "    tokenizer=None,\n",
    "    max_vocab_size=20_000,\n",
    "    max_seq_len=200,\n",
    "):\n",
    "\n",
    "    return load_dataset(\n",
    "        \"data/POL\",\n",
    "        meta=meta,\n",
    "        tokenizer=tokenizer,\n",
    "        max_vocab_size=max_vocab_size,\n",
    "        max_seq_len=max_seq_len,\n",
    "    )\n",
    "\n",
    "\n",
    "def load_subj(\n",
    "    meta,\n",
    "    tokenizer=None,\n",
    "    max_vocab_size=20_000,\n",
    "    max_seq_len=200,\n",
    "):\n",
    "\n",
    "    return load_dataset(\n",
    "        \"data/SUBJ\",\n",
    "        meta=meta,\n",
    "        tokenizer=tokenizer,\n",
    "        max_vocab_size=max_vocab_size,\n",
    "        max_seq_len=max_seq_len,\n",
    "    )\n",
    "\n",
    "\n",
    "def load_trec_hf(label=\"label-coarse\", max_vocab_size=20_000, max_seq_len=200):\n",
    "    vocab = Vocab(max_size=max_vocab_size)\n",
    "    fields = [\n",
    "        Field(\n",
    "            \"text\",\n",
    "            numericalizer=vocab,\n",
    "            include_lengths=True,\n",
    "            posttokenize_hooks=[MaxLenHook(max_seq_len)],\n",
    "            keep_raw=True,\n",
    "        ),\n",
    "        LabelField(\"label\"),\n",
    "    ]\n",
    "    hf_dataset = load_dataset(\"trec\")\n",
    "    hf_dataset = hf_dataset.rename_column(label, \"label\")\n",
    "    print(hf_dataset)\n",
    "    hf_train_val, hf_test = (\n",
    "        hf_dataset[\"train\"],\n",
    "        hf_dataset[\"test\"],\n",
    "    )\n",
    "    train_val_conv = HFDatasetConverter(hf_train_val, fields=fields)\n",
    "    test_conv = HFDatasetConverter(hf_test, fields=fields)\n",
    "    train_val, test = (\n",
    "        train_val_conv.as_dataset(),\n",
    "        test_conv.as_dataset(),\n",
    "    )\n",
    "    train, val = train_val.split(split_ratio=0.8, random_state=0)\n",
    "    train.finalize_fields()\n",
    "    print(train)\n",
    "    return (train, val, test), vocab\n",
    "\n",
    "\n",
    "def add_ids_to_files(root_folder):\n",
    "    split_ins = [\"train_old.csv\", \"dev_old.csv\", \"test_old.csv\"]\n",
    "    split_outs = [\"train.csv\", \"dev.csv\", \"test.csv\"]\n",
    "\n",
    "    for split_in, split_out in zip(split_ins, split_outs):\n",
    "        with open(os.path.join(root_folder, split_in), \"r\") as infile:\n",
    "            with open(os.path.join(root_folder, split_out), \"w\") as outfile:\n",
    "                for idx, line in enumerate(infile):\n",
    "                    parts = line.strip().split(\",\")\n",
    "                    if idx == 0:\n",
    "                        continue\n",
    "                    outfile.write(f\"{idx-1},{parts[0]},{parts[1]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58f7155d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "from args import *\n",
    "\n",
    "args = Config()\n",
    "args.lr = 2e-5\n",
    "args.l2 = 0\n",
    "args.model = \"BERT\"\n",
    "args.data = \"COLA\"\n",
    "args.adapter = \"unipelt\"\n",
    "args.batch_size = 32\n",
    "args.epochs = 10\n",
    "args.clip = 1\n",
    "\n",
    "meta = Config()\n",
    "\n",
    "dataloader = dataset_loaders[args.data]\n",
    "tokenizer = AutoTokenizer.from_pretrained(TRANSFORMERS[args.model])\n",
    "(train, val, test), vocab = dataloader(meta=meta, tokenizer=tokenizer)\n",
    "\n",
    "if args.data in pair_sequence_datasets:\n",
    "    meta.pair_sequence = True\n",
    "else:\n",
    "    meta.pair_sequence = False\n",
    "\n",
    "if meta.num_labels == 2:\n",
    "    # Binary classification\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    meta.num_targets = 1\n",
    "else:\n",
    "    # Multiclass classification\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    meta.num_targets = meta.num_labels\n",
    "    \n",
    "model = Transformer(args, meta, args.model, adapter=args.adapter)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def make_iterable(dataset, device, batch_size=32, train=False, indices=None):\n",
    "    \"\"\"\n",
    "    Construct a DataLoader from a podium Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def instance_length(instance):\n",
    "        raw, tokenized = instance.text\n",
    "        return -len(tokenized)\n",
    "\n",
    "    def cast_to_device(data):\n",
    "        return torch.tensor(np.array(data), device=device)\n",
    "\n",
    "    # Selects examples at given indices to support subset iteration.\n",
    "    if indices is not None:\n",
    "        dataset = dataset[indices]\n",
    "\n",
    "    iterator = BucketIterator(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        sort_key=instance_length,\n",
    "        shuffle=train,\n",
    "        matrix_class=cast_to_device,\n",
    "        look_ahead_multiplier=20,\n",
    "    )\n",
    "\n",
    "    return iterator\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "train_iter = make_iterable(\n",
    "    train,\n",
    "    device,\n",
    "    batch_size=args.batch_size,\n",
    "    train=True,\n",
    "#     indices=indices,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b948087d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (classifier): BertAdapterModel(\n",
       "    (shared_parameters): ModuleDict()\n",
       "    (bert): BertModel(\n",
       "      (shared_parameters): ModuleDict()\n",
       "      (invertible_adapters): ModuleDict()\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict(\n",
       "                    (COLA): LoRA(\n",
       "                      (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (key): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (value): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict(\n",
       "                    (COLA): LoRA(\n",
       "                      (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (prefix_tuning): PrefixTuningShim(\n",
       "                  (prefix_gates): ModuleDict(\n",
       "                    (COLA): Linear(in_features=768, out_features=1, bias=True)\n",
       "                  )\n",
       "                  (pool): PrefixTuningPool(\n",
       "                    (prefix_tunings): ModuleDict(\n",
       "                      (COLA): PrefixTuningGroup(\n",
       "                        (self_prefix): PrefixTuning(\n",
       "                          (wte): Embedding(10, 768)\n",
       "                          (control_trans): Sequential(\n",
       "                            (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "                            (1): Activation_Function_Class(\n",
       "                              (f): Tanh()\n",
       "                            )\n",
       "                            (2): Linear(in_features=512, out_features=18432, bias=True)\n",
       "                          )\n",
       "                          (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        )\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (adapters): ModuleDict()\n",
       "                (adapter_fusion_layer): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(\n",
       "                in_features=768, out_features=3072, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(\n",
       "                in_features=3072, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict(\n",
       "                (COLA): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (1): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict(\n",
       "                    (COLA): LoRA(\n",
       "                      (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (key): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (value): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict(\n",
       "                    (COLA): LoRA(\n",
       "                      (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (prefix_tuning): PrefixTuningShim(\n",
       "                  (prefix_gates): ModuleDict(\n",
       "                    (COLA): Linear(in_features=768, out_features=1, bias=True)\n",
       "                  )\n",
       "                  (pool): PrefixTuningPool(\n",
       "                    (prefix_tunings): ModuleDict(\n",
       "                      (COLA): PrefixTuningGroup(\n",
       "                        (self_prefix): PrefixTuning(\n",
       "                          (wte): Embedding(10, 768)\n",
       "                          (control_trans): Sequential(\n",
       "                            (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "                            (1): Activation_Function_Class(\n",
       "                              (f): Tanh()\n",
       "                            )\n",
       "                            (2): Linear(in_features=512, out_features=18432, bias=True)\n",
       "                          )\n",
       "                          (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        )\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (adapters): ModuleDict()\n",
       "                (adapter_fusion_layer): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(\n",
       "                in_features=768, out_features=3072, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(\n",
       "                in_features=3072, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict(\n",
       "                (COLA): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (2): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict(\n",
       "                    (COLA): LoRA(\n",
       "                      (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (key): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (value): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict(\n",
       "                    (COLA): LoRA(\n",
       "                      (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (prefix_tuning): PrefixTuningShim(\n",
       "                  (prefix_gates): ModuleDict(\n",
       "                    (COLA): Linear(in_features=768, out_features=1, bias=True)\n",
       "                  )\n",
       "                  (pool): PrefixTuningPool(\n",
       "                    (prefix_tunings): ModuleDict(\n",
       "                      (COLA): PrefixTuningGroup(\n",
       "                        (self_prefix): PrefixTuning(\n",
       "                          (wte): Embedding(10, 768)\n",
       "                          (control_trans): Sequential(\n",
       "                            (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "                            (1): Activation_Function_Class(\n",
       "                              (f): Tanh()\n",
       "                            )\n",
       "                            (2): Linear(in_features=512, out_features=18432, bias=True)\n",
       "                          )\n",
       "                          (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        )\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (adapters): ModuleDict()\n",
       "                (adapter_fusion_layer): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(\n",
       "                in_features=768, out_features=3072, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(\n",
       "                in_features=3072, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict(\n",
       "                (COLA): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (3): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict(\n",
       "                    (COLA): LoRA(\n",
       "                      (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (key): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (value): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict(\n",
       "                    (COLA): LoRA(\n",
       "                      (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (prefix_tuning): PrefixTuningShim(\n",
       "                  (prefix_gates): ModuleDict(\n",
       "                    (COLA): Linear(in_features=768, out_features=1, bias=True)\n",
       "                  )\n",
       "                  (pool): PrefixTuningPool(\n",
       "                    (prefix_tunings): ModuleDict(\n",
       "                      (COLA): PrefixTuningGroup(\n",
       "                        (self_prefix): PrefixTuning(\n",
       "                          (wte): Embedding(10, 768)\n",
       "                          (control_trans): Sequential(\n",
       "                            (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "                            (1): Activation_Function_Class(\n",
       "                              (f): Tanh()\n",
       "                            )\n",
       "                            (2): Linear(in_features=512, out_features=18432, bias=True)\n",
       "                          )\n",
       "                          (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        )\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (adapters): ModuleDict()\n",
       "                (adapter_fusion_layer): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(\n",
       "                in_features=768, out_features=3072, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(\n",
       "                in_features=3072, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict(\n",
       "                (COLA): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (4): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict(\n",
       "                    (COLA): LoRA(\n",
       "                      (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (key): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (value): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict(\n",
       "                    (COLA): LoRA(\n",
       "                      (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (prefix_tuning): PrefixTuningShim(\n",
       "                  (prefix_gates): ModuleDict(\n",
       "                    (COLA): Linear(in_features=768, out_features=1, bias=True)\n",
       "                  )\n",
       "                  (pool): PrefixTuningPool(\n",
       "                    (prefix_tunings): ModuleDict(\n",
       "                      (COLA): PrefixTuningGroup(\n",
       "                        (self_prefix): PrefixTuning(\n",
       "                          (wte): Embedding(10, 768)\n",
       "                          (control_trans): Sequential(\n",
       "                            (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "                            (1): Activation_Function_Class(\n",
       "                              (f): Tanh()\n",
       "                            )\n",
       "                            (2): Linear(in_features=512, out_features=18432, bias=True)\n",
       "                          )\n",
       "                          (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        )\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (adapters): ModuleDict()\n",
       "                (adapter_fusion_layer): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(\n",
       "                in_features=768, out_features=3072, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(\n",
       "                in_features=3072, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict(\n",
       "                (COLA): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (5): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict(\n",
       "                    (COLA): LoRA(\n",
       "                      (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (key): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (value): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict(\n",
       "                    (COLA): LoRA(\n",
       "                      (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (prefix_tuning): PrefixTuningShim(\n",
       "                  (prefix_gates): ModuleDict(\n",
       "                    (COLA): Linear(in_features=768, out_features=1, bias=True)\n",
       "                  )\n",
       "                  (pool): PrefixTuningPool(\n",
       "                    (prefix_tunings): ModuleDict(\n",
       "                      (COLA): PrefixTuningGroup(\n",
       "                        (self_prefix): PrefixTuning(\n",
       "                          (wte): Embedding(10, 768)\n",
       "                          (control_trans): Sequential(\n",
       "                            (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "                            (1): Activation_Function_Class(\n",
       "                              (f): Tanh()\n",
       "                            )\n",
       "                            (2): Linear(in_features=512, out_features=18432, bias=True)\n",
       "                          )\n",
       "                          (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        )\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (adapters): ModuleDict()\n",
       "                (adapter_fusion_layer): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(\n",
       "                in_features=768, out_features=3072, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(\n",
       "                in_features=3072, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict(\n",
       "                (COLA): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (6): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict(\n",
       "                    (COLA): LoRA(\n",
       "                      (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (key): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (value): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict(\n",
       "                    (COLA): LoRA(\n",
       "                      (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (prefix_tuning): PrefixTuningShim(\n",
       "                  (prefix_gates): ModuleDict(\n",
       "                    (COLA): Linear(in_features=768, out_features=1, bias=True)\n",
       "                  )\n",
       "                  (pool): PrefixTuningPool(\n",
       "                    (prefix_tunings): ModuleDict(\n",
       "                      (COLA): PrefixTuningGroup(\n",
       "                        (self_prefix): PrefixTuning(\n",
       "                          (wte): Embedding(10, 768)\n",
       "                          (control_trans): Sequential(\n",
       "                            (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "                            (1): Activation_Function_Class(\n",
       "                              (f): Tanh()\n",
       "                            )\n",
       "                            (2): Linear(in_features=512, out_features=18432, bias=True)\n",
       "                          )\n",
       "                          (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        )\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (adapters): ModuleDict()\n",
       "                (adapter_fusion_layer): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(\n",
       "                in_features=768, out_features=3072, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(\n",
       "                in_features=3072, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict(\n",
       "                (COLA): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (7): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict(\n",
       "                    (COLA): LoRA(\n",
       "                      (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (key): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (value): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict(\n",
       "                    (COLA): LoRA(\n",
       "                      (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (prefix_tuning): PrefixTuningShim(\n",
       "                  (prefix_gates): ModuleDict(\n",
       "                    (COLA): Linear(in_features=768, out_features=1, bias=True)\n",
       "                  )\n",
       "                  (pool): PrefixTuningPool(\n",
       "                    (prefix_tunings): ModuleDict(\n",
       "                      (COLA): PrefixTuningGroup(\n",
       "                        (self_prefix): PrefixTuning(\n",
       "                          (wte): Embedding(10, 768)\n",
       "                          (control_trans): Sequential(\n",
       "                            (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "                            (1): Activation_Function_Class(\n",
       "                              (f): Tanh()\n",
       "                            )\n",
       "                            (2): Linear(in_features=512, out_features=18432, bias=True)\n",
       "                          )\n",
       "                          (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        )\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (adapters): ModuleDict()\n",
       "                (adapter_fusion_layer): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(\n",
       "                in_features=768, out_features=3072, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(\n",
       "                in_features=3072, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict(\n",
       "                (COLA): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (8): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict(\n",
       "                    (COLA): LoRA(\n",
       "                      (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (key): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (value): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict(\n",
       "                    (COLA): LoRA(\n",
       "                      (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (prefix_tuning): PrefixTuningShim(\n",
       "                  (prefix_gates): ModuleDict(\n",
       "                    (COLA): Linear(in_features=768, out_features=1, bias=True)\n",
       "                  )\n",
       "                  (pool): PrefixTuningPool(\n",
       "                    (prefix_tunings): ModuleDict(\n",
       "                      (COLA): PrefixTuningGroup(\n",
       "                        (self_prefix): PrefixTuning(\n",
       "                          (wte): Embedding(10, 768)\n",
       "                          (control_trans): Sequential(\n",
       "                            (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "                            (1): Activation_Function_Class(\n",
       "                              (f): Tanh()\n",
       "                            )\n",
       "                            (2): Linear(in_features=512, out_features=18432, bias=True)\n",
       "                          )\n",
       "                          (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        )\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (adapters): ModuleDict()\n",
       "                (adapter_fusion_layer): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(\n",
       "                in_features=768, out_features=3072, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(\n",
       "                in_features=3072, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict(\n",
       "                (COLA): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (9): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict(\n",
       "                    (COLA): LoRA(\n",
       "                      (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (key): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (value): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict(\n",
       "                    (COLA): LoRA(\n",
       "                      (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (prefix_tuning): PrefixTuningShim(\n",
       "                  (prefix_gates): ModuleDict(\n",
       "                    (COLA): Linear(in_features=768, out_features=1, bias=True)\n",
       "                  )\n",
       "                  (pool): PrefixTuningPool(\n",
       "                    (prefix_tunings): ModuleDict(\n",
       "                      (COLA): PrefixTuningGroup(\n",
       "                        (self_prefix): PrefixTuning(\n",
       "                          (wte): Embedding(10, 768)\n",
       "                          (control_trans): Sequential(\n",
       "                            (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "                            (1): Activation_Function_Class(\n",
       "                              (f): Tanh()\n",
       "                            )\n",
       "                            (2): Linear(in_features=512, out_features=18432, bias=True)\n",
       "                          )\n",
       "                          (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        )\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (adapters): ModuleDict()\n",
       "                (adapter_fusion_layer): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(\n",
       "                in_features=768, out_features=3072, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(\n",
       "                in_features=3072, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict(\n",
       "                (COLA): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (10): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict(\n",
       "                    (COLA): LoRA(\n",
       "                      (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (key): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (value): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict(\n",
       "                    (COLA): LoRA(\n",
       "                      (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (prefix_tuning): PrefixTuningShim(\n",
       "                  (prefix_gates): ModuleDict(\n",
       "                    (COLA): Linear(in_features=768, out_features=1, bias=True)\n",
       "                  )\n",
       "                  (pool): PrefixTuningPool(\n",
       "                    (prefix_tunings): ModuleDict(\n",
       "                      (COLA): PrefixTuningGroup(\n",
       "                        (self_prefix): PrefixTuning(\n",
       "                          (wte): Embedding(10, 768)\n",
       "                          (control_trans): Sequential(\n",
       "                            (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "                            (1): Activation_Function_Class(\n",
       "                              (f): Tanh()\n",
       "                            )\n",
       "                            (2): Linear(in_features=512, out_features=18432, bias=True)\n",
       "                          )\n",
       "                          (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        )\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (adapters): ModuleDict()\n",
       "                (adapter_fusion_layer): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(\n",
       "                in_features=768, out_features=3072, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(\n",
       "                in_features=3072, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict(\n",
       "                (COLA): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (11): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict(\n",
       "                    (COLA): LoRA(\n",
       "                      (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (key): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (value): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (loras): ModuleDict(\n",
       "                    (COLA): LoRA(\n",
       "                      (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (prefix_tuning): PrefixTuningShim(\n",
       "                  (prefix_gates): ModuleDict(\n",
       "                    (COLA): Linear(in_features=768, out_features=1, bias=True)\n",
       "                  )\n",
       "                  (pool): PrefixTuningPool(\n",
       "                    (prefix_tunings): ModuleDict(\n",
       "                      (COLA): PrefixTuningGroup(\n",
       "                        (self_prefix): PrefixTuning(\n",
       "                          (wte): Embedding(10, 768)\n",
       "                          (control_trans): Sequential(\n",
       "                            (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "                            (1): Activation_Function_Class(\n",
       "                              (f): Tanh()\n",
       "                            )\n",
       "                            (2): Linear(in_features=512, out_features=18432, bias=True)\n",
       "                          )\n",
       "                          (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        )\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (adapters): ModuleDict()\n",
       "                (adapter_fusion_layer): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(\n",
       "                in_features=768, out_features=3072, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(\n",
       "                in_features=3072, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict(\n",
       "                (COLA): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (gate): Linear(in_features=768, out_features=1, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "      (prefix_tuning): PrefixTuningPool(\n",
       "        (prefix_tunings): ModuleDict(\n",
       "          (COLA): PrefixTuningGroup(\n",
       "            (self_prefix): PrefixTuning(\n",
       "              (wte): Embedding(10, 768)\n",
       "              (control_trans): Sequential(\n",
       "                (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "                (1): Activation_Function_Class(\n",
       "                  (f): Tanh()\n",
       "                )\n",
       "                (2): Linear(in_features=512, out_features=18432, bias=True)\n",
       "              )\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (heads): ModuleDict(\n",
       "      (COLA): ClassificationHead(\n",
       "        (0): Dropout(p=0.1, inplace=False)\n",
       "        (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (2): Activation_Function_Class(\n",
       "          (f): Tanh()\n",
       "        )\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "        (4): Linear(in_features=768, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a11544a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Padding symbol is not in the vocabulary.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1664478/1352271905.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/adapter-al/dataloaders.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0mbatch_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbucket\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gen/lib/python3.9/site-packages/podium/datasets/iterator.py\u001b[0m in \u001b[0;36m_create_batch\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    366\u001b[0m                 \u001b[0;32mand\u001b[0m \u001b[0mpossible_cast_to_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m             ):\n\u001b[0;32m--> 368\u001b[0;31m                 batch = Iterator._arrays_to_matrix(\n\u001b[0m\u001b[1;32m    369\u001b[0m                     \u001b[0mfield\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumericalizations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix_class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                 )\n",
      "\u001b[0;32m~/.conda/envs/gen/lib/python3.9/site-packages/podium/datasets/iterator.py\u001b[0m in \u001b[0;36m_arrays_to_matrix\u001b[0;34m(field, arrays, matrix_class)\u001b[0m\n\u001b[1;32m    438\u001b[0m     ) -> np.ndarray:\n\u001b[1;32m    439\u001b[0m         \u001b[0mpad_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_pad_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0mpadded_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pad_to_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_length\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmatrix_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_arrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gen/lib/python3.9/site-packages/podium/datasets/iterator.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    438\u001b[0m     ) -> np.ndarray:\n\u001b[1;32m    439\u001b[0m         \u001b[0mpad_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_pad_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0mpadded_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pad_to_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_length\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmatrix_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_arrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gen/lib/python3.9/site-packages/podium/field.py\u001b[0m in \u001b[0;36m_pad_to_length\u001b[0;34m(self, array, length, custom_pad_symbol, pad_left, truncate_left)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_vocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m                 \u001b[0mpad_symbol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_padding_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gen/lib/python3.9/site-packages/podium/vocab.py\u001b[0m in \u001b[0;36mget_padding_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \"\"\"\n\u001b[1;32m    342\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mVocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pad\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Padding symbol is not in the vocabulary.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mVocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pad\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Padding symbol is not in the vocabulary."
     ]
    }
   ],
   "source": [
    "for i in train_iter:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea71c157",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.fields[1].vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd1a6909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from al.experiment import Experiment\n",
    "\n",
    "\n",
    "def train_model(model, optimizer, criterion, train_iter):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    accuracy, confusion_matrix = 0, np.zeros(\n",
    "        (meta.num_labels, meta.num_labels), dtype=int\n",
    "    )\n",
    "\n",
    "    logit_list = []\n",
    "    y_true_list = []\n",
    "    ids = []\n",
    "    for batch_num, batch in enumerate(train_iter, 1):\n",
    "        t = time.time()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ids.extend([int(id[0]) for id in batch.id])\n",
    "\n",
    "        # Unpack batch & cast to device\n",
    "        if meta.pair_sequence:\n",
    "            (x_sequence1, sequence1_lengths) = batch.sequence1\n",
    "            (x_sequence2, sequence2_lengths) = batch.sequence2\n",
    "        else:\n",
    "            (x, lengths) = batch.text\n",
    "\n",
    "        y = batch.label\n",
    "        y_true_list.append(y.squeeze(0) if y.numel() == 1 else y.squeeze())\n",
    "\n",
    "        if meta.pair_sequence:\n",
    "            # PSQ\n",
    "            lengths = (sequence1_lengths, sequence2_lengths)\n",
    "            logits, return_dict = model(x_sequence1, x_sequence2, lengths)\n",
    "        else:\n",
    "            # SSQ\n",
    "            logits, return_dict = model(x, lengths)\n",
    "        logit_list.append(logits)\n",
    "\n",
    "        # Bookkeeping and cast label to float\n",
    "        accuracy, confusion_matrix = Experiment.update_stats(\n",
    "            accuracy, confusion_matrix, logits, y\n",
    "        )\n",
    "        if logits.shape[-1] == 1:\n",
    "            # binary cross entropy, cast labels to float\n",
    "            y = y.type(torch.float)\n",
    "\n",
    "        loss = criterion(logits.view(-1, meta.num_targets).squeeze(), y.squeeze())\n",
    "\n",
    "        total_loss += float(loss)\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        print(\n",
    "            \"[Batch]: {}/{} in {:.5f} seconds\".format(\n",
    "                batch_num, len(train_iter), time.time() - t\n",
    "            ),\n",
    "            end=\"\\r\",\n",
    "            flush=True,\n",
    "        )\n",
    "\n",
    "    loss = total_loss / len(train_iter)\n",
    "    result_dict = {\"loss\": loss}\n",
    "    logit_tensor = torch.cat(logit_list)\n",
    "    y_true = torch.cat(y_true_list)\n",
    "    return result_dict, logit_tensor, y_true, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "022001ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Padding symbol is not in the vocabulary.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1441522/3166091545.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training epoch: {epoch}/{args.epochs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# a) Train for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     result_dict_train, logits, y_true, ids = train_model(\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     )\n",
      "\u001b[0;32m/tmp/ipykernel_1441522/1683375134.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, criterion, train_iter)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0my_true_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/adapter-al/dataloaders.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0mbatch_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbucket\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gen/lib/python3.9/site-packages/podium/datasets/iterator.py\u001b[0m in \u001b[0;36m_create_batch\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    366\u001b[0m                 \u001b[0;32mand\u001b[0m \u001b[0mpossible_cast_to_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m             ):\n\u001b[0;32m--> 368\u001b[0;31m                 batch = Iterator._arrays_to_matrix(\n\u001b[0m\u001b[1;32m    369\u001b[0m                     \u001b[0mfield\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumericalizations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix_class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                 )\n",
      "\u001b[0;32m~/.conda/envs/gen/lib/python3.9/site-packages/podium/datasets/iterator.py\u001b[0m in \u001b[0;36m_arrays_to_matrix\u001b[0;34m(field, arrays, matrix_class)\u001b[0m\n\u001b[1;32m    438\u001b[0m     ) -> np.ndarray:\n\u001b[1;32m    439\u001b[0m         \u001b[0mpad_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_pad_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0mpadded_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pad_to_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_length\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmatrix_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_arrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gen/lib/python3.9/site-packages/podium/datasets/iterator.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    438\u001b[0m     ) -> np.ndarray:\n\u001b[1;32m    439\u001b[0m         \u001b[0mpad_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_pad_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0mpadded_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pad_to_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_length\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmatrix_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_arrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gen/lib/python3.9/site-packages/podium/field.py\u001b[0m in \u001b[0;36m_pad_to_length\u001b[0;34m(self, array, length, custom_pad_symbol, pad_left, truncate_left)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_vocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m                 \u001b[0mpad_symbol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_padding_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gen/lib/python3.9/site-packages/podium/vocab.py\u001b[0m in \u001b[0;36mget_padding_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \"\"\"\n\u001b[1;32m    342\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mVocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pad\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Padding symbol is not in the vocabulary.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mVocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pad\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Padding symbol is not in the vocabulary."
     ]
    }
   ],
   "source": [
    "train_results = []\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    print(f\"Training epoch: {epoch}/{args.epochs}\")\n",
    "    # a) Train for one epoch\n",
    "    result_dict_train, logits, y_true, ids = train_model(\n",
    "        model, optimizer, criterion, train_iter\n",
    "    )\n",
    "    print(result_dict_train)\n",
    "    train_results.append(result_dict_train)\n",
    "\n",
    "    # b) Evaluate model (test set)\n",
    "#     eval_result_dict = self._evaluate_model(model)\n",
    "#     acc.append(eval_result_dict[\"accuracy\"])\n",
    "#     loss.append(result_dict_train[\"loss\"])\n",
    "#     eval_results.append(eval_result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "57b1c29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "name = model.get_classifier_name()\n",
    "clf = getattr(model.classifier, name)\n",
    "config = model.classifier.config\n",
    "num_layers = config.num_hidden_layers\n",
    "\n",
    "print(num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "27410ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch({\n",
      "    id: [['0'], ['1'], ['2'], ['3'], ['4'], ['5'], ['6'], ['7'], ['8'], ['9'], ['10'], ['11'], ['12'], ['13'], ['14'], ['15'], ['16'], ['17'], ['18'], ['19'], ['20'], ['21'], ['22'], ['23'], ['24'], ['25'], ['26'], ['27'], ['28'], ['29'], ['30'], ['31']],\n",
      "    sequence1: (tensor([[2054, 2055, 1996,  ...,    0,    0,    0],\n",
      "            [2515, 5423, 2669,  ...,    0,    0,    0],\n",
      "            [2054, 2003, 2115,  ...,    0,    0,    0],\n",
      "            ...,\n",
      "            [2054, 2024, 1996,  ...,    0,    0,    0],\n",
      "            [2003, 2009, 1037,  ...,    0,    0,    0],\n",
      "            [2054, 2097, 2022,  ...,    0,    0,    0]], device='cuda:0'), tensor([ 7,  9,  6,  9, 22, 10, 23, 11, 13,  6,  9,  8, 14, 11,  7, 12,  7,  7,\n",
      "             5, 11, 17, 16, 12,  7, 38, 14, 11,  6,  9, 16, 23, 14],\n",
      "           device='cuda:0')),\n",
      "    sequence2: (tensor([[ 2054,  1055,  1996,  2087,  2709,  2017,  2310,  2081,  1999,  1037,\n",
      "              4518,  7047, 17795,  5211,  3119,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2515,  2665,  5572,  2428, 13416,  3635,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2054,  2024,  1996,  2070,  1997,  1996,  2204,  5365,  2774,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2054,  2003,  1996,  2190,  2126,  2000,  4553,  2151,  2047,  3097,\n",
      "              2653,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2129,  2172,  2515,  2019, 19330,  2050,  7163,  9298,  7796,  2006,\n",
      "              2779,  2566,  3204,  2013,  2019, 14316,  1055,  2391,  1997,  3193,\n",
      "              1999,  9212,  4305, 13484,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2073,  2064,  1045,  4553,  2397,  2595,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2323,  1045,  2175,  2000,  1037,  2047,  2231,  2966,  2267,  3225,\n",
      "              2034,  5219,  2013,  2023,  2095,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2129,  2064,  1045,  4468,  2893, 14255, 23344,  2015,  3561,  2007,\n",
      "              2668,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2429,  2000,  7025,  2054,  6433,  2000,  2111,  1999,  1996, 25115,\n",
      "              2040,  2973,  2077,  7187,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2079,  2308,  2428,  2424, 23905,  2158,  8702,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2129,  2323,  1045, 15697,  2871,  2243,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2129,  2323,  2017,  2707,  4083,  4730,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2054,  2003,  1996,  5197,  1997,  1996,  7328,  2422, 24635,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2064,  2070,  2028,  2507,  2033, 10247,  2006,  4083,  3682,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2323,  1045,  6366,  2659,  3737,  4180,  2013,  2026,  2609,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2054,  2003,  1996,  5675,  2005,  1996, 16913, 11627,  1997, 21274,\n",
      "              3012,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2054,  2079,  7325, 22752,  2015,  2079,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2054,  2024,  1996,  2190,  4684,  4773, 10439,  7705,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2003,  1996,  3103,  5255,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2003,  1037,  2117,  3147,  2162,  2007,  3607,  1037,  2613,  6061,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 5702,  1999, 16595,     0,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2339,  2097,  1996,  3818, 13926, 15687,  3578,  2022,  2028,  3329,\n",
      "              7820,  2084,  2028,  1059, 13535,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2054,  1055,  1037,  2204,  2299,  1045,  2064,  2224,  2000, 13677,\n",
      "             26418,  2026,  3124,  2767,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2129,  2515, 19902,  7461,  2256,  2166,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2339,  2079,  9430, 14555,  2012,  2312,  2695, 12997,  2080,  6627,\n",
      "              3316,  2421, 10439,  2890,  7405,  3468,  9430,  1999,  4518,  2738,\n",
      "              2084,  2074,  2164,  2035,  1996,  9430,  1999,  1996,  2918, 10300],\n",
      "            [ 2129,  2064,  1045, 10172,  2122,  2015,  2006,  7388,  2470,  2006,\n",
      "              2248,  3075,  2005,  9459,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2129,  2079,  1045,  7374,  2005,  1996, 21307,  4523,  8325,  2961,\n",
      "              1045,  1056,  2961, 11360,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2129,  2515,  4702,  6444,  6541,  2147,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2054,  2003,  1996,  2364,  2801,  1997,  2023,  2338,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2054,  2024,  1996,  3808, 29361,  2006,  8304, 13305,  2015,  3818,\n",
      "              2011,  1996, 17212,  2050,  1999,  5334,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2339,  2003,  2045,  2145,  1037, 12078,  2306,  1996,  2142,  2983,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "            [ 2054,  2097,  2022,  1996, 18520,  7207,  1055,  2634,  3343,  2065,\n",
      "              2016,  2468,  1996,  2343,  1997,  3915,     0,     0,     0,     0,\n",
      "                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0]],\n",
      "           device='cuda:0'), tensor([15,  6,  9, 11, 24,  6, 15, 11, 14,  7,  6,  6,  9,  9,  9, 11,  6,  8,\n",
      "             4, 10,  3, 15, 14,  6, 30, 14, 14,  6,  8, 16, 10, 16],\n",
      "           device='cuda:0')),\n",
      "    label: tensor([[0],\n",
      "            [1],\n",
      "            [1],\n",
      "            [1],\n",
      "            [0],\n",
      "            [1],\n",
      "            [0],\n",
      "            [0],\n",
      "            [0],\n",
      "            [0],\n",
      "            [1],\n",
      "            [0],\n",
      "            [1],\n",
      "            [1],\n",
      "            [0],\n",
      "            [0],\n",
      "            [0],\n",
      "            [1],\n",
      "            [0],\n",
      "            [1],\n",
      "            [0],\n",
      "            [0],\n",
      "            [0],\n",
      "            [1],\n",
      "            [0],\n",
      "            [0],\n",
      "            [1],\n",
      "            [1],\n",
      "            [0],\n",
      "            [1],\n",
      "            [0],\n",
      "            [1]], device='cuda:0')\n",
      "})\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Batch' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1437920/3226874358.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gen/lib/python3.9/site-packages/podium/datasets/iterator.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Batch' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "enc = []\n",
    "grads = []\n",
    "labels = []\n",
    "enc_layers = {i: [] for i in range(num_layers)}\n",
    "\n",
    "\n",
    "train_iter = make_iterable(\n",
    "    train,\n",
    "    device,\n",
    "    batch_size=args.batch_size,\n",
    "    train=False,\n",
    "#     indices=indices,\n",
    ")\n",
    "\n",
    "for batch in train_iter:\n",
    "\n",
    "    print(batch)\n",
    "    inputs, _ = batch.text\n",
    "    labels.append(batch.label)\n",
    "    inputs.requires_grad = False\n",
    "\n",
    "    embedded_tokens = clf.embeddings(inputs)\n",
    "    embedded_tokens = torch.autograd.Variable(\n",
    "        embedded_tokens, requires_grad=True\n",
    "    )\n",
    "    encoded_all = clf.encoder(\n",
    "        embedded_tokens,\n",
    "        output_hidden_states=True,\n",
    "        # head_mask=head_mask,\n",
    "        # attention_mask=attention_mask,\n",
    "    )\n",
    "    # Skip the embedding layer [1:]\n",
    "    for i, enc_layer in enumerate(encoded_all[1][1:]):\n",
    "        enc_layers[i].append(enc_layer[:, 0].cpu())\n",
    "\n",
    "    encoded = encoded_all[0][:, 0]\n",
    "    enc.append(encoded.cpu())\n",
    "\n",
    "    mean = encoded.mean()\n",
    "    mean.backward()\n",
    "    enc_grad = embedded_tokens.grad.data\n",
    "    grads.append(enc_grad.norm(p=2, dim=(1, 2)))\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "y = torch.cat(labels)\n",
    "for k, v in enc_layers.items():\n",
    "    X = torch.cat(v)\n",
    "    enc_layers[k] = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "1748e9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0\n",
      "Layer 1\n",
      "Layer 2\n",
      "Layer 3\n",
      "Layer 4\n",
      "Layer 5\n",
      "Layer 6\n",
      "Layer 7\n",
      "Layer 8\n",
      "Layer 9\n",
      "Layer 10\n",
      "Layer 11\n"
     ]
    }
   ],
   "source": [
    "from cka import *\n",
    "\n",
    "lin_vals = np.empty((num_layers, num_layers))\n",
    "rbf_vals = np.empty((num_layers, num_layers))\n",
    "for i in range(num_layers):\n",
    "    X = enc_layers[i].detach().numpy()\n",
    "    print(f\"Layer {i}\")\n",
    "    for j in range(num_layers-1, i-1, -1):\n",
    "        Y = enc_layers[j].detach().numpy()\n",
    "        lin = linear_CKA(X, Y)\n",
    "        rbf = kernel_CKA(X, Y)\n",
    "        lin_vals[i, j] = lin_vals[j, i] = lin\n",
    "        rbf_vals[i, j] = rbf_vals[j, i] = rbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a95f274a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(np.flip(lin_vals, 1), xticklabels=range(1, 12+1), yticklabels=range(12, 0, -1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9c3d2c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.98248242 0.96114412 0.95121319 0.90460212 0.71041576\n",
      "  0.47315452 0.38221755 0.31722107 0.27903575 0.25892686 0.24506719]\n",
      " [0.98248242 1.         0.99294096 0.98335407 0.95183908 0.75358897\n",
      "  0.50744447 0.41371993 0.34471121 0.30324662 0.28150918 0.26641857]\n",
      " [0.96114412 0.99294096 1.         0.99445655 0.96767089 0.76605118\n",
      "  0.51650029 0.42217762 0.3524165  0.31063183 0.28872567 0.27348558]\n",
      " [0.95121319 0.98335407 0.99445655 1.         0.97601547 0.78270474\n",
      "  0.53881083 0.44590866 0.37730722 0.33579883 0.31386207 0.29857912]\n",
      " [0.90460212 0.95183908 0.96767089 0.97601547 1.         0.86372819\n",
      "  0.64402416 0.55500195 0.48563069 0.44082381 0.41642609 0.39986809]\n",
      " [0.71041576 0.75358897 0.76605118 0.78270474 0.86372819 1.\n",
      "  0.91641715 0.85514805 0.80074403 0.7619774  0.73903904 0.72382307]\n",
      " [0.47315452 0.50744447 0.51650029 0.53881083 0.64402416 0.91641715\n",
      "  1.         0.98668085 0.96352048 0.94102505 0.92561751 0.91487866]\n",
      " [0.38221755 0.41371993 0.42217762 0.44590866 0.55500195 0.85514805\n",
      "  0.98668085 1.         0.99073546 0.97514698 0.96322383 0.95479626]\n",
      " [0.31722107 0.34471121 0.3524165  0.37730722 0.48563069 0.80074403\n",
      "  0.96352048 0.99073546 1.         0.99368034 0.98627515 0.980397  ]\n",
      " [0.27903575 0.30324662 0.31063183 0.33579883 0.44082381 0.7619774\n",
      "  0.94102505 0.97514698 0.99368034 1.         0.99771465 0.99458776]\n",
      " [0.25892686 0.28150918 0.28872567 0.31386207 0.41642609 0.73903904\n",
      "  0.92561751 0.96322383 0.98627515 0.99771465 1.         0.99915245]\n",
      " [0.24506719 0.26641857 0.27348558 0.29857912 0.39986809 0.72382307\n",
      "  0.91487866 0.95479626 0.980397   0.99458776 0.99915245 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(lin_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e25b09b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.98822128 0.96864577 0.94506764 0.84834738 0.58058964\n",
      "  0.38203942 0.30834932 0.27336096 0.25623951 0.2498741  0.24393084]\n",
      " [0.98822128 1.         0.98886123 0.96045117 0.87905716 0.61475402\n",
      "  0.41201909 0.33673577 0.30072946 0.28261005 0.27595218 0.26954692]\n",
      " [0.96864577 0.98886123 1.         0.9828193  0.90749149 0.63409695\n",
      "  0.42791082 0.35237271 0.31687092 0.29899749 0.29216579 0.28537392]\n",
      " [0.94506764 0.96045117 0.9828193  1.         0.93184827 0.66351566\n",
      "  0.46338771 0.3904026  0.3567184  0.33894004 0.33143854 0.32377679]\n",
      " [0.84834738 0.87905716 0.90749149 0.93184827 1.         0.83521269\n",
      "  0.66546147 0.5968039  0.55941694 0.53621035 0.52402475 0.51286493]\n",
      " [0.58058964 0.61475402 0.63409695 0.66351566 0.83521269 1.\n",
      "  0.94082216 0.89526446 0.86034934 0.83374846 0.81842072 0.80571088]\n",
      " [0.38203942 0.41201909 0.42791082 0.46338771 0.66546147 0.94082216\n",
      "  1.         0.9877828  0.96883966 0.94919752 0.9360177  0.92503854]\n",
      " [0.30834932 0.33673577 0.35237271 0.3904026  0.5968039  0.89526446\n",
      "  0.9877828  1.         0.99094239 0.97567427 0.96464111 0.95507823]\n",
      " [0.27336096 0.30072946 0.31687092 0.3567184  0.55941694 0.86034934\n",
      "  0.96883966 0.99094239 1.         0.99297577 0.98468996 0.97586278]\n",
      " [0.25623951 0.28261005 0.29899749 0.33894004 0.53621035 0.83374846\n",
      "  0.94919752 0.97567427 0.99297577 1.         0.99728635 0.99119932]\n",
      " [0.2498741  0.27595218 0.29216579 0.33143854 0.52402475 0.81842072\n",
      "  0.9360177  0.96464111 0.98468996 0.99728635 1.         0.99753495]\n",
      " [0.24393084 0.26954692 0.28537392 0.32377679 0.51286493 0.80571088\n",
      "  0.92503854 0.95507823 0.97586278 0.99119932 0.99753495 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(lin_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b913a0e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Random\n",
    "sns.heatmap(np.flip(lin_vals, 1), xticklabels=range(1, 12+1), yticklabels=range(12, 0, -1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fda8257d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Core set\n",
    "sns.heatmap(np.flip(lin_vals, 1), xticklabels=range(1, 12+1), yticklabels=range(12, 0, -1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "617026dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Entropy\n",
    "sns.heatmap(np.flip(lin_vals, 1), xticklabels=range(1, 12+1), yticklabels=range(12, 0, -1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc285785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200.0, 1000.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "g = sns.lineplot(\n",
    "    data=df_tr[df_tr.index.get_level_values(\"mode\") == \"long-besov\"],\n",
    "    x=\"labeled\",\n",
    "    y=\"f1_micro\",\n",
    "    hue=\"sampler\",\n",
    "    style=\"sampler\",\n",
    "    markers=True,\n",
    "    dashes=False,\n",
    "    ci=95,\n",
    "    linewidth=3,\n",
    ")\n",
    "g.set_ylim(0.89, 0.98)\n",
    "g.set_xlim(200, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33d25a5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not interpret value `acuraccy` for parameter `y`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1434047/548833310.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m g = sns.lineplot(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_tr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mode\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"ada-besov\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"labeled\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"acuraccy\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mhue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sampler\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gen/lib/python3.9/site-packages/seaborn/_decorators.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m             )\n\u001b[1;32m     45\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gen/lib/python3.9/site-packages/seaborn/relational.py\u001b[0m in \u001b[0;36mlineplot\u001b[0;34m(x, y, hue, size, style, data, palette, hue_order, hue_norm, sizes, size_order, size_norm, dashes, markers, style_order, units, estimator, ci, n_boot, seed, sort, err_style, err_kws, legend, ax, **kwargs)\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m     \u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_LinePlotter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_semantics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m     p = _LinePlotter(\n\u001b[0m\u001b[1;32m    686\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mci\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mci\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_boot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_boot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gen/lib/python3.9/site-packages/seaborn/relational.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, variables, estimator, ci, n_boot, seed, sort, err_style, err_kws, legend)\u001b[0m\n\u001b[1;32m    365\u001b[0m         )\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gen/lib/python3.9/site-packages/seaborn/_core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m    602\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_semantic_mappings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gen/lib/python3.9/site-packages/seaborn/_core.py\u001b[0m in \u001b[0;36massign_variables\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m    665\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"long\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             plot_data, variables = self._assign_variables_longform(\n\u001b[0m\u001b[1;32m    668\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m             )\n",
      "\u001b[0;32m~/.conda/envs/gen/lib/python3.9/site-packages/seaborn/_core.py\u001b[0m in \u001b[0;36m_assign_variables_longform\u001b[0;34m(self, data, **kwargs)\u001b[0m\n\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m                 \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Could not interpret value `{val}` for parameter `{key}`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 902\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Could not interpret value `acuraccy` for parameter `y`"
     ]
    }
   ],
   "source": [
    "g = sns.lineplot(\n",
    "    data=df_tr[df_tr.index.get_level_values(\"mode\") == \"ada-besov\"],\n",
    "    x=\"labeled\",\n",
    "    y=\"f1_micro\",\n",
    "    hue=\"sampler\",\n",
    "    style=\"sampler\",\n",
    "    markers=True,\n",
    "    dashes=False,\n",
    "    ci=95,\n",
    "    linewidth=3,\n",
    ")\n",
    "# g.set_ylim(0.82, 0.92)\n",
    "# g.set_xlim(500, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "44e34a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoAdapterModel, AdapterConfig\n",
    "from datasets import load_dataset\n",
    "\n",
    "d = load_dataset(\"glue\", \"qqp\")\n",
    "\n",
    "\n",
    "def save_dataset(hfd, name):\n",
    "    hfd[\"train\"].to_pandas()[[\"question1\", \"question2\", \"label\"]].sample(\n",
    "        5_000\n",
    "    ).reset_index(drop=True).to_csv(f\"data/{name}/train.csv\", header=False)\n",
    "    hfd[\"test\"].to_pandas()[[\"question1\", \"question2\", \"label\"]].sample(\n",
    "        2_000\n",
    "    ).reset_index(drop=True).to_csv(f\"data/{name}/test.csv\", header=False)\n",
    "    hfd[\"validation\"].to_pandas()[[\"question1\", \"question2\", \"label\"]].sample(\n",
    "        1_000\n",
    "    ).reset_index(drop=True).to_csv(f\"data/{name}/validation.csv\", header=False)\n",
    "\n",
    "\n",
    "save_dataset(d, \"QQP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe182e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoAdapterModel.from_pretrained(\"bert-base-uncased\")\n",
    "a = model.load_adapter(\"adapters/TREC-2-BERT-pfeiffer\")\n",
    "model.add_classification_head(\"head\", num_labels=2)\n",
    "model.add_adapter(\"head\")\n",
    "model.train_adapter(\"head\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f14e35e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/jjukic/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01c15a5dfd90473ba1783414b42d727c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "d = load_dataset(\"glue\", \"cola\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a5243de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'the kittens yawned awake and played.', 'label': -1, 'idx': 3}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[\"test\"][3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "23fee3a0d468748c44cd2b5f7c2d15d26ebb787aed261e84470feedc3724e7bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
